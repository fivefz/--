{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CropModels\n",
    "from CropDataset import MyDataSet,normalize_torch,normalize_05,normalize_dataset,preprocess,preprocess_hflip,preprocess_with_augmentation\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from tensorboardX import SummaryWriter\n",
    "import datetime\n",
    "import os\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from utils import RunningMean\n",
    "import utils\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "NB_CLASS=59\n",
    "SEED=888\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "IMAGE_SIZE=299    # 不同模型修改不同的Size\n",
    "IMAGE_TRAIN_PRE='../data/AgriculturalDisease_trainingset/images/'\n",
    "ANNOTATION_TRAIN='../data/AgriculturalDisease_trainingset/AgriculturalDisease_train_annotations_deleteNoise.json' #是否需要剔除两类异常类\n",
    "IMAGE_VAL_PRE='../data/AgriculturalDisease_validationset/images/'\n",
    "ANNOTATION_VAL='../data/AgriculturalDisease_validationset/AgriculturalDisease_validation_annotations_deleteNoise.json' #是否需要剔除两类异常类\n",
    "date=str(datetime.date.today())\n",
    "with open(ANNOTATION_TRAIN) as datafile1:\n",
    "    trainDataFram=pd.read_json(datafile1,orient='records')\n",
    "with open(ANNOTATION_VAL) as datafile2: #first check if it's a valid json file or not\n",
    "    validateDataFram =pd.read_json(datafile2,orient='records')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getmodel():\n",
    "    print('[+] loading model... ', end='', flush=True)\n",
    "    model=CropModels.InceptionV3Finetune(NB_CLASS)\n",
    "    model.cuda()\n",
    "    print('Done')\n",
    "    return model\n",
    "def train(epochNum):\n",
    "    writer=SummaryWriter('log/'+date+'/InceptionV3/') # 创建 /log/日期/InceptionResnet的组织形式  不同模型需要修改不同名称\n",
    "    train_dataset=MyDataSet(json_Description=ANNOTATION_TRAIN,transform=preprocess_with_augmentation(normalize_05,IMAGE_SIZE),path_pre=IMAGE_TRAIN_PRE)\n",
    "    val_dataset=MyDataSet(json_Description=ANNOTATION_VAL,transform=preprocess(normalize_05,IMAGE_SIZE),path_pre=IMAGE_VAL_PRE)\n",
    "    train_dataLoader=DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE,num_workers=16,shuffle=True)\n",
    "    val_dataLoader=DataLoader(dataset=val_dataset,batch_size=BATCH_SIZE,num_workers=1,shuffle=False)\n",
    "    model=getmodel()\n",
    "    weight=torch.Tensor([1,3,3,3,3,4,2,3,3,3,3,3,3,3,3,3,2,3,3,3,2,3,4,2,3,1,1,3,2,2,1,3,3,1,3,2,3,3,3,3,2,1,3,2,3,3,3,1,3,3,4,4,3,2,2,3,1,1,3]).cuda()\n",
    "    criterion=nn.CrossEntropyLoss(weight=weight).cuda()\n",
    "#     lx, px = utils.predict(model,val_dataLoader)\n",
    "#     min_loss = criterion(Variable(px), Variable(lx)).item()\n",
    "    min_loss=4.1\n",
    "    print('min_loss is :%f'%(min_loss))\n",
    "    min_acc=0.80\n",
    "    patience=0\n",
    "    lr=0.0\n",
    "    momentum=0.9\n",
    "    for epoch in range(epochNum):\n",
    "        print('Epoch {}/{}'.format(epoch, epochNum - 1))\n",
    "        print('-' * 10)\n",
    "        if epoch==3 or epoch==4 or epoch==5:\n",
    "            lr=0.00006\n",
    "            momentum=0.95\n",
    "            print('set lr=:%f,momentum=%f'%(lr,momentum))\n",
    "        if epoch==6:\n",
    "            lr=1e-4\n",
    "            momentum=0.9\n",
    "            print('set lr=:%f,momentum=%f'%(lr,momentum))        \n",
    "        if patience==2:\n",
    "            patience=0\n",
    "            model.load_state_dict(torch.load('../model/InceptionV3/'+date+'_loss_best.pth')['state_dict'])\n",
    "            lr=lr/5\n",
    "            print('loss has increased lr divide 10 lr now is :%f'%(lr))\n",
    "        if epoch==0 or epoch==1 or epoch==2: #第一轮首先训练全连接层\n",
    "            lr=1e-3\n",
    "            optimizer = torch.optim.Adam(model.fresh_params(),lr = lr,amsgrad=True,weight_decay=1e-4)\n",
    "#             optimizer=torch.optim.SGD(params=model.fresh_params(),lr=lr,momentum=0.9)\n",
    "            #optimizer=torch.optim.SGD(params=model.fresh_params(),lr=lr,momentum=0.9)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(model.parameters(),lr = lr,amsgrad=True,weight_decay=1e-4)\n",
    "             #optimizer=torch.optim.SGD(params=model.parameters(),lr=lr,momentum=momentum)\n",
    "#             optimizer=torch.optim.SGD(params=model.parameters(),lr=lr,momentum=momentum)\n",
    "        running_loss = RunningMean()\n",
    "        running_corrects = RunningMean()\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_dataLoader):\n",
    "            model.train(True)\n",
    "            n_batchsize=inputs.size(0)\n",
    "            inputs = Variable(inputs).cuda()\n",
    "            labels = Variable(labels).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            if isinstance(outputs,tuple):\n",
    "                loss=sum((criterion(o,labels)) for o in outputs)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "            running_loss.update(loss.item(),1)\n",
    "            running_corrects.update(torch.sum(preds == labels.data).data,n_batchsize)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx%30==29:\n",
    "                print('[epoch:%d,batch:%d]:acc: %f,loss:%f'%(epoch,batch_idx,running_corrects.value,running_loss.value))\n",
    "                if batch_idx%300==299: \n",
    "                    niter = epoch * len(train_dataset)/BATCH_SIZE + batch_idx\n",
    "                    writer.add_scalar('Train/Acc',running_corrects.value,niter)\n",
    "                    writer.add_scalar('Train/Loss',running_loss.value,niter)\n",
    "                    lx,px=utils.predict(model,val_dataLoader)\n",
    "                    log_loss = criterion(Variable(px), Variable(lx))\n",
    "                    log_loss = log_loss.item()\n",
    "                    _, preds = torch.max(px, dim=1)\n",
    "                    accuracy = torch.mean((preds == lx).float())\n",
    "                    writer.add_scalar('Val/Acc',accuracy,niter)\n",
    "                    writer.add_scalar('Val/Loss',log_loss,niter)\n",
    "                    print('[epoch:%d,batch:%d]: val_loss:%f,val_acc:%f,val_total:%d'%(epoch,batch_idx,log_loss,accuracy,len(val_dataset)))\n",
    "        print('[epoch:%d] :acc: %f,loss:%f,lr:%f,patience:%d'%(epoch,running_corrects.value,running_loss.value,lr,patience))       \n",
    "        lx,px=utils.predict(model,val_dataLoader)\n",
    "        log_loss = criterion(Variable(px), Variable(lx))\n",
    "        log_loss = log_loss.item()\n",
    "        _, preds = torch.max(px, dim=1)\n",
    "        accuracy = torch.mean((preds == lx).float())\n",
    "        writer.add_scalar('Val/Acc',accuracy,(epoch+1) * len(train_dataset)/BATCH_SIZE)\n",
    "        writer.add_scalar('Val/Loss',log_loss,(epoch+1) * len(train_dataset)/BATCH_SIZE)\n",
    "        print('[epoch:%d]: val_loss:%f,val_acc:%f,'%(epoch,log_loss,accuracy))\n",
    "        if  log_loss < min_loss:\n",
    "            utils.snapshot('../model/', 'InceptionV3', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy })          \n",
    "            patience = 0\n",
    "            min_loss=log_loss\n",
    "            print('save new model loss,now loss is ',min_loss)\n",
    "        else:\n",
    "            patience += 1\n",
    "        if accuracy>min_acc:\n",
    "            utils.snapshot('../model/', 'InceptionV3', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy },key='acc') \n",
    "            min_acc=accuracy\n",
    "            print('save new model acc,now acc is ',min_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] loading model... Done\n",
      "min_loss is :4.100000\n",
      "Epoch 0/59\n",
      "----------\n",
      "[epoch:0,batch:29]:acc: 0.079167,loss:4.044367\n",
      "[epoch:0,batch:59]:acc: 0.172917,loss:3.756970\n",
      "[epoch:0,batch:89]:acc: 0.246875,loss:3.509690\n",
      "[epoch:0,batch:119]:acc: 0.299740,loss:3.279542\n",
      "[epoch:0,batch:149]:acc: 0.340000,loss:3.105226\n",
      "[epoch:0,batch:179]:acc: 0.371528,loss:2.942487\n",
      "[epoch:0,batch:209]:acc: 0.401935,loss:2.796054\n",
      "[epoch:0,batch:239]:acc: 0.422266,loss:2.690154\n",
      "[epoch:0,batch:269]:acc: 0.440856,loss:2.587785\n",
      "[epoch:0,batch:299]:acc: 0.456250,loss:2.500065\n",
      "[epoch:0,batch:299]: val_loss:1.505650,val_acc:0.652126,val_total:4539\n",
      "[epoch:0,batch:329]:acc: 0.466477,loss:2.426651\n",
      "[epoch:0,batch:359]:acc: 0.476997,loss:2.360043\n",
      "[epoch:0,batch:389]:acc: 0.487660,loss:2.295033\n",
      "[epoch:0,batch:419]:acc: 0.498363,loss:2.236728\n",
      "[epoch:0,batch:449]:acc: 0.504167,loss:2.190358\n",
      "[epoch:0,batch:479]:acc: 0.509701,loss:2.151367\n",
      "[epoch:0,batch:509]:acc: 0.516912,loss:2.111625\n",
      "[epoch:0,batch:539]:acc: 0.522627,loss:2.074899\n",
      "[epoch:0,batch:569]:acc: 0.529496,loss:2.034051\n",
      "[epoch:0,batch:599]:acc: 0.534635,loss:1.999186\n",
      "[epoch:0,batch:599]: val_loss:1.134097,val_acc:0.683631,val_total:4539\n",
      "[epoch:0,batch:629]:acc: 0.539534,loss:1.965622\n",
      "[epoch:0,batch:659]:acc: 0.544792,loss:1.936220\n",
      "[epoch:0,batch:689]:acc: 0.550000,loss:1.905373\n",
      "[epoch:0,batch:719]:acc: 0.552951,loss:1.880161\n",
      "[epoch:0,batch:749]:acc: 0.557083,loss:1.854603\n",
      "[epoch:0,batch:779]:acc: 0.561218,loss:1.828412\n",
      "[epoch:0,batch:809]:acc: 0.565934,loss:1.804418\n",
      "[epoch:0,batch:839]:acc: 0.569010,loss:1.783856\n",
      "[epoch:0,batch:869]:acc: 0.571947,loss:1.764870\n",
      "[epoch:0,batch:899]:acc: 0.574722,loss:1.744498\n",
      "[epoch:0,batch:899]: val_loss:0.965788,val_acc:0.707425,val_total:4539\n",
      "[epoch:0,batch:929]:acc: 0.578125,loss:1.724296\n",
      "[epoch:0,batch:959]:acc: 0.580501,loss:1.709169\n",
      "[epoch:0,batch:989]:acc: 0.582765,loss:1.692045\n",
      "[epoch:0] :acc: 0.582842,loss:1.691306,lr:0.001000,patience:0\n",
      "[epoch:0]: val_loss:0.951780,val_acc:0.722406,\n",
      "save new model loss,now loss is  0.9517800211906433\n",
      "Epoch 1/59\n",
      "----------\n",
      "[epoch:1,batch:29]:acc: 0.639583,loss:1.273256\n",
      "[epoch:1,batch:59]:acc: 0.641146,loss:1.269830\n",
      "[epoch:1,batch:89]:acc: 0.650347,loss:1.220161\n",
      "[epoch:1,batch:119]:acc: 0.651042,loss:1.191684\n",
      "[epoch:1,batch:149]:acc: 0.656250,loss:1.177840\n",
      "[epoch:1,batch:179]:acc: 0.658681,loss:1.169589\n",
      "[epoch:1,batch:209]:acc: 0.658631,loss:1.167363\n",
      "[epoch:1,batch:239]:acc: 0.661068,loss:1.156343\n",
      "[epoch:1,batch:269]:acc: 0.658565,loss:1.153911\n",
      "[epoch:1,batch:299]:acc: 0.660000,loss:1.149443\n",
      "[epoch:1,batch:299]: val_loss:0.867814,val_acc:0.719321,val_total:4539\n",
      "[epoch:1,batch:329]:acc: 0.660606,loss:1.144422\n",
      "[epoch:1,batch:359]:acc: 0.661285,loss:1.142192\n",
      "[epoch:1,batch:389]:acc: 0.660737,loss:1.139424\n",
      "[epoch:1,batch:419]:acc: 0.660268,loss:1.138727\n",
      "[epoch:1,batch:449]:acc: 0.660625,loss:1.135109\n",
      "[epoch:1,batch:479]:acc: 0.662109,loss:1.129201\n",
      "[epoch:1,batch:509]:acc: 0.661581,loss:1.130236\n",
      "[epoch:1,batch:539]:acc: 0.662037,loss:1.126990\n",
      "[epoch:1,batch:569]:acc: 0.660855,loss:1.125981\n",
      "[epoch:1,batch:599]:acc: 0.660937,loss:1.126402\n",
      "[epoch:1,batch:599]: val_loss:0.843897,val_acc:0.727693,val_total:4539\n",
      "[epoch:1,batch:629]:acc: 0.661210,loss:1.122426\n",
      "[epoch:1,batch:659]:acc: 0.661553,loss:1.120406\n",
      "[epoch:1,batch:689]:acc: 0.662998,loss:1.115642\n",
      "[epoch:1,batch:719]:acc: 0.663238,loss:1.112254\n",
      "[epoch:1,batch:749]:acc: 0.664125,loss:1.110273\n",
      "[epoch:1,batch:779]:acc: 0.664423,loss:1.108339\n",
      "[epoch:1,batch:809]:acc: 0.664236,loss:1.106751\n",
      "[epoch:1,batch:839]:acc: 0.664732,loss:1.104058\n",
      "[epoch:1,batch:869]:acc: 0.664835,loss:1.101451\n",
      "[epoch:1,batch:899]:acc: 0.664896,loss:1.100477\n",
      "[epoch:1,batch:899]: val_loss:0.826461,val_acc:0.721745,val_total:4539\n",
      "[epoch:1,batch:929]:acc: 0.665289,loss:1.098499\n",
      "[epoch:1,batch:959]:acc: 0.665820,loss:1.096498\n",
      "[epoch:1,batch:989]:acc: 0.666162,loss:1.096531\n",
      "[epoch:1] :acc: 0.666204,loss:1.096571,lr:0.001000,patience:0\n",
      "[epoch:1]: val_loss:0.899084,val_acc:0.687817,\n",
      "save new model loss,now loss is  0.8990842700004578\n",
      "Epoch 2/59\n",
      "----------\n",
      "[epoch:2,batch:29]:acc: 0.636458,loss:1.148923\n",
      "[epoch:2,batch:59]:acc: 0.643750,loss:1.144604\n",
      "[epoch:2,batch:89]:acc: 0.652431,loss:1.111399\n",
      "[epoch:2,batch:119]:acc: 0.656771,loss:1.089098\n",
      "[epoch:2,batch:149]:acc: 0.664792,loss:1.065862\n",
      "[epoch:2,batch:179]:acc: 0.667708,loss:1.046778\n",
      "[epoch:2,batch:209]:acc: 0.667560,loss:1.049896\n",
      "[epoch:2,batch:239]:acc: 0.665755,loss:1.050043\n",
      "[epoch:2,batch:269]:acc: 0.666551,loss:1.048553\n",
      "[epoch:2,batch:299]:acc: 0.668646,loss:1.048091\n",
      "[epoch:2,batch:299]: val_loss:0.797414,val_acc:0.737387,val_total:4539\n",
      "[epoch:2,batch:329]:acc: 0.670076,loss:1.041724\n",
      "[epoch:2,batch:359]:acc: 0.670660,loss:1.042655\n",
      "[epoch:2,batch:389]:acc: 0.672035,loss:1.043095\n",
      "[epoch:2,batch:419]:acc: 0.672321,loss:1.036669\n",
      "[epoch:2,batch:449]:acc: 0.672153,loss:1.036341\n",
      "[epoch:2,batch:479]:acc: 0.672917,loss:1.031942\n",
      "[epoch:2,batch:509]:acc: 0.674694,loss:1.027920\n",
      "[epoch:2,batch:539]:acc: 0.675058,loss:1.026630\n",
      "[epoch:2,batch:569]:acc: 0.675768,loss:1.026804\n",
      "[epoch:2,batch:599]:acc: 0.675781,loss:1.027619\n",
      "[epoch:2,batch:599]: val_loss:0.777552,val_acc:0.741132,val_total:4539\n",
      "[epoch:2,batch:629]:acc: 0.675149,loss:1.028054\n",
      "[epoch:2,batch:659]:acc: 0.676231,loss:1.026039\n",
      "[epoch:2,batch:689]:acc: 0.675543,loss:1.025844\n",
      "[epoch:2,batch:719]:acc: 0.675651,loss:1.026583\n",
      "[epoch:2,batch:749]:acc: 0.675958,loss:1.026647\n",
      "[epoch:2,batch:779]:acc: 0.675921,loss:1.026287\n",
      "[epoch:2,batch:809]:acc: 0.675154,loss:1.029628\n",
      "[epoch:2,batch:839]:acc: 0.676004,loss:1.028253\n",
      "[epoch:2,batch:869]:acc: 0.676329,loss:1.026508\n",
      "[epoch:2,batch:899]:acc: 0.675972,loss:1.026986\n",
      "[epoch:2,batch:899]: val_loss:0.786398,val_acc:0.737607,val_total:4539\n",
      "[epoch:2,batch:929]:acc: 0.675806,loss:1.027192\n",
      "[epoch:2,batch:959]:acc: 0.675911,loss:1.028155\n",
      "[epoch:2,batch:989]:acc: 0.675821,loss:1.026045\n",
      "[epoch:2] :acc: 0.675757,loss:1.026012,lr:0.001000,patience:0\n",
      "[epoch:2]: val_loss:0.744330,val_acc:0.754351,\n",
      "save new model loss,now loss is  0.7443299293518066\n",
      "Epoch 3/59\n",
      "----------\n",
      "set lr=:0.000060,momentum=0.950000\n",
      "[epoch:3,batch:29]:acc: 0.693750,loss:0.939122\n",
      "[epoch:3,batch:59]:acc: 0.722917,loss:0.856453\n",
      "[epoch:3,batch:89]:acc: 0.725694,loss:0.827660\n",
      "[epoch:3,batch:119]:acc: 0.732292,loss:0.813569\n",
      "[epoch:3,batch:149]:acc: 0.738542,loss:0.792034\n",
      "[epoch:3,batch:179]:acc: 0.739757,loss:0.770612\n",
      "[epoch:3,batch:209]:acc: 0.743899,loss:0.761962\n",
      "[epoch:3,batch:239]:acc: 0.745964,loss:0.746636\n",
      "[epoch:3,batch:269]:acc: 0.749421,loss:0.739720\n",
      "[epoch:3,batch:299]:acc: 0.749479,loss:0.736577\n",
      "[epoch:3,batch:299]: val_loss:0.555241,val_acc:0.787839,val_total:4539\n",
      "[epoch:3,batch:329]:acc: 0.749148,loss:0.732802\n",
      "[epoch:3,batch:359]:acc: 0.751910,loss:0.724795\n",
      "[epoch:3,batch:389]:acc: 0.753766,loss:0.717650\n",
      "[epoch:3,batch:419]:acc: 0.753943,loss:0.713007\n",
      "[epoch:3,batch:449]:acc: 0.755000,loss:0.708284\n",
      "[epoch:3,batch:479]:acc: 0.756055,loss:0.700562\n",
      "[epoch:3,batch:509]:acc: 0.757169,loss:0.697075\n",
      "[epoch:3,batch:539]:acc: 0.757060,loss:0.695284\n",
      "[epoch:3,batch:569]:acc: 0.759101,loss:0.691645\n",
      "[epoch:3,batch:599]:acc: 0.759271,loss:0.691188\n",
      "[epoch:3,batch:599]: val_loss:0.481195,val_acc:0.820225,val_total:4539\n",
      "[epoch:3,batch:629]:acc: 0.760218,loss:0.687327\n",
      "[epoch:3,batch:659]:acc: 0.761222,loss:0.683506\n",
      "[epoch:3,batch:689]:acc: 0.760960,loss:0.679475\n",
      "[epoch:3,batch:719]:acc: 0.761675,loss:0.675451\n",
      "[epoch:3,batch:749]:acc: 0.763042,loss:0.672359\n",
      "[epoch:3,batch:779]:acc: 0.763862,loss:0.669904\n",
      "[epoch:3,batch:809]:acc: 0.764892,loss:0.665823\n",
      "[epoch:3,batch:839]:acc: 0.766332,loss:0.659726\n",
      "[epoch:3,batch:869]:acc: 0.767241,loss:0.656540\n",
      "[epoch:3,batch:899]:acc: 0.768229,loss:0.654952\n",
      "[epoch:3,batch:899]: val_loss:0.451463,val_acc:0.831901,val_total:4539\n",
      "[epoch:3,batch:929]:acc: 0.769355,loss:0.651797\n",
      "[epoch:3,batch:959]:acc: 0.770443,loss:0.648032\n",
      "[epoch:3,batch:989]:acc: 0.771654,loss:0.643850\n",
      "[epoch:3] :acc: 0.771574,loss:0.643779,lr:0.000060,patience:0\n",
      "[epoch:3]: val_loss:0.435628,val_acc:0.834986,\n",
      "save new model loss,now loss is  0.4356275796890259\n",
      "save new model acc,now acc is  tensor(0.8350, device='cuda:0')\n",
      "Epoch 4/59\n",
      "----------\n",
      "set lr=:0.000060,momentum=0.950000\n",
      "[epoch:4,batch:29]:acc: 0.816667,loss:0.484016\n",
      "[epoch:4,batch:59]:acc: 0.826042,loss:0.466590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:4,batch:89]:acc: 0.826389,loss:0.468957\n",
      "[epoch:4,batch:119]:acc: 0.820573,loss:0.485902\n",
      "[epoch:4,batch:149]:acc: 0.816250,loss:0.486111\n",
      "[epoch:4,batch:179]:acc: 0.816840,loss:0.484819\n",
      "[epoch:4,batch:209]:acc: 0.814137,loss:0.489679\n",
      "[epoch:4,batch:239]:acc: 0.814844,loss:0.482803\n",
      "[epoch:4,batch:269]:acc: 0.813426,loss:0.484627\n",
      "[epoch:4,batch:299]:acc: 0.811771,loss:0.488163\n",
      "[epoch:4,batch:299]: val_loss:0.487751,val_acc:0.823309,val_total:4539\n",
      "[epoch:4,batch:329]:acc: 0.811269,loss:0.490298\n",
      "[epoch:4,batch:359]:acc: 0.809201,loss:0.493789\n",
      "[epoch:4,batch:389]:acc: 0.808173,loss:0.492869\n",
      "[epoch:4,batch:419]:acc: 0.808259,loss:0.491687\n",
      "[epoch:4,batch:449]:acc: 0.807639,loss:0.492044\n",
      "[epoch:4,batch:479]:acc: 0.808594,loss:0.488945\n",
      "[epoch:4,batch:509]:acc: 0.809865,loss:0.486640\n",
      "[epoch:4,batch:539]:acc: 0.810475,loss:0.484796\n",
      "[epoch:4,batch:569]:acc: 0.811513,loss:0.485277\n",
      "[epoch:4,batch:599]:acc: 0.811354,loss:0.487266\n",
      "[epoch:4,batch:599]: val_loss:0.452308,val_acc:0.842036,val_total:4539\n",
      "[epoch:4,batch:629]:acc: 0.811359,loss:0.488187\n",
      "[epoch:4,batch:659]:acc: 0.810559,loss:0.490720\n",
      "[epoch:4,batch:689]:acc: 0.810734,loss:0.491400\n",
      "[epoch:4,batch:719]:acc: 0.811372,loss:0.489027\n",
      "[epoch:4,batch:749]:acc: 0.811750,loss:0.486991\n",
      "[epoch:4,batch:779]:acc: 0.811859,loss:0.487915\n",
      "[epoch:4,batch:809]:acc: 0.811497,loss:0.489555\n",
      "[epoch:4,batch:839]:acc: 0.811458,loss:0.490589\n",
      "[epoch:4,batch:869]:acc: 0.811351,loss:0.489994\n",
      "[epoch:4,batch:899]:acc: 0.811319,loss:0.490395\n",
      "[epoch:4,batch:899]: val_loss:0.448776,val_acc:0.836087,val_total:4539\n",
      "[epoch:4,batch:929]:acc: 0.811660,loss:0.490666\n",
      "[epoch:4,batch:959]:acc: 0.811523,loss:0.490331\n",
      "[epoch:4,batch:989]:acc: 0.812058,loss:0.489455\n",
      "[epoch:4] :acc: 0.812120,loss:0.489795,lr:0.000060,patience:0\n",
      "[epoch:4]: val_loss:0.453871,val_acc:0.827715,\n",
      "Epoch 5/59\n",
      "----------\n",
      "set lr=:0.000060,momentum=0.950000\n",
      "[epoch:5,batch:29]:acc: 0.846875,loss:0.391478\n",
      "[epoch:5,batch:59]:acc: 0.841146,loss:0.406872\n",
      "[epoch:5,batch:89]:acc: 0.841667,loss:0.405475\n",
      "[epoch:5,batch:119]:acc: 0.838281,loss:0.409897\n",
      "[epoch:5,batch:149]:acc: 0.839583,loss:0.404849\n",
      "[epoch:5,batch:179]:acc: 0.840278,loss:0.402527\n",
      "[epoch:5,batch:209]:acc: 0.841369,loss:0.400730\n",
      "[epoch:5,batch:239]:acc: 0.841797,loss:0.399653\n",
      "[epoch:5,batch:269]:acc: 0.842593,loss:0.400936\n",
      "[epoch:5,batch:299]:acc: 0.839479,loss:0.412494\n",
      "[epoch:5,batch:299]: val_loss:0.427823,val_acc:0.849526,val_total:4539\n",
      "[epoch:5,batch:329]:acc: 0.835985,loss:0.417306\n",
      "[epoch:5,batch:359]:acc: 0.836372,loss:0.419974\n",
      "[epoch:5,batch:389]:acc: 0.836859,loss:0.417797\n",
      "[epoch:5,batch:419]:acc: 0.837202,loss:0.417649\n",
      "[epoch:5,batch:449]:acc: 0.837222,loss:0.419079\n",
      "[epoch:5,batch:479]:acc: 0.837109,loss:0.420560\n",
      "[epoch:5,batch:509]:acc: 0.837255,loss:0.417689\n",
      "[epoch:5,batch:539]:acc: 0.837616,loss:0.417853\n",
      "[epoch:5,batch:569]:acc: 0.837007,loss:0.420746\n",
      "[epoch:5,batch:599]:acc: 0.836771,loss:0.420496\n",
      "[epoch:5,batch:599]: val_loss:0.437676,val_acc:0.842476,val_total:4539\n",
      "[epoch:5,batch:629]:acc: 0.835962,loss:0.421945\n",
      "[epoch:5,batch:659]:acc: 0.836411,loss:0.421459\n",
      "[epoch:5,batch:689]:acc: 0.836504,loss:0.420763\n",
      "[epoch:5,batch:719]:acc: 0.836068,loss:0.422151\n",
      "[epoch:5,batch:749]:acc: 0.836000,loss:0.421576\n",
      "[epoch:5,batch:779]:acc: 0.835938,loss:0.421385\n",
      "[epoch:5,batch:809]:acc: 0.835494,loss:0.421426\n",
      "[epoch:5,batch:839]:acc: 0.835603,loss:0.422300\n",
      "[epoch:5,batch:869]:acc: 0.835668,loss:0.421788\n",
      "[epoch:5,batch:899]:acc: 0.835313,loss:0.422720\n",
      "[epoch:5,batch:899]: val_loss:0.429903,val_acc:0.843137,val_total:4539\n",
      "[epoch:5,batch:929]:acc: 0.835349,loss:0.422094\n",
      "[epoch:5,batch:959]:acc: 0.835807,loss:0.421056\n",
      "[epoch:5,batch:989]:acc: 0.835890,loss:0.420739\n",
      "[epoch:5] :acc: 0.835861,loss:0.421182,lr:0.000060,patience:1\n",
      "[epoch:5]: val_loss:0.440980,val_acc:0.839833,\n",
      "save new model acc,now acc is  tensor(0.8398, device='cuda:0')\n",
      "Epoch 6/59\n",
      "----------\n",
      "set lr=:0.000100,momentum=0.900000\n",
      "loss has increased lr divide 10 lr now is :0.000020\n",
      "[epoch:6,batch:29]:acc: 0.816667,loss:0.527562\n",
      "[epoch:6,batch:59]:acc: 0.830208,loss:0.463262\n",
      "[epoch:6,batch:89]:acc: 0.826042,loss:0.448614\n",
      "[epoch:6,batch:119]:acc: 0.822135,loss:0.455435\n",
      "[epoch:6,batch:149]:acc: 0.825625,loss:0.449898\n",
      "[epoch:6,batch:179]:acc: 0.826389,loss:0.448204\n",
      "[epoch:6,batch:209]:acc: 0.827827,loss:0.451466\n",
      "[epoch:6,batch:239]:acc: 0.827083,loss:0.453304\n",
      "[epoch:6,batch:269]:acc: 0.828356,loss:0.448968\n",
      "[epoch:6,batch:299]:acc: 0.828229,loss:0.444860\n",
      "[epoch:6,batch:299]: val_loss:0.424700,val_acc:0.841815,val_total:4539\n",
      "[epoch:6,batch:329]:acc: 0.829735,loss:0.441934\n",
      "[epoch:6,batch:359]:acc: 0.828906,loss:0.441515\n",
      "[epoch:6,batch:389]:acc: 0.829167,loss:0.438698\n",
      "[epoch:6,batch:419]:acc: 0.828571,loss:0.438716\n",
      "[epoch:6,batch:449]:acc: 0.828750,loss:0.438293\n",
      "[epoch:6,batch:479]:acc: 0.828516,loss:0.437576\n",
      "[epoch:6,batch:509]:acc: 0.828922,loss:0.435529\n",
      "[epoch:6,batch:539]:acc: 0.829340,loss:0.433752\n",
      "[epoch:6,batch:569]:acc: 0.829605,loss:0.431949\n",
      "[epoch:6,batch:599]:acc: 0.829375,loss:0.431101\n",
      "[epoch:6,batch:599]: val_loss:0.424236,val_acc:0.840053,val_total:4539\n",
      "[epoch:6,batch:629]:acc: 0.829911,loss:0.429355\n",
      "[epoch:6,batch:659]:acc: 0.830303,loss:0.429595\n",
      "[epoch:6,batch:689]:acc: 0.830435,loss:0.429998\n",
      "[epoch:6,batch:719]:acc: 0.831120,loss:0.428605\n",
      "[epoch:6,batch:749]:acc: 0.831333,loss:0.427250\n",
      "[epoch:6,batch:779]:acc: 0.831250,loss:0.427676\n",
      "[epoch:6,batch:809]:acc: 0.830633,loss:0.428695\n",
      "[epoch:6,batch:839]:acc: 0.831027,loss:0.428047\n",
      "[epoch:6,batch:869]:acc: 0.830963,loss:0.429279\n",
      "[epoch:6,batch:899]:acc: 0.831007,loss:0.428623\n",
      "[epoch:6,batch:899]: val_loss:0.421402,val_acc:0.846222,val_total:4539\n",
      "[epoch:6,batch:929]:acc: 0.830712,loss:0.429109\n",
      "[epoch:6,batch:959]:acc: 0.830469,loss:0.429052\n",
      "[epoch:6,batch:989]:acc: 0.830366,loss:0.430132\n",
      "[epoch:6] :acc: 0.830375,loss:0.430549,lr:0.000020,patience:0\n",
      "[epoch:6]: val_loss:0.418619,val_acc:0.845120,\n",
      "save new model loss,now loss is  0.4186194837093353\n",
      "save new model acc,now acc is  tensor(0.8451, device='cuda:0')\n",
      "Epoch 7/59\n",
      "----------\n",
      "[epoch:7,batch:29]:acc: 0.840625,loss:0.385860\n",
      "[epoch:7,batch:59]:acc: 0.846875,loss:0.383723\n",
      "[epoch:7,batch:89]:acc: 0.844097,loss:0.387354\n",
      "[epoch:7,batch:119]:acc: 0.846094,loss:0.379968\n",
      "[epoch:7,batch:149]:acc: 0.849583,loss:0.375976\n",
      "[epoch:7,batch:179]:acc: 0.848785,loss:0.382197\n",
      "[epoch:7,batch:209]:acc: 0.848214,loss:0.379801\n",
      "[epoch:7,batch:239]:acc: 0.850911,loss:0.374617\n",
      "[epoch:7,batch:269]:acc: 0.848032,loss:0.379330\n",
      "[epoch:7,batch:299]:acc: 0.847812,loss:0.379201\n",
      "[epoch:7,batch:299]: val_loss:0.415010,val_acc:0.848645,val_total:4539\n",
      "[epoch:7,batch:329]:acc: 0.846970,loss:0.379835\n",
      "[epoch:7,batch:359]:acc: 0.846701,loss:0.378498\n",
      "[epoch:7,batch:389]:acc: 0.846154,loss:0.380853\n",
      "[epoch:7,batch:419]:acc: 0.845685,loss:0.383229\n",
      "[epoch:7,batch:449]:acc: 0.845625,loss:0.384127\n",
      "[epoch:7,batch:479]:acc: 0.843880,loss:0.385931\n",
      "[epoch:7,batch:509]:acc: 0.843934,loss:0.384312\n",
      "[epoch:7,batch:539]:acc: 0.843981,loss:0.384435\n",
      "[epoch:7,batch:569]:acc: 0.844408,loss:0.383470\n",
      "[epoch:7,batch:599]:acc: 0.844792,loss:0.382739\n",
      "[epoch:7,batch:599]: val_loss:0.424069,val_acc:0.846662,val_total:4539\n",
      "[epoch:7,batch:629]:acc: 0.845040,loss:0.382692\n",
      "[epoch:7,batch:659]:acc: 0.845360,loss:0.381935\n",
      "[epoch:7,batch:689]:acc: 0.845607,loss:0.381963\n",
      "[epoch:7,batch:719]:acc: 0.846137,loss:0.380578\n",
      "[epoch:7,batch:749]:acc: 0.846208,loss:0.380422\n",
      "[epoch:7,batch:779]:acc: 0.846474,loss:0.379507\n",
      "[epoch:7,batch:809]:acc: 0.846759,loss:0.377805\n",
      "[epoch:7,batch:839]:acc: 0.846838,loss:0.377040\n",
      "[epoch:7,batch:869]:acc: 0.846588,loss:0.377934\n",
      "[epoch:7,batch:899]:acc: 0.846354,loss:0.378006\n",
      "[epoch:7,batch:899]: val_loss:0.412695,val_acc:0.847764,val_total:4539\n",
      "[epoch:7,batch:929]:acc: 0.846707,loss:0.378680\n",
      "[epoch:7,batch:959]:acc: 0.847135,loss:0.378639\n",
      "[epoch:7,batch:989]:acc: 0.846686,loss:0.379448\n",
      "[epoch:7] :acc: 0.846644,loss:0.379610,lr:0.000020,patience:0\n",
      "[epoch:7]: val_loss:0.418355,val_acc:0.844679,\n",
      "save new model loss,now loss is  0.41835495829582214\n",
      "Epoch 8/59\n",
      "----------\n",
      "[epoch:8,batch:29]:acc: 0.860417,loss:0.350876\n",
      "[epoch:8,batch:59]:acc: 0.858854,loss:0.347417\n",
      "[epoch:8,batch:89]:acc: 0.865278,loss:0.338326\n",
      "[epoch:8,batch:119]:acc: 0.866406,loss:0.333498\n",
      "[epoch:8,batch:149]:acc: 0.867708,loss:0.326122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:8,batch:179]:acc: 0.866146,loss:0.330260\n",
      "[epoch:8,batch:209]:acc: 0.865179,loss:0.332653\n",
      "[epoch:8,batch:239]:acc: 0.863802,loss:0.335374\n",
      "[epoch:8,batch:269]:acc: 0.864931,loss:0.329194\n",
      "[epoch:8,batch:299]:acc: 0.863750,loss:0.332480\n",
      "[epoch:8,batch:299]: val_loss:0.421895,val_acc:0.844018,val_total:4539\n",
      "[epoch:8,batch:329]:acc: 0.862879,loss:0.334995\n",
      "[epoch:8,batch:359]:acc: 0.861806,loss:0.336820\n",
      "[epoch:8,batch:389]:acc: 0.862821,loss:0.334192\n",
      "[epoch:8,batch:419]:acc: 0.863318,loss:0.334783\n",
      "[epoch:8,batch:449]:acc: 0.863750,loss:0.333529\n",
      "[epoch:8,batch:479]:acc: 0.863672,loss:0.333742\n",
      "[epoch:8,batch:509]:acc: 0.862500,loss:0.336674\n",
      "[epoch:8,batch:539]:acc: 0.861863,loss:0.337482\n",
      "[epoch:8,batch:569]:acc: 0.861787,loss:0.337558\n",
      "[epoch:8,batch:599]:acc: 0.861875,loss:0.336719\n",
      "[epoch:8,batch:599]: val_loss:0.406975,val_acc:0.852390,val_total:4539\n",
      "[epoch:8,batch:629]:acc: 0.862599,loss:0.335430\n",
      "[epoch:8,batch:659]:acc: 0.862027,loss:0.338158\n",
      "[epoch:8,batch:689]:acc: 0.860553,loss:0.340252\n",
      "[epoch:8,batch:719]:acc: 0.860677,loss:0.339565\n",
      "[epoch:8,batch:749]:acc: 0.860583,loss:0.339058\n",
      "[epoch:8,batch:779]:acc: 0.859776,loss:0.340572\n",
      "[epoch:8,batch:809]:acc: 0.859722,loss:0.340690\n",
      "[epoch:8,batch:839]:acc: 0.860268,loss:0.340321\n",
      "[epoch:8,batch:869]:acc: 0.860560,loss:0.341237\n",
      "[epoch:8,batch:899]:acc: 0.860764,loss:0.340795\n",
      "[epoch:8,batch:899]: val_loss:0.420733,val_acc:0.847764,val_total:4539\n",
      "[epoch:8,batch:929]:acc: 0.860484,loss:0.341597\n",
      "[epoch:8,batch:959]:acc: 0.860319,loss:0.342909\n",
      "[epoch:8,batch:989]:acc: 0.859880,loss:0.342907\n",
      "[epoch:8] :acc: 0.859823,loss:0.343403,lr:0.000020,patience:0\n",
      "[epoch:8]: val_loss:0.410148,val_acc:0.853492,\n",
      "save new model loss,now loss is  0.41014763712882996\n",
      "save new model acc,now acc is  tensor(0.8535, device='cuda:0')\n",
      "Epoch 9/59\n",
      "----------\n",
      "[epoch:9,batch:29]:acc: 0.863542,loss:0.289873\n",
      "[epoch:9,batch:59]:acc: 0.880729,loss:0.286011\n",
      "[epoch:9,batch:89]:acc: 0.873611,loss:0.289155\n",
      "[epoch:9,batch:119]:acc: 0.869531,loss:0.289825\n",
      "[epoch:9,batch:149]:acc: 0.869792,loss:0.292113\n",
      "[epoch:9,batch:179]:acc: 0.868924,loss:0.295184\n",
      "[epoch:9,batch:209]:acc: 0.866815,loss:0.299850\n",
      "[epoch:9,batch:239]:acc: 0.866667,loss:0.304064\n",
      "[epoch:9,batch:269]:acc: 0.869676,loss:0.300130\n",
      "[epoch:9,batch:299]:acc: 0.870313,loss:0.300085\n",
      "[epoch:9,batch:299]: val_loss:0.423607,val_acc:0.843137,val_total:4539\n",
      "[epoch:9,batch:329]:acc: 0.870549,loss:0.302650\n",
      "[epoch:9,batch:359]:acc: 0.870313,loss:0.302475\n",
      "[epoch:9,batch:389]:acc: 0.869952,loss:0.302539\n",
      "[epoch:9,batch:419]:acc: 0.869717,loss:0.302560\n",
      "[epoch:9,batch:449]:acc: 0.871042,loss:0.299885\n",
      "[epoch:9,batch:479]:acc: 0.871094,loss:0.302210\n",
      "[epoch:9,batch:509]:acc: 0.870833,loss:0.304291\n",
      "[epoch:9,batch:539]:acc: 0.870891,loss:0.305318\n",
      "[epoch:9,batch:569]:acc: 0.869737,loss:0.306939\n",
      "[epoch:9,batch:599]:acc: 0.870365,loss:0.305874\n",
      "[epoch:9,batch:599]: val_loss:0.412469,val_acc:0.852390,val_total:4539\n",
      "[epoch:9,batch:629]:acc: 0.870337,loss:0.306147\n",
      "[epoch:9,batch:659]:acc: 0.870313,loss:0.306104\n",
      "[epoch:9,batch:689]:acc: 0.870697,loss:0.305850\n",
      "[epoch:9,batch:719]:acc: 0.871050,loss:0.307194\n",
      "[epoch:9,batch:749]:acc: 0.871000,loss:0.308464\n",
      "[epoch:9,batch:779]:acc: 0.871595,loss:0.307818\n",
      "[epoch:9,batch:809]:acc: 0.871065,loss:0.309391\n",
      "[epoch:9,batch:839]:acc: 0.870833,loss:0.310800\n",
      "[epoch:9,batch:869]:acc: 0.870941,loss:0.310937\n",
      "[epoch:9,batch:899]:acc: 0.871250,loss:0.309770\n",
      "[epoch:9,batch:899]: val_loss:0.420210,val_acc:0.847103,val_total:4539\n",
      "[epoch:9,batch:929]:acc: 0.871035,loss:0.311520\n",
      "[epoch:9,batch:959]:acc: 0.871289,loss:0.311405\n",
      "[epoch:9,batch:989]:acc: 0.870707,loss:0.311906\n",
      "[epoch:9] :acc: 0.870606,loss:0.311740,lr:0.000020,patience:0\n",
      "[epoch:9]: val_loss:0.421890,val_acc:0.853272,\n",
      "Epoch 10/59\n",
      "----------\n",
      "[epoch:10,batch:29]:acc: 0.878125,loss:0.293575\n",
      "[epoch:10,batch:59]:acc: 0.877604,loss:0.295743\n",
      "[epoch:10,batch:89]:acc: 0.877431,loss:0.291169\n",
      "[epoch:10,batch:119]:acc: 0.878385,loss:0.283770\n",
      "[epoch:10,batch:149]:acc: 0.877500,loss:0.290208\n",
      "[epoch:10,batch:179]:acc: 0.880035,loss:0.283470\n",
      "[epoch:10,batch:209]:acc: 0.881994,loss:0.278136\n",
      "[epoch:10,batch:239]:acc: 0.879687,loss:0.279145\n",
      "[epoch:10,batch:269]:acc: 0.879745,loss:0.281536\n",
      "[epoch:10,batch:299]:acc: 0.879479,loss:0.281770\n",
      "[epoch:10,batch:299]: val_loss:0.424157,val_acc:0.850848,val_total:4539\n",
      "[epoch:10,batch:329]:acc: 0.882008,loss:0.278392\n",
      "[epoch:10,batch:359]:acc: 0.881771,loss:0.281257\n",
      "[epoch:10,batch:389]:acc: 0.881170,loss:0.280689\n",
      "[epoch:10,batch:419]:acc: 0.880655,loss:0.279917\n",
      "[epoch:10,batch:449]:acc: 0.881319,loss:0.278980\n",
      "[epoch:10,batch:479]:acc: 0.881901,loss:0.276955\n",
      "[epoch:10,batch:509]:acc: 0.881250,loss:0.280273\n",
      "[epoch:10,batch:539]:acc: 0.881771,loss:0.280491\n",
      "[epoch:10,batch:569]:acc: 0.881579,loss:0.281361\n",
      "[epoch:10,batch:599]:acc: 0.880833,loss:0.282830\n",
      "[epoch:10,batch:599]: val_loss:0.418002,val_acc:0.849526,val_total:4539\n",
      "[epoch:10,batch:629]:acc: 0.880853,loss:0.282980\n",
      "[epoch:10,batch:659]:acc: 0.881203,loss:0.283095\n",
      "[epoch:10,batch:689]:acc: 0.881522,loss:0.282600\n",
      "[epoch:10,batch:719]:acc: 0.881727,loss:0.282700\n",
      "[epoch:10,batch:749]:acc: 0.882167,loss:0.281649\n",
      "[epoch:10,batch:779]:acc: 0.882171,loss:0.283252\n",
      "[epoch:10,batch:809]:acc: 0.881983,loss:0.283713\n",
      "[epoch:10,batch:839]:acc: 0.881622,loss:0.284197\n",
      "[epoch:10,batch:869]:acc: 0.881286,loss:0.285158\n",
      "[epoch:10,batch:899]:acc: 0.881111,loss:0.285133\n",
      "[epoch:10,batch:899]: val_loss:0.422468,val_acc:0.849967,val_total:4539\n",
      "[epoch:10,batch:929]:acc: 0.880847,loss:0.285125\n",
      "[epoch:10,batch:959]:acc: 0.880729,loss:0.286012\n",
      "[epoch:10,batch:989]:acc: 0.880398,loss:0.286751\n",
      "[epoch:10] :acc: 0.880411,loss:0.286599,lr:0.000020,patience:1\n",
      "[epoch:10]: val_loss:0.431021,val_acc:0.848865,\n",
      "Epoch 11/59\n",
      "----------\n",
      "loss has increased lr divide 10 lr now is :0.000004\n",
      "[epoch:11,batch:29]:acc: 0.885417,loss:0.305476\n",
      "[epoch:11,batch:59]:acc: 0.887500,loss:0.286512\n",
      "[epoch:11,batch:89]:acc: 0.880208,loss:0.291490\n",
      "[epoch:11,batch:119]:acc: 0.875521,loss:0.296822\n",
      "[epoch:11,batch:149]:acc: 0.875833,loss:0.297457\n",
      "[epoch:11,batch:179]:acc: 0.873785,loss:0.299706\n",
      "[epoch:11,batch:209]:acc: 0.876637,loss:0.292903\n",
      "[epoch:11,batch:239]:acc: 0.877865,loss:0.291058\n",
      "[epoch:11,batch:269]:acc: 0.877894,loss:0.290413\n",
      "[epoch:11,batch:299]:acc: 0.878646,loss:0.287626\n",
      "[epoch:11,batch:299]: val_loss:0.402290,val_acc:0.853492,val_total:4539\n",
      "[epoch:11,batch:329]:acc: 0.879356,loss:0.288614\n",
      "[epoch:11,batch:359]:acc: 0.878733,loss:0.289058\n",
      "[epoch:11,batch:389]:acc: 0.877965,loss:0.289188\n",
      "[epoch:11,batch:419]:acc: 0.876265,loss:0.292050\n",
      "[epoch:11,batch:449]:acc: 0.876806,loss:0.290705\n",
      "[epoch:11,batch:479]:acc: 0.876888,loss:0.291566\n",
      "[epoch:11,batch:509]:acc: 0.876593,loss:0.291269\n",
      "[epoch:11,batch:539]:acc: 0.876678,loss:0.290247\n",
      "[epoch:11,batch:569]:acc: 0.876645,loss:0.291056\n",
      "[epoch:11,batch:599]:acc: 0.876198,loss:0.291608\n",
      "[epoch:11,batch:599]: val_loss:0.404771,val_acc:0.857017,val_total:4539\n",
      "[epoch:11,batch:629]:acc: 0.876290,loss:0.292435\n",
      "[epoch:11,batch:659]:acc: 0.875994,loss:0.292120\n",
      "[epoch:11,batch:689]:acc: 0.876132,loss:0.291581\n",
      "[epoch:11,batch:719]:acc: 0.875998,loss:0.290660\n",
      "[epoch:11,batch:749]:acc: 0.875792,loss:0.290770\n",
      "[epoch:11,batch:779]:acc: 0.875841,loss:0.291442\n",
      "[epoch:11,batch:809]:acc: 0.875540,loss:0.291231\n",
      "[epoch:11,batch:839]:acc: 0.875595,loss:0.291141\n",
      "[epoch:11,batch:869]:acc: 0.875682,loss:0.291877\n",
      "[epoch:11,batch:899]:acc: 0.875764,loss:0.291679\n",
      "[epoch:11,batch:899]: val_loss:0.401423,val_acc:0.856136,val_total:4539\n",
      "[epoch:11,batch:929]:acc: 0.874933,loss:0.292653\n",
      "[epoch:11,batch:959]:acc: 0.874902,loss:0.293086\n",
      "[epoch:11,batch:989]:acc: 0.875505,loss:0.292245\n",
      "[epoch:11] :acc: 0.875461,loss:0.292087,lr:0.000004,patience:0\n",
      "[epoch:11]: val_loss:0.401411,val_acc:0.855695,\n",
      "save new model loss,now loss is  0.40141063928604126\n",
      "save new model acc,now acc is  tensor(0.8557, device='cuda:0')\n",
      "Epoch 12/59\n",
      "----------\n",
      "[epoch:12,batch:29]:acc: 0.873958,loss:0.307426\n",
      "[epoch:12,batch:59]:acc: 0.881771,loss:0.282626\n",
      "[epoch:12,batch:89]:acc: 0.881250,loss:0.282104\n",
      "[epoch:12,batch:119]:acc: 0.880208,loss:0.279582\n",
      "[epoch:12,batch:149]:acc: 0.883333,loss:0.275345\n",
      "[epoch:12,batch:179]:acc: 0.883681,loss:0.274634\n",
      "[epoch:12,batch:209]:acc: 0.883036,loss:0.277244\n",
      "[epoch:12,batch:239]:acc: 0.881641,loss:0.273619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:12,batch:269]:acc: 0.882292,loss:0.272554\n",
      "[epoch:12,batch:299]:acc: 0.880833,loss:0.272900\n",
      "[epoch:12,batch:299]: val_loss:0.405588,val_acc:0.853712,val_total:4539\n",
      "[epoch:12,batch:329]:acc: 0.881345,loss:0.274671\n",
      "[epoch:12,batch:359]:acc: 0.882205,loss:0.273312\n",
      "[epoch:12,batch:389]:acc: 0.882372,loss:0.274121\n",
      "[epoch:12,batch:419]:acc: 0.883110,loss:0.273723\n",
      "[epoch:12,batch:449]:acc: 0.883333,loss:0.273373\n",
      "[epoch:12,batch:479]:acc: 0.883398,loss:0.274222\n",
      "[epoch:12,batch:509]:acc: 0.883578,loss:0.274083\n",
      "[epoch:12,batch:539]:acc: 0.883275,loss:0.273001\n",
      "[epoch:12,batch:569]:acc: 0.882456,loss:0.273727\n",
      "[epoch:12,batch:599]:acc: 0.882760,loss:0.275466\n",
      "[epoch:12,batch:599]: val_loss:0.404142,val_acc:0.856576,val_total:4539\n",
      "[epoch:12,batch:629]:acc: 0.883383,loss:0.274488\n",
      "[epoch:12,batch:659]:acc: 0.882860,loss:0.274877\n",
      "[epoch:12,batch:689]:acc: 0.881975,loss:0.276958\n",
      "[epoch:12,batch:719]:acc: 0.882118,loss:0.277353\n",
      "[epoch:12,batch:749]:acc: 0.882042,loss:0.278993\n",
      "[epoch:12,batch:779]:acc: 0.881891,loss:0.279713\n",
      "[epoch:12,batch:809]:acc: 0.882292,loss:0.279946\n",
      "[epoch:12,batch:839]:acc: 0.882217,loss:0.279930\n",
      "[epoch:12,batch:869]:acc: 0.881825,loss:0.280818\n",
      "[epoch:12,batch:899]:acc: 0.881910,loss:0.281925\n",
      "[epoch:12,batch:899]: val_loss:0.400205,val_acc:0.854594,val_total:4539\n",
      "[epoch:12,batch:929]:acc: 0.881821,loss:0.281376\n",
      "[epoch:12,batch:959]:acc: 0.882194,loss:0.280581\n",
      "[epoch:12,batch:989]:acc: 0.882197,loss:0.280795\n",
      "[epoch:12] :acc: 0.882082,loss:0.281097,lr:0.000004,patience:0\n",
      "[epoch:12]: val_loss:0.402004,val_acc:0.855695,\n",
      "Epoch 13/59\n",
      "----------\n",
      "[epoch:13,batch:29]:acc: 0.889583,loss:0.255078\n",
      "[epoch:13,batch:59]:acc: 0.885417,loss:0.268815\n",
      "[epoch:13,batch:89]:acc: 0.884375,loss:0.263828\n",
      "[epoch:13,batch:119]:acc: 0.882031,loss:0.267295\n",
      "[epoch:13,batch:149]:acc: 0.882500,loss:0.268041\n",
      "[epoch:13,batch:179]:acc: 0.885417,loss:0.263379\n",
      "[epoch:13,batch:209]:acc: 0.884673,loss:0.267764\n",
      "[epoch:13,batch:239]:acc: 0.883984,loss:0.269245\n",
      "[epoch:13,batch:269]:acc: 0.884259,loss:0.270458\n",
      "[epoch:13,batch:299]:acc: 0.883125,loss:0.272464\n",
      "[epoch:13,batch:299]: val_loss:0.399760,val_acc:0.855034,val_total:4539\n",
      "[epoch:13,batch:329]:acc: 0.882386,loss:0.274826\n",
      "[epoch:13,batch:359]:acc: 0.882205,loss:0.275270\n",
      "[epoch:13,batch:389]:acc: 0.881410,loss:0.274654\n",
      "[epoch:13,batch:419]:acc: 0.882366,loss:0.271702\n",
      "[epoch:13,batch:449]:acc: 0.882778,loss:0.271477\n",
      "[epoch:13,batch:479]:acc: 0.882617,loss:0.270153\n",
      "[epoch:13,batch:509]:acc: 0.881985,loss:0.270614\n",
      "[epoch:13,batch:539]:acc: 0.883044,loss:0.268976\n",
      "[epoch:13,batch:569]:acc: 0.883333,loss:0.268761\n",
      "[epoch:13,batch:599]:acc: 0.882292,loss:0.269364\n",
      "[epoch:13,batch:599]: val_loss:0.403409,val_acc:0.854373,val_total:4539\n",
      "[epoch:13,batch:629]:acc: 0.882589,loss:0.269152\n",
      "[epoch:13,batch:659]:acc: 0.882102,loss:0.267778\n",
      "[epoch:13,batch:689]:acc: 0.882654,loss:0.267297\n",
      "[epoch:13,batch:719]:acc: 0.882986,loss:0.266645\n",
      "[epoch:13,batch:749]:acc: 0.882375,loss:0.267593\n",
      "[epoch:13,batch:779]:acc: 0.882612,loss:0.267492\n",
      "[epoch:13,batch:809]:acc: 0.882716,loss:0.267014\n",
      "[epoch:13,batch:839]:acc: 0.882403,loss:0.266768\n",
      "[epoch:13,batch:869]:acc: 0.882579,loss:0.266592\n",
      "[epoch:13,batch:899]:acc: 0.883056,loss:0.265807\n",
      "[epoch:13,batch:899]: val_loss:0.409319,val_acc:0.858339,val_total:4539\n",
      "[epoch:13,batch:929]:acc: 0.882460,loss:0.266163\n",
      "[epoch:13,batch:959]:acc: 0.882682,loss:0.265955\n",
      "[epoch:13,batch:989]:acc: 0.882039,loss:0.267183\n",
      "[epoch:13] :acc: 0.881956,loss:0.267844,lr:0.000004,patience:1\n",
      "[epoch:13]: val_loss:0.413121,val_acc:0.852611,\n",
      "Epoch 14/59\n",
      "----------\n",
      "loss has increased lr divide 10 lr now is :0.000001\n",
      "[epoch:14,batch:29]:acc: 0.889583,loss:0.251883\n",
      "[epoch:14,batch:59]:acc: 0.885938,loss:0.246527\n",
      "[epoch:14,batch:89]:acc: 0.882639,loss:0.264658\n",
      "[epoch:14,batch:119]:acc: 0.884896,loss:0.263429\n",
      "[epoch:14,batch:149]:acc: 0.880625,loss:0.274118\n",
      "[epoch:14,batch:179]:acc: 0.880035,loss:0.278283\n",
      "[epoch:14,batch:209]:acc: 0.877083,loss:0.283767\n",
      "[epoch:14,batch:239]:acc: 0.877604,loss:0.280195\n",
      "[epoch:14,batch:269]:acc: 0.878125,loss:0.279005\n",
      "[epoch:14,batch:299]:acc: 0.877708,loss:0.284170\n",
      "[epoch:14,batch:299]: val_loss:0.399022,val_acc:0.856576,val_total:4539\n",
      "[epoch:14,batch:329]:acc: 0.877557,loss:0.282937\n",
      "[epoch:14,batch:359]:acc: 0.877257,loss:0.284170\n",
      "[epoch:14,batch:389]:acc: 0.877484,loss:0.282532\n",
      "[epoch:14,batch:419]:acc: 0.877530,loss:0.284741\n",
      "[epoch:14,batch:449]:acc: 0.877847,loss:0.282980\n",
      "[epoch:14,batch:479]:acc: 0.878255,loss:0.282322\n",
      "[epoch:14,batch:509]:acc: 0.878615,loss:0.280705\n",
      "[epoch:14,batch:539]:acc: 0.878183,loss:0.281016\n",
      "[epoch:14,batch:569]:acc: 0.878509,loss:0.280804\n",
      "[epoch:14,batch:599]:acc: 0.878125,loss:0.280208\n",
      "[epoch:14,batch:599]: val_loss:0.402601,val_acc:0.854153,val_total:4539\n",
      "[epoch:14,batch:629]:acc: 0.878522,loss:0.279524\n",
      "[epoch:14,batch:659]:acc: 0.878693,loss:0.278615\n",
      "[epoch:14,batch:689]:acc: 0.879257,loss:0.277801\n",
      "[epoch:14,batch:719]:acc: 0.879514,loss:0.278373\n",
      "[epoch:14,batch:749]:acc: 0.879667,loss:0.277458\n",
      "[epoch:14,batch:779]:acc: 0.879487,loss:0.277621\n",
      "[epoch:14,batch:809]:acc: 0.879475,loss:0.277632\n",
      "[epoch:14,batch:839]:acc: 0.878757,loss:0.279533\n",
      "[epoch:14,batch:869]:acc: 0.878879,loss:0.280350\n",
      "[epoch:14,batch:899]:acc: 0.879479,loss:0.279005\n",
      "[epoch:14,batch:899]: val_loss:0.405810,val_acc:0.857237,val_total:4539\n",
      "[epoch:14,batch:929]:acc: 0.879167,loss:0.280199\n",
      "[epoch:14,batch:959]:acc: 0.879525,loss:0.279850\n",
      "[epoch:14,batch:989]:acc: 0.879640,loss:0.279633\n",
      "[epoch:14] :acc: 0.879654,loss:0.280030,lr:0.000001,patience:0\n",
      "[epoch:14]: val_loss:0.397841,val_acc:0.856136,\n",
      "save new model loss,now loss is  0.397840678691864\n",
      "save new model acc,now acc is  tensor(0.8561, device='cuda:0')\n",
      "Epoch 15/59\n",
      "----------\n",
      "[epoch:15,batch:29]:acc: 0.863542,loss:0.317351\n",
      "[epoch:15,batch:59]:acc: 0.871354,loss:0.291914\n",
      "[epoch:15,batch:89]:acc: 0.872569,loss:0.301600\n",
      "[epoch:15,batch:119]:acc: 0.877604,loss:0.294314\n",
      "[epoch:15,batch:149]:acc: 0.880625,loss:0.290938\n",
      "[epoch:15,batch:179]:acc: 0.882118,loss:0.281831\n",
      "[epoch:15,batch:209]:acc: 0.881994,loss:0.282321\n",
      "[epoch:15,batch:239]:acc: 0.882161,loss:0.282216\n",
      "[epoch:15,batch:269]:acc: 0.880903,loss:0.284497\n",
      "[epoch:15,batch:299]:acc: 0.880417,loss:0.283901\n",
      "[epoch:15,batch:299]: val_loss:0.400136,val_acc:0.854594,val_total:4539\n",
      "[epoch:15,batch:329]:acc: 0.880208,loss:0.284354\n",
      "[epoch:15,batch:359]:acc: 0.880295,loss:0.283222\n",
      "[epoch:15,batch:389]:acc: 0.880369,loss:0.283927\n",
      "[epoch:15,batch:419]:acc: 0.881324,loss:0.281733\n",
      "[epoch:15,batch:449]:acc: 0.880903,loss:0.282338\n",
      "[epoch:15,batch:479]:acc: 0.880729,loss:0.281391\n",
      "[epoch:15,batch:509]:acc: 0.880882,loss:0.280258\n",
      "[epoch:15,batch:539]:acc: 0.880208,loss:0.279773\n",
      "[epoch:15,batch:569]:acc: 0.879989,loss:0.279363\n",
      "[epoch:15,batch:599]:acc: 0.880365,loss:0.278369\n",
      "[epoch:15,batch:599]: val_loss:0.402032,val_acc:0.854373,val_total:4539\n",
      "[epoch:15,batch:629]:acc: 0.880159,loss:0.279823\n",
      "[epoch:15,batch:659]:acc: 0.880019,loss:0.279694\n",
      "[epoch:15,batch:689]:acc: 0.879801,loss:0.280001\n",
      "[epoch:15,batch:719]:acc: 0.880165,loss:0.279663\n",
      "[epoch:15,batch:749]:acc: 0.880292,loss:0.279862\n",
      "[epoch:15,batch:779]:acc: 0.880449,loss:0.278793\n",
      "[epoch:15,batch:809]:acc: 0.881096,loss:0.277870\n",
      "[epoch:15,batch:839]:acc: 0.880729,loss:0.279052\n",
      "[epoch:15,batch:869]:acc: 0.880352,loss:0.279898\n",
      "[epoch:15,batch:899]:acc: 0.880694,loss:0.279027\n",
      "[epoch:15,batch:899]: val_loss:0.400828,val_acc:0.856797,val_total:4539\n",
      "[epoch:15,batch:929]:acc: 0.880914,loss:0.278244\n",
      "[epoch:15,batch:959]:acc: 0.881608,loss:0.277506\n",
      "[epoch:15,batch:989]:acc: 0.881503,loss:0.277623\n",
      "[epoch:15] :acc: 0.881452,loss:0.278208,lr:0.000001,patience:0\n",
      "[epoch:15]: val_loss:0.403395,val_acc:0.857678,\n",
      "save new model acc,now acc is  tensor(0.8577, device='cuda:0')\n",
      "Epoch 16/59\n",
      "----------\n",
      "[epoch:16,batch:29]:acc: 0.875000,loss:0.272289\n",
      "[epoch:16,batch:59]:acc: 0.874479,loss:0.272300\n",
      "[epoch:16,batch:89]:acc: 0.880208,loss:0.267838\n",
      "[epoch:16,batch:119]:acc: 0.885417,loss:0.264765\n",
      "[epoch:16,batch:149]:acc: 0.887083,loss:0.267680\n",
      "[epoch:16,batch:179]:acc: 0.886632,loss:0.269873\n",
      "[epoch:16,batch:209]:acc: 0.884821,loss:0.274806\n",
      "[epoch:16,batch:239]:acc: 0.885026,loss:0.271278\n",
      "[epoch:16,batch:269]:acc: 0.884028,loss:0.275700\n",
      "[epoch:16,batch:299]:acc: 0.884271,loss:0.273594\n",
      "[epoch:16,batch:299]: val_loss:0.403145,val_acc:0.853712,val_total:4539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:16,batch:329]:acc: 0.886080,loss:0.270472\n",
      "[epoch:16,batch:359]:acc: 0.884896,loss:0.271295\n",
      "[epoch:16,batch:389]:acc: 0.884455,loss:0.272247\n",
      "[epoch:16,batch:419]:acc: 0.884226,loss:0.270969\n",
      "[epoch:16,batch:449]:acc: 0.884653,loss:0.270167\n",
      "[epoch:16,batch:479]:acc: 0.884375,loss:0.271685\n",
      "[epoch:16,batch:509]:acc: 0.884804,loss:0.270916\n",
      "[epoch:16,batch:539]:acc: 0.885012,loss:0.270483\n",
      "[epoch:16,batch:569]:acc: 0.884923,loss:0.270051\n",
      "[epoch:16,batch:599]:acc: 0.885000,loss:0.270463\n",
      "[epoch:16,batch:599]: val_loss:0.401665,val_acc:0.857898,val_total:4539\n",
      "[epoch:16,batch:629]:acc: 0.884325,loss:0.272445\n",
      "[epoch:16,batch:659]:acc: 0.884754,loss:0.271665\n",
      "[epoch:16,batch:689]:acc: 0.884239,loss:0.271983\n",
      "[epoch:16,batch:719]:acc: 0.884852,loss:0.270887\n",
      "[epoch:16,batch:749]:acc: 0.884625,loss:0.272414\n",
      "[epoch:16,batch:779]:acc: 0.884415,loss:0.272090\n",
      "[epoch:16,batch:809]:acc: 0.884375,loss:0.271081\n",
      "[epoch:16,batch:839]:acc: 0.884487,loss:0.270671\n",
      "[epoch:16,batch:869]:acc: 0.884950,loss:0.269589\n",
      "[epoch:16,batch:899]:acc: 0.884722,loss:0.270184\n",
      "[epoch:16,batch:899]: val_loss:0.398448,val_acc:0.857678,val_total:4539\n",
      "[epoch:16,batch:929]:acc: 0.884509,loss:0.270327\n",
      "[epoch:16,batch:959]:acc: 0.884603,loss:0.269479\n",
      "[epoch:16,batch:989]:acc: 0.884280,loss:0.269615\n",
      "[epoch:16] :acc: 0.884258,loss:0.269726,lr:0.000001,patience:1\n",
      "[epoch:16]: val_loss:0.406634,val_acc:0.855034,\n",
      "Epoch 17/59\n",
      "----------\n",
      "loss has increased lr divide 10 lr now is :0.000000\n",
      "[epoch:17,batch:29]:acc: 0.881250,loss:0.299511\n",
      "[epoch:17,batch:59]:acc: 0.887500,loss:0.271632\n",
      "[epoch:17,batch:89]:acc: 0.886806,loss:0.262773\n",
      "[epoch:17,batch:119]:acc: 0.888021,loss:0.270780\n",
      "[epoch:17,batch:149]:acc: 0.888542,loss:0.272473\n",
      "[epoch:17,batch:179]:acc: 0.890104,loss:0.267444\n",
      "[epoch:17,batch:209]:acc: 0.887798,loss:0.266538\n",
      "[epoch:17,batch:239]:acc: 0.885547,loss:0.272426\n",
      "[epoch:17,batch:269]:acc: 0.883912,loss:0.275696\n",
      "[epoch:17,batch:299]:acc: 0.884167,loss:0.278481\n",
      "[epoch:17,batch:299]: val_loss:0.401622,val_acc:0.854594,val_total:4539\n",
      "[epoch:17,batch:329]:acc: 0.882860,loss:0.281100\n",
      "[epoch:17,batch:359]:acc: 0.882118,loss:0.281704\n",
      "[epoch:17,batch:389]:acc: 0.881891,loss:0.283329\n",
      "[epoch:17,batch:419]:acc: 0.881845,loss:0.282812\n",
      "[epoch:17,batch:449]:acc: 0.882292,loss:0.283191\n",
      "[epoch:17,batch:479]:acc: 0.882096,loss:0.283791\n",
      "[epoch:17,batch:509]:acc: 0.881740,loss:0.284064\n",
      "[epoch:17,batch:539]:acc: 0.882697,loss:0.283418\n",
      "[epoch:17,batch:569]:acc: 0.883004,loss:0.282451\n",
      "[epoch:17,batch:599]:acc: 0.883073,loss:0.281870\n",
      "[epoch:17,batch:599]: val_loss:0.401747,val_acc:0.855475,val_total:4539\n",
      "[epoch:17,batch:629]:acc: 0.882192,loss:0.282396\n",
      "[epoch:17,batch:659]:acc: 0.882434,loss:0.282146\n",
      "[epoch:17,batch:689]:acc: 0.882699,loss:0.281725\n",
      "[epoch:17,batch:719]:acc: 0.882292,loss:0.280475\n",
      "[epoch:17,batch:749]:acc: 0.882292,loss:0.280804\n",
      "[epoch:17,batch:779]:acc: 0.883333,loss:0.278775\n",
      "[epoch:17,batch:809]:acc: 0.883063,loss:0.279763\n",
      "[epoch:17,batch:839]:acc: 0.882850,loss:0.279846\n",
      "[epoch:17,batch:869]:acc: 0.882615,loss:0.278990\n",
      "[epoch:17,batch:899]:acc: 0.882361,loss:0.278314\n",
      "[epoch:17,batch:899]: val_loss:0.401590,val_acc:0.855475,val_total:4539\n",
      "[epoch:17,batch:929]:acc: 0.882124,loss:0.278542\n",
      "[epoch:17,batch:959]:acc: 0.882161,loss:0.278185\n",
      "[epoch:17,batch:989]:acc: 0.882039,loss:0.278319\n",
      "[epoch:17] :acc: 0.881988,loss:0.279080,lr:0.000000,patience:0\n",
      "[epoch:17]: val_loss:0.400540,val_acc:0.857017,\n",
      "Epoch 18/59\n",
      "----------\n",
      "[epoch:18,batch:29]:acc: 0.881250,loss:0.273832\n",
      "[epoch:18,batch:59]:acc: 0.883854,loss:0.267356\n",
      "[epoch:18,batch:89]:acc: 0.886458,loss:0.264782\n",
      "[epoch:18,batch:119]:acc: 0.885677,loss:0.271704\n",
      "[epoch:18,batch:149]:acc: 0.884583,loss:0.276007\n",
      "[epoch:18,batch:179]:acc: 0.885243,loss:0.281239\n",
      "[epoch:18,batch:209]:acc: 0.884970,loss:0.278013\n",
      "[epoch:18,batch:239]:acc: 0.886719,loss:0.278052\n",
      "[epoch:18,batch:269]:acc: 0.887269,loss:0.275718\n",
      "[epoch:18,batch:299]:acc: 0.887500,loss:0.273984\n",
      "[epoch:18,batch:299]: val_loss:0.398636,val_acc:0.858559,val_total:4539\n",
      "[epoch:18,batch:329]:acc: 0.888258,loss:0.274298\n",
      "[epoch:18,batch:359]:acc: 0.888628,loss:0.272032\n",
      "[epoch:18,batch:389]:acc: 0.888381,loss:0.271033\n",
      "[epoch:18,batch:419]:acc: 0.887649,loss:0.269886\n",
      "[epoch:18,batch:449]:acc: 0.887222,loss:0.271112\n",
      "[epoch:18,batch:479]:acc: 0.887109,loss:0.271182\n",
      "[epoch:18,batch:509]:acc: 0.886581,loss:0.272284\n",
      "[epoch:18,batch:539]:acc: 0.885532,loss:0.273561\n",
      "[epoch:18,batch:569]:acc: 0.885471,loss:0.273985\n",
      "[epoch:18,batch:599]:acc: 0.884948,loss:0.273599\n",
      "[epoch:18,batch:599]: val_loss:0.400510,val_acc:0.855034,val_total:4539\n",
      "[epoch:18,batch:629]:acc: 0.884970,loss:0.272772\n",
      "[epoch:18,batch:659]:acc: 0.885511,loss:0.271529\n",
      "[epoch:18,batch:689]:acc: 0.885960,loss:0.270616\n",
      "[epoch:18,batch:719]:acc: 0.884722,loss:0.272420\n",
      "[epoch:18,batch:749]:acc: 0.884500,loss:0.271864\n",
      "[epoch:18,batch:779]:acc: 0.884054,loss:0.271860\n",
      "[epoch:18,batch:809]:acc: 0.884221,loss:0.271102\n",
      "[epoch:18,batch:839]:acc: 0.883966,loss:0.270532\n",
      "[epoch:18,batch:869]:acc: 0.883477,loss:0.272207\n",
      "[epoch:18,batch:899]:acc: 0.883194,loss:0.272322\n",
      "[epoch:18,batch:899]: val_loss:0.395301,val_acc:0.857237,val_total:4539\n",
      "[epoch:18,batch:929]:acc: 0.883367,loss:0.272638\n",
      "[epoch:18,batch:959]:acc: 0.883008,loss:0.273653\n",
      "[epoch:18,batch:989]:acc: 0.882986,loss:0.274076\n",
      "[epoch:18] :acc: 0.882997,loss:0.273933,lr:0.000000,patience:1\n",
      "[epoch:18]: val_loss:0.398227,val_acc:0.856797,\n",
      "Epoch 19/59\n",
      "----------\n",
      "loss has increased lr divide 10 lr now is :0.000000\n",
      "[epoch:19,batch:29]:acc: 0.866667,loss:0.344862\n",
      "[epoch:19,batch:59]:acc: 0.876042,loss:0.301911\n",
      "[epoch:19,batch:89]:acc: 0.873958,loss:0.300505\n",
      "[epoch:19,batch:119]:acc: 0.874479,loss:0.303942\n",
      "[epoch:19,batch:149]:acc: 0.876875,loss:0.294515\n"
     ]
    }
   ],
   "source": [
    "train(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainWithRawData(path,epochNum):\n",
    "    train_dataset=MyDataSet(json_Description=ANNOTATION_TRAIN,transform=preprocess(normalize_05,IMAGE_SIZE),path_pre=IMAGE_TRAIN_PRE)\n",
    "    val_dataset=MyDataSet(json_Description=ANNOTATION_VAL,transform=preprocess(normalize_05,IMAGE_SIZE),path_pre=IMAGE_VAL_PRE)\n",
    "    train_dataLoader=DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE,num_workers=16,shuffle=True)\n",
    "    val_dataLoader=DataLoader(dataset=val_dataset,batch_size=BATCH_SIZE,num_workers=1,shuffle=False)\n",
    "    model=getmodel()\n",
    "    criterion=nn.CrossEntropyLoss().cuda()\n",
    "    modelParams=torch.load(path)\n",
    "    model.load_state_dict(modelParams['state_dict'])\n",
    "    min_loss=modelParams['val_loss']\n",
    "    print('min_loss is :%f'%(min_loss))\n",
    "    print('val_correct is %f'%(modelParams['val_correct']))\n",
    "    min_acc=max(modelParams['val_correct'],0.81)\n",
    "    optinizerSave=modelParams['optimizer']\n",
    "    patience=0\n",
    "    lr=1e-4\n",
    "    momentum=0.9\n",
    "    beginepoch=modelParams['epoch']\n",
    "    for epoch in range(beginepoch,epochNum):\n",
    "        print('Epoch {}/{}'.format(epoch, epochNum - 1))\n",
    "        print('-' * 10)\n",
    "        if patience==3:\n",
    "            patience=0\n",
    "            model.load_state_dict(torch.load('../model/InceptionV3/'+date+'_loss_best.pth')['state_dict'])\n",
    "            lr=lr/5\n",
    "            print('lr desencd')\n",
    "        if epoch==beginepoch:\n",
    "            optimizer=torch.optim.SGD(params=model.parameters(),lr=lr,momentum=momentum)\n",
    "#             optimizer.load_state_dict(optinizerSave)\n",
    "#             lr=optimizer['lr']\n",
    "#             momentum=optimizer['momentum']\n",
    "            print('begin lr is ',lr)\n",
    "            \n",
    "        else:\n",
    "            optimizer=torch.optim.SGD(params=model.parameters(),lr=lr,momentum=momentum)\n",
    "                   \n",
    "        running_loss = RunningMean()\n",
    "        running_corrects = RunningMean()\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_dataLoader):\n",
    "            model.train(True)\n",
    "            n_batchsize=inputs.size(0)\n",
    "            inputs = Variable(inputs).cuda()\n",
    "            labels = Variable(labels).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            if isinstance(outputs,tuple):\n",
    "                loss=sum((criterion(o,labels)) for o in outputs)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "            running_loss.update(loss.item(),1)\n",
    "            running_corrects.update(torch.sum(preds == labels.data).data,n_batchsize)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx%30==29:\n",
    "                print('[epoch:%d,batch:%d]:acc: %f,loss:%f'%(epoch,batch_idx,running_corrects.value,running_loss.value))\n",
    "                if batch_idx%300==299: \n",
    "                    niter = epoch * len(train_dataset)/BATCH_SIZE + batch_idx\n",
    "                    lx,px=utils.predict(model,val_dataLoader)\n",
    "                    log_loss = criterion(Variable(px), Variable(lx))\n",
    "                    log_loss = log_loss.item()\n",
    "                    _, preds = torch.max(px, dim=1)\n",
    "                    accuracy = torch.mean((preds == lx).float())\n",
    "                    print('[epoch:%d,batch:%d]: val_loss:%f,val_acc:%f,val_total:%d'%(epoch,batch_idx,log_loss,accuracy,len(val_dataset)))\n",
    "                    if  log_loss < min_loss:\n",
    "                        utils.snapshot('../model/', 'InceptionV3', {\n",
    "                               'epoch': epoch + 1,\n",
    "                               'state_dict': model.state_dict(),\n",
    "                               'optimizer': optimizer.state_dict(),\n",
    "                               'val_loss': log_loss,\n",
    "                               'val_correct':accuracy })          \n",
    "\n",
    "                        min_loss=log_loss\n",
    "                        patience=0\n",
    "                        print('save new model loss,now loss is ',min_loss)\n",
    "\n",
    "                    if accuracy>min_acc:\n",
    "                        utils.snapshot('../model/', 'InceptionV3', {\n",
    "                               'epoch': epoch + 1,\n",
    "                               'state_dict': model.state_dict(),\n",
    "                               'optimizer': optimizer.state_dict(),\n",
    "                               'val_loss': log_loss,\n",
    "                               'val_correct':accuracy },key='acc') \n",
    "                        min_acc=accuracy\n",
    "                        print('save new model acc,now acc is ',min_acc)\n",
    "        print('[epoch:%d] :acc: %f,loss:%f,lr:%f,patience:%d'%(epoch,running_corrects.value,running_loss.value,lr,patience))         \n",
    "        lx,px=utils.predict(model,val_dataLoader)\n",
    "        log_loss = criterion(Variable(px), Variable(lx))\n",
    "        log_loss = log_loss.item()\n",
    "        _, preds = torch.max(px, dim=1)\n",
    "        accuracy = torch.mean((preds == lx).float())\n",
    "        print('[epoch:%d]: val_loss:%f,val_acc:%f,'%(epoch,log_loss,accuracy))\n",
    "        if  log_loss < min_loss:\n",
    "            utils.snapshot('../model/', 'InceptionV3', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy })          \n",
    "            patience = 0\n",
    "            min_loss=log_loss\n",
    "            print('save new model loss,now loss is ',min_loss)\n",
    "        else:\n",
    "            patience += 1\n",
    "        if accuracy>min_acc:\n",
    "            utils.snapshot('../model/', 'InceptionV3', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy },key='acc') \n",
    "            min_acc=accuracy\n",
    "            print('save new model acc,now acc is ',min_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] loading model... Done\n",
      "min_loss is :0.403395\n",
      "val_correct is 0.857678\n",
      "Epoch 16/59\n",
      "----------\n",
      "begin lr is  0.0001\n",
      "[epoch:16,batch:29]:acc: 0.881250,loss:0.267384\n",
      "[epoch:16,batch:59]:acc: 0.903646,loss:0.232065\n",
      "[epoch:16,batch:89]:acc: 0.911111,loss:0.216546\n",
      "[epoch:16,batch:119]:acc: 0.910937,loss:0.217283\n",
      "[epoch:16,batch:149]:acc: 0.907708,loss:0.219685\n",
      "[epoch:16,batch:179]:acc: 0.907639,loss:0.223879\n",
      "[epoch:16,batch:209]:acc: 0.907887,loss:0.222966\n",
      "[epoch:16,batch:239]:acc: 0.908854,loss:0.222858\n",
      "[epoch:16,batch:269]:acc: 0.907176,loss:0.226573\n",
      "[epoch:16,batch:299]:acc: 0.908229,loss:0.225940\n",
      "[epoch:16,batch:299]: val_loss:0.365122,val_acc:0.859220,val_total:4539\n",
      "save new model loss,now loss is  0.3651220500469208\n",
      "save new model acc,now acc is  tensor(0.8592, device='cuda:0')\n",
      "[epoch:16,batch:329]:acc: 0.906250,loss:0.228048\n",
      "[epoch:16,batch:359]:acc: 0.906424,loss:0.227143\n",
      "[epoch:16,batch:389]:acc: 0.906891,loss:0.225844\n",
      "[epoch:16,batch:419]:acc: 0.905878,loss:0.225585\n",
      "[epoch:16,batch:449]:acc: 0.905208,loss:0.227057\n",
      "[epoch:16,batch:479]:acc: 0.905794,loss:0.226599\n",
      "[epoch:16,batch:509]:acc: 0.906801,loss:0.226479\n",
      "[epoch:16,batch:539]:acc: 0.905440,loss:0.229179\n",
      "[epoch:16,batch:569]:acc: 0.905702,loss:0.229081\n",
      "[epoch:16,batch:599]:acc: 0.905260,loss:0.228554\n",
      "[epoch:16,batch:599]: val_loss:0.362742,val_acc:0.860762,val_total:4539\n",
      "save new model loss,now loss is  0.36274242401123047\n",
      "save new model acc,now acc is  tensor(0.8608, device='cuda:0')\n",
      "[epoch:16,batch:629]:acc: 0.905952,loss:0.227252\n",
      "[epoch:16,batch:659]:acc: 0.906297,loss:0.226007\n",
      "[epoch:16,batch:689]:acc: 0.906975,loss:0.224773\n",
      "[epoch:16,batch:719]:acc: 0.906771,loss:0.224356\n",
      "[epoch:16,batch:749]:acc: 0.906750,loss:0.223820\n",
      "[epoch:16,batch:779]:acc: 0.906530,loss:0.223928\n",
      "[epoch:16,batch:809]:acc: 0.906752,loss:0.223054\n",
      "[epoch:16,batch:839]:acc: 0.907031,loss:0.222742\n",
      "[epoch:16,batch:869]:acc: 0.907435,loss:0.222068\n",
      "[epoch:16,batch:899]:acc: 0.907500,loss:0.221535\n",
      "[epoch:16,batch:899]: val_loss:0.361666,val_acc:0.864508,val_total:4539\n",
      "save new model loss,now loss is  0.36166632175445557\n",
      "save new model acc,now acc is  tensor(0.8645, device='cuda:0')\n",
      "[epoch:16,batch:929]:acc: 0.907695,loss:0.221022\n",
      "[epoch:16,batch:959]:acc: 0.907813,loss:0.220956\n",
      "[epoch:16,batch:989]:acc: 0.907891,loss:0.221060\n",
      "[epoch:16] :acc: 0.907841,loss:0.221277,lr:0.000100,patience:0\n",
      "[epoch:16]: val_loss:0.361832,val_acc:0.863406,\n",
      "Epoch 17/59\n",
      "----------\n",
      "[epoch:17,batch:29]:acc: 0.905208,loss:0.209252\n",
      "[epoch:17,batch:59]:acc: 0.906771,loss:0.209070\n",
      "[epoch:17,batch:89]:acc: 0.914931,loss:0.197105\n",
      "[epoch:17,batch:119]:acc: 0.915885,loss:0.202860\n",
      "[epoch:17,batch:149]:acc: 0.917500,loss:0.197856\n",
      "[epoch:17,batch:179]:acc: 0.916493,loss:0.200435\n",
      "[epoch:17,batch:209]:acc: 0.914881,loss:0.201103\n",
      "[epoch:17,batch:239]:acc: 0.914323,loss:0.202554\n",
      "[epoch:17,batch:269]:acc: 0.912963,loss:0.203360\n",
      "[epoch:17,batch:299]:acc: 0.913125,loss:0.202208\n",
      "[epoch:17,batch:299]: val_loss:0.365201,val_acc:0.862304,val_total:4539\n",
      "[epoch:17,batch:329]:acc: 0.913258,loss:0.201212\n",
      "[epoch:17,batch:359]:acc: 0.914757,loss:0.198446\n",
      "[epoch:17,batch:389]:acc: 0.915465,loss:0.197229\n",
      "[epoch:17,batch:419]:acc: 0.914955,loss:0.197810\n",
      "[epoch:17,batch:449]:acc: 0.914931,loss:0.199188\n",
      "[epoch:17,batch:479]:acc: 0.914909,loss:0.199072\n",
      "[epoch:17,batch:509]:acc: 0.915564,loss:0.198208\n",
      "[epoch:17,batch:539]:acc: 0.916204,loss:0.197616\n",
      "[epoch:17,batch:569]:acc: 0.916393,loss:0.197373\n",
      "[epoch:17,batch:599]:acc: 0.916927,loss:0.196780\n",
      "[epoch:17,batch:599]: val_loss:0.362270,val_acc:0.862965,val_total:4539\n",
      "[epoch:17,batch:629]:acc: 0.917262,loss:0.196812\n",
      "[epoch:17,batch:659]:acc: 0.917519,loss:0.197196\n",
      "[epoch:17,batch:689]:acc: 0.918297,loss:0.195406\n",
      "[epoch:17,batch:719]:acc: 0.918359,loss:0.194582\n",
      "[epoch:17,batch:749]:acc: 0.919292,loss:0.192668\n",
      "[epoch:17,batch:779]:acc: 0.919471,loss:0.192354\n",
      "[epoch:17,batch:809]:acc: 0.919444,loss:0.192258\n",
      "[epoch:17,batch:839]:acc: 0.919568,loss:0.191784\n",
      "[epoch:17,batch:869]:acc: 0.919899,loss:0.191398\n",
      "[epoch:17,batch:899]:acc: 0.919583,loss:0.191445\n",
      "[epoch:17,batch:899]: val_loss:0.364538,val_acc:0.863186,val_total:4539\n",
      "[epoch:17,batch:929]:acc: 0.919758,loss:0.191668\n",
      "[epoch:17,batch:959]:acc: 0.919727,loss:0.191707\n",
      "[epoch:17,batch:989]:acc: 0.919571,loss:0.192312\n",
      "[epoch:17] :acc: 0.919570,loss:0.192693,lr:0.000100,patience:1\n",
      "[epoch:17]: val_loss:0.363910,val_acc:0.860762,\n",
      "Epoch 18/59\n",
      "----------\n",
      "[epoch:18,batch:29]:acc: 0.938542,loss:0.175053\n",
      "[epoch:18,batch:59]:acc: 0.933333,loss:0.176656\n",
      "[epoch:18,batch:89]:acc: 0.934028,loss:0.170045\n",
      "[epoch:18,batch:119]:acc: 0.932031,loss:0.170661\n",
      "[epoch:18,batch:149]:acc: 0.933750,loss:0.163916\n",
      "[epoch:18,batch:179]:acc: 0.932639,loss:0.164883\n",
      "[epoch:18,batch:209]:acc: 0.932887,loss:0.166401\n",
      "[epoch:18,batch:239]:acc: 0.932031,loss:0.168879\n",
      "[epoch:18,batch:269]:acc: 0.931134,loss:0.170379\n",
      "[epoch:18,batch:299]:acc: 0.930833,loss:0.171389\n",
      "[epoch:18,batch:299]: val_loss:0.365610,val_acc:0.861423,val_total:4539\n",
      "[epoch:18,batch:329]:acc: 0.930492,loss:0.170843\n",
      "[epoch:18,batch:359]:acc: 0.930903,loss:0.169544\n",
      "[epoch:18,batch:389]:acc: 0.930128,loss:0.169565\n",
      "[epoch:18,batch:419]:acc: 0.930655,loss:0.169314\n",
      "[epoch:18,batch:449]:acc: 0.930347,loss:0.170264\n",
      "[epoch:18,batch:479]:acc: 0.930143,loss:0.170768\n",
      "[epoch:18,batch:509]:acc: 0.931005,loss:0.169321\n",
      "[epoch:18,batch:539]:acc: 0.931134,loss:0.169017\n",
      "[epoch:18,batch:569]:acc: 0.931086,loss:0.169041\n",
      "[epoch:18,batch:599]:acc: 0.931198,loss:0.169046\n",
      "[epoch:18,batch:599]: val_loss:0.364414,val_acc:0.866270,val_total:4539\n",
      "save new model acc,now acc is  tensor(0.8663, device='cuda:0')\n",
      "[epoch:18,batch:629]:acc: 0.931151,loss:0.168756\n",
      "[epoch:18,batch:659]:acc: 0.931345,loss:0.168431\n",
      "[epoch:18,batch:689]:acc: 0.931703,loss:0.168744\n",
      "[epoch:18,batch:719]:acc: 0.931597,loss:0.169339\n",
      "[epoch:18,batch:749]:acc: 0.931458,loss:0.169324\n",
      "[epoch:18,batch:779]:acc: 0.931571,loss:0.170068\n",
      "[epoch:18,batch:809]:acc: 0.932137,loss:0.169333\n",
      "[epoch:18,batch:839]:acc: 0.932217,loss:0.168803\n",
      "[epoch:18,batch:869]:acc: 0.932364,loss:0.169328\n",
      "[epoch:18,batch:899]:acc: 0.932049,loss:0.169362\n",
      "[epoch:18,batch:899]: val_loss:0.363606,val_acc:0.864287,val_total:4539\n",
      "[epoch:18,batch:929]:acc: 0.931922,loss:0.169379\n",
      "[epoch:18,batch:959]:acc: 0.931836,loss:0.169590\n",
      "[epoch:18,batch:989]:acc: 0.931503,loss:0.169851\n",
      "[epoch:18] :acc: 0.931456,loss:0.170145,lr:0.000100,patience:2\n",
      "[epoch:18]: val_loss:0.368569,val_acc:0.860542,\n",
      "Epoch 19/59\n",
      "----------\n",
      "lr desencd\n",
      "[epoch:19,batch:29]:acc: 0.908333,loss:0.211598\n",
      "[epoch:19,batch:59]:acc: 0.915625,loss:0.204177\n",
      "[epoch:19,batch:89]:acc: 0.917361,loss:0.201199\n",
      "[epoch:19,batch:119]:acc: 0.915365,loss:0.204930\n",
      "[epoch:19,batch:149]:acc: 0.916042,loss:0.203031\n",
      "[epoch:19,batch:179]:acc: 0.915278,loss:0.202116\n",
      "[epoch:19,batch:209]:acc: 0.916815,loss:0.199707\n",
      "[epoch:19,batch:239]:acc: 0.914844,loss:0.200762\n",
      "[epoch:19,batch:269]:acc: 0.916782,loss:0.197881\n",
      "[epoch:19,batch:299]:acc: 0.917292,loss:0.198483\n",
      "[epoch:19,batch:299]: val_loss:0.359950,val_acc:0.864067,val_total:4539\n",
      "save new model loss,now loss is  0.35994985699653625\n",
      "[epoch:19,batch:329]:acc: 0.917898,loss:0.199835\n",
      "[epoch:19,batch:359]:acc: 0.918316,loss:0.197684\n",
      "[epoch:19,batch:389]:acc: 0.919551,loss:0.196136\n",
      "[epoch:19,batch:419]:acc: 0.919122,loss:0.195832\n",
      "[epoch:19,batch:449]:acc: 0.919375,loss:0.195845\n",
      "[epoch:19,batch:479]:acc: 0.919727,loss:0.195893\n",
      "[epoch:19,batch:509]:acc: 0.919485,loss:0.196789\n",
      "[epoch:19,batch:539]:acc: 0.919155,loss:0.197329\n",
      "[epoch:19,batch:569]:acc: 0.919189,loss:0.196963\n",
      "[epoch:19,batch:599]:acc: 0.919323,loss:0.196717\n",
      "[epoch:19,batch:599]: val_loss:0.362971,val_acc:0.864287,val_total:4539\n",
      "[epoch:19,batch:629]:acc: 0.919147,loss:0.196709\n",
      "[epoch:19,batch:659]:acc: 0.919366,loss:0.196420\n",
      "[epoch:19,batch:689]:acc: 0.919656,loss:0.195969\n",
      "[epoch:19,batch:719]:acc: 0.920182,loss:0.195346\n",
      "[epoch:19,batch:749]:acc: 0.920333,loss:0.195000\n",
      "[epoch:19,batch:779]:acc: 0.920793,loss:0.194304\n",
      "[epoch:19,batch:809]:acc: 0.920756,loss:0.193767\n",
      "[epoch:19,batch:839]:acc: 0.921094,loss:0.192627\n",
      "[epoch:19,batch:869]:acc: 0.921731,loss:0.191593\n",
      "[epoch:19,batch:899]:acc: 0.922049,loss:0.190797\n",
      "[epoch:19,batch:899]: val_loss:0.361850,val_acc:0.863406,val_total:4539\n",
      "[epoch:19,batch:929]:acc: 0.922144,loss:0.190254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:19,batch:959]:acc: 0.922266,loss:0.189728\n",
      "[epoch:19,batch:989]:acc: 0.922317,loss:0.189579\n",
      "[epoch:19] :acc: 0.922218,loss:0.189767,lr:0.000020,patience:0\n",
      "[epoch:19]: val_loss:0.361762,val_acc:0.864067,\n",
      "Epoch 20/59\n",
      "----------\n",
      "[epoch:20,batch:29]:acc: 0.898958,loss:0.244552\n",
      "[epoch:20,batch:59]:acc: 0.909375,loss:0.232781\n",
      "[epoch:20,batch:89]:acc: 0.912500,loss:0.219499\n",
      "[epoch:20,batch:119]:acc: 0.909635,loss:0.223318\n",
      "[epoch:20,batch:149]:acc: 0.907500,loss:0.225994\n",
      "[epoch:20,batch:179]:acc: 0.905729,loss:0.225317\n",
      "[epoch:20,batch:209]:acc: 0.903571,loss:0.228768\n",
      "[epoch:20,batch:239]:acc: 0.906380,loss:0.224709\n",
      "[epoch:20,batch:269]:acc: 0.905440,loss:0.227514\n",
      "[epoch:20,batch:299]:acc: 0.903542,loss:0.230220\n",
      "[epoch:20,batch:299]: val_loss:0.362256,val_acc:0.862304,val_total:4539\n",
      "[epoch:20,batch:329]:acc: 0.902178,loss:0.232401\n",
      "[epoch:20,batch:359]:acc: 0.902257,loss:0.232799\n",
      "[epoch:20,batch:389]:acc: 0.901522,loss:0.234222\n",
      "[epoch:20,batch:419]:acc: 0.901265,loss:0.234360\n",
      "[epoch:20,batch:449]:acc: 0.901250,loss:0.234814\n",
      "[epoch:20,batch:479]:acc: 0.901172,loss:0.234984\n",
      "[epoch:20,batch:509]:acc: 0.901532,loss:0.233403\n",
      "[epoch:20,batch:539]:acc: 0.902199,loss:0.232485\n",
      "[epoch:20,batch:569]:acc: 0.902467,loss:0.232065\n",
      "[epoch:20,batch:599]:acc: 0.901823,loss:0.233085\n",
      "[epoch:20,batch:599]: val_loss:0.361549,val_acc:0.866490,val_total:4539\n",
      "save new model acc,now acc is  tensor(0.8665, device='cuda:0')\n",
      "[epoch:20,batch:629]:acc: 0.901835,loss:0.233245\n",
      "[epoch:20,batch:659]:acc: 0.900710,loss:0.234144\n",
      "[epoch:20,batch:689]:acc: 0.900498,loss:0.234398\n",
      "[epoch:20,batch:719]:acc: 0.900564,loss:0.233890\n",
      "[epoch:20,batch:749]:acc: 0.900833,loss:0.232960\n",
      "[epoch:20,batch:779]:acc: 0.900321,loss:0.233688\n",
      "[epoch:20,batch:809]:acc: 0.899614,loss:0.235118\n",
      "[epoch:20,batch:839]:acc: 0.899591,loss:0.235419\n",
      "[epoch:20,batch:869]:acc: 0.899425,loss:0.236083\n",
      "[epoch:20,batch:899]:acc: 0.899722,loss:0.235708\n",
      "[epoch:20,batch:899]: val_loss:0.360911,val_acc:0.864948,val_total:4539\n",
      "[epoch:20,batch:929]:acc: 0.899597,loss:0.235716\n",
      "[epoch:20,batch:959]:acc: 0.899674,loss:0.235785\n",
      "[epoch:20,batch:989]:acc: 0.899211,loss:0.235910\n",
      "[epoch:20] :acc: 0.899202,loss:0.236738,lr:0.000020,patience:1\n",
      "[epoch:20]: val_loss:0.361262,val_acc:0.863626,\n",
      "Epoch 21/59\n",
      "----------\n",
      "[epoch:21,batch:29]:acc: 0.901042,loss:0.237169\n",
      "[epoch:21,batch:59]:acc: 0.906771,loss:0.226190\n",
      "[epoch:21,batch:89]:acc: 0.904167,loss:0.223902\n",
      "[epoch:21,batch:119]:acc: 0.903646,loss:0.225312\n",
      "[epoch:21,batch:149]:acc: 0.904375,loss:0.222796\n",
      "[epoch:21,batch:179]:acc: 0.904514,loss:0.222205\n",
      "[epoch:21,batch:209]:acc: 0.905208,loss:0.221870\n",
      "[epoch:21,batch:239]:acc: 0.906120,loss:0.220996\n",
      "[epoch:21,batch:269]:acc: 0.904745,loss:0.224374\n",
      "[epoch:21,batch:299]:acc: 0.903229,loss:0.227960\n",
      "[epoch:21,batch:299]: val_loss:0.362337,val_acc:0.862084,val_total:4539\n",
      "[epoch:21,batch:329]:acc: 0.901610,loss:0.231916\n",
      "[epoch:21,batch:359]:acc: 0.901563,loss:0.232155\n",
      "[epoch:21,batch:389]:acc: 0.901202,loss:0.231695\n",
      "[epoch:21,batch:419]:acc: 0.901711,loss:0.232229\n",
      "[epoch:21,batch:449]:acc: 0.902014,loss:0.232914\n",
      "[epoch:21,batch:479]:acc: 0.901758,loss:0.233662\n",
      "[epoch:21,batch:509]:acc: 0.902206,loss:0.232377\n",
      "[epoch:21,batch:539]:acc: 0.901968,loss:0.232595\n",
      "[epoch:21,batch:569]:acc: 0.901974,loss:0.233030\n",
      "[epoch:21,batch:599]:acc: 0.901823,loss:0.233094\n",
      "[epoch:21,batch:599]: val_loss:0.358262,val_acc:0.860762,val_total:4539\n",
      "save new model loss,now loss is  0.35826241970062256\n",
      "[epoch:21,batch:629]:acc: 0.901438,loss:0.234065\n",
      "[epoch:21,batch:659]:acc: 0.901847,loss:0.233739\n",
      "[epoch:21,batch:689]:acc: 0.901676,loss:0.233527\n",
      "[epoch:21,batch:719]:acc: 0.901519,loss:0.233785\n",
      "[epoch:21,batch:749]:acc: 0.901292,loss:0.234245\n",
      "[epoch:21,batch:779]:acc: 0.901122,loss:0.234169\n",
      "[epoch:21,batch:809]:acc: 0.901235,loss:0.233913\n",
      "[epoch:21,batch:839]:acc: 0.901637,loss:0.233227\n",
      "[epoch:21,batch:869]:acc: 0.901545,loss:0.233541\n",
      "[epoch:21,batch:899]:acc: 0.901458,loss:0.233182\n",
      "[epoch:21,batch:899]: val_loss:0.362207,val_acc:0.862965,val_total:4539\n",
      "[epoch:21,batch:929]:acc: 0.901579,loss:0.232628\n",
      "[epoch:21,batch:959]:acc: 0.901758,loss:0.231668\n",
      "[epoch:21,batch:989]:acc: 0.901326,loss:0.231837\n",
      "[epoch:21] :acc: 0.901346,loss:0.231649,lr:0.000020,patience:0\n",
      "[epoch:21]: val_loss:0.363602,val_acc:0.862965,\n",
      "Epoch 22/59\n",
      "----------\n",
      "[epoch:22,batch:29]:acc: 0.914583,loss:0.220564\n",
      "[epoch:22,batch:59]:acc: 0.918750,loss:0.204635\n",
      "[epoch:22,batch:89]:acc: 0.917361,loss:0.202181\n",
      "[epoch:22,batch:119]:acc: 0.919531,loss:0.199592\n",
      "[epoch:22,batch:149]:acc: 0.921042,loss:0.198552\n",
      "[epoch:22,batch:179]:acc: 0.920486,loss:0.197935\n",
      "[epoch:22,batch:209]:acc: 0.919643,loss:0.198465\n",
      "[epoch:22,batch:239]:acc: 0.918750,loss:0.200518\n",
      "[epoch:22,batch:269]:acc: 0.919329,loss:0.199661\n",
      "[epoch:22,batch:299]:acc: 0.918958,loss:0.199222\n",
      "[epoch:22,batch:299]: val_loss:0.362574,val_acc:0.863186,val_total:4539\n",
      "[epoch:22,batch:329]:acc: 0.919602,loss:0.197701\n",
      "[epoch:22,batch:359]:acc: 0.919184,loss:0.197755\n",
      "[epoch:22,batch:389]:acc: 0.919151,loss:0.198526\n",
      "[epoch:22,batch:419]:acc: 0.918527,loss:0.199047\n",
      "[epoch:22,batch:449]:acc: 0.918194,loss:0.199182\n",
      "[epoch:22,batch:479]:acc: 0.918490,loss:0.198337\n",
      "[epoch:22,batch:509]:acc: 0.919424,loss:0.196976\n",
      "[epoch:22,batch:539]:acc: 0.919444,loss:0.196781\n",
      "[epoch:22,batch:569]:acc: 0.919463,loss:0.196441\n",
      "[epoch:22,batch:599]:acc: 0.919531,loss:0.196940\n",
      "[epoch:22,batch:599]: val_loss:0.360920,val_acc:0.862965,val_total:4539\n",
      "[epoch:22,batch:629]:acc: 0.919940,loss:0.195912\n",
      "[epoch:22,batch:659]:acc: 0.919697,loss:0.196664\n",
      "[epoch:22,batch:689]:acc: 0.919248,loss:0.197285\n",
      "[epoch:22,batch:719]:acc: 0.919314,loss:0.197090\n",
      "[epoch:22,batch:749]:acc: 0.919208,loss:0.196918\n",
      "[epoch:22,batch:779]:acc: 0.919631,loss:0.196598\n",
      "[epoch:22,batch:809]:acc: 0.919522,loss:0.196607\n",
      "[epoch:22,batch:839]:acc: 0.919420,loss:0.196420\n",
      "[epoch:22,batch:869]:acc: 0.919325,loss:0.196622\n",
      "[epoch:22,batch:899]:acc: 0.919306,loss:0.196669\n",
      "[epoch:22,batch:899]: val_loss:0.360718,val_acc:0.864067,val_total:4539\n",
      "[epoch:22,batch:929]:acc: 0.918952,loss:0.196543\n",
      "[epoch:22,batch:959]:acc: 0.918848,loss:0.196759\n",
      "[epoch:22,batch:989]:acc: 0.918403,loss:0.197455\n",
      "[epoch:22] :acc: 0.918372,loss:0.197831,lr:0.000020,patience:1\n",
      "[epoch:22]: val_loss:0.359884,val_acc:0.865169,\n",
      "Epoch 23/59\n",
      "----------\n",
      "[epoch:23,batch:29]:acc: 0.911458,loss:0.201515\n",
      "[epoch:23,batch:59]:acc: 0.922917,loss:0.185426\n",
      "[epoch:23,batch:89]:acc: 0.923611,loss:0.188571\n",
      "[epoch:23,batch:119]:acc: 0.922917,loss:0.188622\n",
      "[epoch:23,batch:149]:acc: 0.923542,loss:0.187553\n",
      "[epoch:23,batch:179]:acc: 0.923785,loss:0.186695\n",
      "[epoch:23,batch:209]:acc: 0.923661,loss:0.186069\n",
      "[epoch:23,batch:239]:acc: 0.922917,loss:0.186335\n",
      "[epoch:23,batch:269]:acc: 0.923495,loss:0.187645\n",
      "[epoch:23,batch:299]:acc: 0.922292,loss:0.188444\n",
      "[epoch:23,batch:299]: val_loss:0.362658,val_acc:0.864287,val_total:4539\n",
      "[epoch:23,batch:329]:acc: 0.921970,loss:0.190295\n",
      "[epoch:23,batch:359]:acc: 0.922569,loss:0.188591\n",
      "[epoch:23,batch:389]:acc: 0.921474,loss:0.190066\n",
      "[epoch:23,batch:419]:acc: 0.921652,loss:0.190229\n",
      "[epoch:23,batch:449]:acc: 0.921944,loss:0.191094\n",
      "[epoch:23,batch:479]:acc: 0.921484,loss:0.192077\n",
      "[epoch:23,batch:509]:acc: 0.922059,loss:0.191060\n",
      "[epoch:23,batch:539]:acc: 0.922338,loss:0.190574\n",
      "[epoch:23,batch:569]:acc: 0.922917,loss:0.189415\n",
      "[epoch:23,batch:599]:acc: 0.922813,loss:0.189232\n",
      "[epoch:23,batch:599]: val_loss:0.362566,val_acc:0.866711,val_total:4539\n",
      "save new model acc,now acc is  tensor(0.8667, device='cuda:0')\n",
      "[epoch:23,batch:629]:acc: 0.922123,loss:0.190225\n",
      "[epoch:23,batch:659]:acc: 0.922443,loss:0.189764\n",
      "[epoch:23,batch:689]:acc: 0.922101,loss:0.190701\n",
      "[epoch:23,batch:719]:acc: 0.922049,loss:0.190436\n",
      "[epoch:23,batch:749]:acc: 0.922000,loss:0.190352\n",
      "[epoch:23,batch:779]:acc: 0.922276,loss:0.189793\n",
      "[epoch:23,batch:809]:acc: 0.922338,loss:0.189145\n",
      "[epoch:23,batch:839]:acc: 0.922173,loss:0.189310\n",
      "[epoch:23,batch:869]:acc: 0.922342,loss:0.189339\n",
      "[epoch:23,batch:899]:acc: 0.922153,loss:0.189337\n",
      "[epoch:23,batch:899]: val_loss:0.360386,val_acc:0.866050,val_total:4539\n",
      "[epoch:23,batch:929]:acc: 0.921673,loss:0.189627\n",
      "[epoch:23,batch:959]:acc: 0.921257,loss:0.190113\n",
      "[epoch:23,batch:989]:acc: 0.920676,loss:0.191260\n",
      "[epoch:23] :acc: 0.920579,loss:0.191446,lr:0.000020,patience:2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:23]: val_loss:0.359866,val_acc:0.865169,\n",
      "Epoch 24/59\n",
      "----------\n",
      "lr desencd\n",
      "[epoch:24,batch:29]:acc: 0.917708,loss:0.197088\n",
      "[epoch:24,batch:59]:acc: 0.921875,loss:0.192556\n",
      "[epoch:24,batch:89]:acc: 0.923958,loss:0.190294\n",
      "[epoch:24,batch:119]:acc: 0.925260,loss:0.185550\n",
      "[epoch:24,batch:149]:acc: 0.924375,loss:0.185052\n",
      "[epoch:24,batch:179]:acc: 0.925521,loss:0.183981\n",
      "[epoch:24,batch:209]:acc: 0.924851,loss:0.184335\n",
      "[epoch:24,batch:239]:acc: 0.924219,loss:0.185699\n",
      "[epoch:24,batch:269]:acc: 0.924769,loss:0.184526\n",
      "[epoch:24,batch:299]:acc: 0.925208,loss:0.184945\n",
      "[epoch:24,batch:299]: val_loss:0.363393,val_acc:0.863406,val_total:4539\n",
      "[epoch:24,batch:329]:acc: 0.925852,loss:0.184372\n",
      "[epoch:24,batch:359]:acc: 0.925955,loss:0.184110\n",
      "[epoch:24,batch:389]:acc: 0.925721,loss:0.183978\n",
      "[epoch:24,batch:419]:acc: 0.925223,loss:0.184706\n",
      "[epoch:24,batch:449]:acc: 0.924653,loss:0.185568\n",
      "[epoch:24,batch:479]:acc: 0.924674,loss:0.186215\n",
      "[epoch:24,batch:509]:acc: 0.924203,loss:0.188180\n",
      "[epoch:24,batch:539]:acc: 0.924479,loss:0.188181\n",
      "[epoch:24,batch:569]:acc: 0.924287,loss:0.187985\n",
      "[epoch:24,batch:599]:acc: 0.924323,loss:0.187933\n",
      "[epoch:24,batch:599]: val_loss:0.360993,val_acc:0.864508,val_total:4539\n",
      "[epoch:24,batch:629]:acc: 0.924752,loss:0.186747\n",
      "[epoch:24,batch:659]:acc: 0.924574,loss:0.187261\n",
      "[epoch:24,batch:689]:acc: 0.924004,loss:0.188366\n",
      "[epoch:24,batch:719]:acc: 0.924436,loss:0.187478\n",
      "[epoch:24,batch:749]:acc: 0.924167,loss:0.187896\n",
      "[epoch:24,batch:779]:acc: 0.923798,loss:0.189218\n",
      "[epoch:24,batch:809]:acc: 0.924074,loss:0.188757\n",
      "[epoch:24,batch:839]:acc: 0.924442,loss:0.188104\n",
      "[epoch:24,batch:869]:acc: 0.924641,loss:0.188007\n",
      "[epoch:24,batch:899]:acc: 0.924653,loss:0.188028\n",
      "[epoch:24,batch:899]: val_loss:0.361578,val_acc:0.865609,val_total:4539\n",
      "[epoch:24,batch:929]:acc: 0.924933,loss:0.187521\n",
      "[epoch:24,batch:959]:acc: 0.924837,loss:0.187860\n",
      "[epoch:24,batch:989]:acc: 0.924432,loss:0.188452\n",
      "[epoch:24] :acc: 0.924362,loss:0.188672,lr:0.000004,patience:0\n",
      "[epoch:24]: val_loss:0.365171,val_acc:0.864067,\n",
      "Epoch 25/59\n",
      "----------\n",
      "[epoch:25,batch:29]:acc: 0.909375,loss:0.206475\n",
      "[epoch:25,batch:59]:acc: 0.909896,loss:0.210124\n",
      "[epoch:25,batch:89]:acc: 0.901389,loss:0.219994\n",
      "[epoch:25,batch:119]:acc: 0.903646,loss:0.218485\n",
      "[epoch:25,batch:149]:acc: 0.903125,loss:0.220560\n",
      "[epoch:25,batch:179]:acc: 0.900694,loss:0.227101\n",
      "[epoch:25,batch:209]:acc: 0.900000,loss:0.228115\n",
      "[epoch:25,batch:239]:acc: 0.900130,loss:0.227759\n",
      "[epoch:25,batch:269]:acc: 0.901273,loss:0.225132\n",
      "[epoch:25,batch:299]:acc: 0.900417,loss:0.226481\n",
      "[epoch:25,batch:299]: val_loss:0.360182,val_acc:0.863847,val_total:4539\n",
      "[epoch:25,batch:329]:acc: 0.900473,loss:0.227634\n",
      "[epoch:25,batch:359]:acc: 0.900608,loss:0.226753\n",
      "[epoch:25,batch:389]:acc: 0.901122,loss:0.226417\n",
      "[epoch:25,batch:419]:acc: 0.900818,loss:0.226293\n",
      "[epoch:25,batch:449]:acc: 0.902014,loss:0.223884\n",
      "[epoch:25,batch:479]:acc: 0.902865,loss:0.223470\n",
      "[epoch:25,batch:509]:acc: 0.902635,loss:0.223895\n",
      "[epoch:25,batch:539]:acc: 0.902257,loss:0.224799\n",
      "[epoch:25,batch:569]:acc: 0.901206,loss:0.227151\n",
      "[epoch:25,batch:599]:acc: 0.901771,loss:0.225593\n",
      "[epoch:25,batch:599]: val_loss:0.360431,val_acc:0.863626,val_total:4539\n",
      "[epoch:25,batch:629]:acc: 0.902232,loss:0.225057\n",
      "[epoch:25,batch:659]:acc: 0.902604,loss:0.225338\n",
      "[epoch:25,batch:689]:acc: 0.903125,loss:0.225286\n",
      "[epoch:25,batch:719]:acc: 0.903255,loss:0.225254\n",
      "[epoch:25,batch:749]:acc: 0.902917,loss:0.226273\n",
      "[epoch:25,batch:779]:acc: 0.903045,loss:0.225586\n",
      "[epoch:25,batch:809]:acc: 0.902585,loss:0.225999\n",
      "[epoch:25,batch:839]:acc: 0.902344,loss:0.226668\n",
      "[epoch:25,batch:869]:acc: 0.902011,loss:0.226814\n",
      "[epoch:25,batch:899]:acc: 0.901806,loss:0.226545\n",
      "[epoch:25,batch:899]: val_loss:0.363808,val_acc:0.865169,val_total:4539\n",
      "[epoch:25,batch:929]:acc: 0.901714,loss:0.227344\n",
      "[epoch:25,batch:959]:acc: 0.902116,loss:0.227259\n",
      "[epoch:25,batch:989]:acc: 0.902273,loss:0.227191\n",
      "[epoch:25] :acc: 0.902229,loss:0.227066,lr:0.000004,patience:1\n",
      "[epoch:25]: val_loss:0.362469,val_acc:0.863186,\n",
      "Epoch 26/59\n",
      "----------\n",
      "[epoch:26,batch:29]:acc: 0.900000,loss:0.235090\n",
      "[epoch:26,batch:59]:acc: 0.901563,loss:0.237252\n",
      "[epoch:26,batch:89]:acc: 0.899306,loss:0.239386\n",
      "[epoch:26,batch:119]:acc: 0.902344,loss:0.231344\n",
      "[epoch:26,batch:149]:acc: 0.902292,loss:0.233332\n",
      "[epoch:26,batch:179]:acc: 0.903299,loss:0.228797\n",
      "[epoch:26,batch:209]:acc: 0.905060,loss:0.225839\n",
      "[epoch:26,batch:239]:acc: 0.903385,loss:0.225967\n",
      "[epoch:26,batch:269]:acc: 0.902546,loss:0.227314\n",
      "[epoch:26,batch:299]:acc: 0.902188,loss:0.228193\n",
      "[epoch:26,batch:299]: val_loss:0.359188,val_acc:0.865389,val_total:4539\n",
      "[epoch:26,batch:329]:acc: 0.901610,loss:0.227989\n",
      "[epoch:26,batch:359]:acc: 0.900955,loss:0.228791\n",
      "[epoch:26,batch:389]:acc: 0.900561,loss:0.228627\n",
      "[epoch:26,batch:419]:acc: 0.900372,loss:0.228332\n",
      "[epoch:26,batch:449]:acc: 0.901181,loss:0.227339\n",
      "[epoch:26,batch:479]:acc: 0.901563,loss:0.226751\n",
      "[epoch:26,batch:509]:acc: 0.901471,loss:0.227810\n",
      "[epoch:26,batch:539]:acc: 0.901273,loss:0.228305\n",
      "[epoch:26,batch:569]:acc: 0.900932,loss:0.228196\n",
      "[epoch:26,batch:599]:acc: 0.900937,loss:0.228495\n",
      "[epoch:26,batch:599]: val_loss:0.361059,val_acc:0.864728,val_total:4539\n",
      "[epoch:26,batch:629]:acc: 0.900942,loss:0.228469\n",
      "[epoch:26,batch:659]:acc: 0.900900,loss:0.228111\n",
      "[epoch:26,batch:689]:acc: 0.901585,loss:0.227236\n",
      "[epoch:26,batch:719]:acc: 0.902214,loss:0.226362\n",
      "[epoch:26,batch:749]:acc: 0.902667,loss:0.225978\n",
      "[epoch:26,batch:779]:acc: 0.902123,loss:0.226988\n",
      "[epoch:26,batch:809]:acc: 0.902315,loss:0.226966\n",
      "[epoch:26,batch:839]:acc: 0.901897,loss:0.228068\n",
      "[epoch:26,batch:869]:acc: 0.901976,loss:0.228068\n",
      "[epoch:26,batch:899]:acc: 0.902118,loss:0.228328\n",
      "[epoch:26,batch:899]: val_loss:0.363603,val_acc:0.862965,val_total:4539\n",
      "[epoch:26,batch:929]:acc: 0.902453,loss:0.228108\n",
      "[epoch:26,batch:959]:acc: 0.902051,loss:0.228619\n",
      "[epoch:26,batch:989]:acc: 0.902178,loss:0.228833\n",
      "[epoch:26] :acc: 0.902229,loss:0.228575,lr:0.000004,patience:2\n",
      "[epoch:26]: val_loss:0.364528,val_acc:0.861643,\n",
      "Epoch 27/59\n",
      "----------\n",
      "lr desencd\n",
      "[epoch:27,batch:29]:acc: 0.913542,loss:0.210458\n",
      "[epoch:27,batch:59]:acc: 0.914583,loss:0.203842\n",
      "[epoch:27,batch:89]:acc: 0.910764,loss:0.211620\n",
      "[epoch:27,batch:119]:acc: 0.908073,loss:0.214514\n",
      "[epoch:27,batch:149]:acc: 0.908333,loss:0.219275\n",
      "[epoch:27,batch:179]:acc: 0.906250,loss:0.220814\n",
      "[epoch:27,batch:209]:acc: 0.906994,loss:0.217793\n",
      "[epoch:27,batch:239]:acc: 0.907682,loss:0.216879\n",
      "[epoch:27,batch:269]:acc: 0.907523,loss:0.216660\n",
      "[epoch:27,batch:299]:acc: 0.908229,loss:0.215123\n",
      "[epoch:27,batch:299]: val_loss:0.361390,val_acc:0.864508,val_total:4539\n",
      "[epoch:27,batch:329]:acc: 0.908617,loss:0.214498\n",
      "[epoch:27,batch:359]:acc: 0.909115,loss:0.214119\n",
      "[epoch:27,batch:389]:acc: 0.908574,loss:0.214979\n",
      "[epoch:27,batch:419]:acc: 0.907887,loss:0.217046\n",
      "[epoch:27,batch:449]:acc: 0.908403,loss:0.215958\n",
      "[epoch:27,batch:479]:acc: 0.908138,loss:0.215832\n",
      "[epoch:27,batch:509]:acc: 0.907598,loss:0.216792\n",
      "[epoch:27,batch:539]:acc: 0.907523,loss:0.216562\n",
      "[epoch:27,batch:569]:acc: 0.907730,loss:0.216903\n",
      "[epoch:27,batch:599]:acc: 0.907604,loss:0.217138\n",
      "[epoch:27,batch:599]: val_loss:0.362407,val_acc:0.863626,val_total:4539\n",
      "[epoch:27,batch:629]:acc: 0.907143,loss:0.218061\n",
      "[epoch:27,batch:659]:acc: 0.906534,loss:0.218730\n",
      "[epoch:27,batch:689]:acc: 0.906250,loss:0.218436\n",
      "[epoch:27,batch:719]:acc: 0.906163,loss:0.218646\n",
      "[epoch:27,batch:749]:acc: 0.906125,loss:0.218733\n",
      "[epoch:27,batch:779]:acc: 0.906130,loss:0.218735\n",
      "[epoch:27,batch:809]:acc: 0.906250,loss:0.218251\n",
      "[epoch:27,batch:839]:acc: 0.905915,loss:0.218326\n",
      "[epoch:27,batch:869]:acc: 0.905639,loss:0.218556\n",
      "[epoch:27,batch:899]:acc: 0.905486,loss:0.218937\n",
      "[epoch:27,batch:899]: val_loss:0.361254,val_acc:0.864287,val_total:4539\n",
      "[epoch:27,batch:929]:acc: 0.904973,loss:0.219378\n",
      "[epoch:27,batch:959]:acc: 0.905273,loss:0.218999\n",
      "[epoch:27,batch:989]:acc: 0.905997,loss:0.218381\n",
      "[epoch:27] :acc: 0.905981,loss:0.218349,lr:0.000001,patience:0\n",
      "[epoch:27]: val_loss:0.359635,val_acc:0.862965,\n",
      "Epoch 28/59\n",
      "----------\n",
      "[epoch:28,batch:29]:acc: 0.884375,loss:0.244487\n",
      "[epoch:28,batch:59]:acc: 0.899479,loss:0.228866\n",
      "[epoch:28,batch:89]:acc: 0.901042,loss:0.227252\n",
      "[epoch:28,batch:119]:acc: 0.901042,loss:0.226885\n",
      "[epoch:28,batch:149]:acc: 0.904375,loss:0.222377\n",
      "[epoch:28,batch:179]:acc: 0.903125,loss:0.223768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:28,batch:209]:acc: 0.902679,loss:0.226547\n",
      "[epoch:28,batch:239]:acc: 0.902214,loss:0.225037\n",
      "[epoch:28,batch:269]:acc: 0.902546,loss:0.224629\n",
      "[epoch:28,batch:299]:acc: 0.902083,loss:0.224707\n",
      "[epoch:28,batch:299]: val_loss:0.362158,val_acc:0.865389,val_total:4539\n",
      "[epoch:28,batch:329]:acc: 0.903409,loss:0.223529\n",
      "[epoch:28,batch:359]:acc: 0.904253,loss:0.222236\n",
      "[epoch:28,batch:389]:acc: 0.904006,loss:0.223156\n",
      "[epoch:28,batch:419]:acc: 0.904167,loss:0.222368\n",
      "[epoch:28,batch:449]:acc: 0.904514,loss:0.221660\n",
      "[epoch:28,batch:479]:acc: 0.903581,loss:0.223555\n",
      "[epoch:28,batch:509]:acc: 0.903922,loss:0.223840\n",
      "[epoch:28,batch:539]:acc: 0.903877,loss:0.223895\n",
      "[epoch:28,batch:569]:acc: 0.903070,loss:0.225349\n",
      "[epoch:28,batch:599]:acc: 0.903177,loss:0.225888\n",
      "[epoch:28,batch:599]: val_loss:0.361153,val_acc:0.862965,val_total:4539\n",
      "[epoch:28,batch:629]:acc: 0.903323,loss:0.225351\n",
      "[epoch:28,batch:659]:acc: 0.902509,loss:0.226235\n",
      "[epoch:28,batch:689]:acc: 0.902219,loss:0.226720\n",
      "[epoch:28,batch:719]:acc: 0.902431,loss:0.227157\n",
      "[epoch:28,batch:749]:acc: 0.902167,loss:0.227741\n",
      "[epoch:28,batch:779]:acc: 0.902444,loss:0.227816\n",
      "[epoch:28,batch:809]:acc: 0.902392,loss:0.227978\n",
      "[epoch:28,batch:839]:acc: 0.902158,loss:0.227798\n",
      "[epoch:28,batch:869]:acc: 0.901904,loss:0.228251\n",
      "[epoch:28,batch:899]:acc: 0.901424,loss:0.229958\n",
      "[epoch:28,batch:899]: val_loss:0.362403,val_acc:0.863406,val_total:4539\n",
      "[epoch:28,batch:929]:acc: 0.901647,loss:0.229666\n",
      "[epoch:28,batch:959]:acc: 0.901660,loss:0.229544\n",
      "[epoch:28,batch:989]:acc: 0.902083,loss:0.229587\n",
      "[epoch:28] :acc: 0.901945,loss:0.230056,lr:0.000001,patience:1\n",
      "[epoch:28]: val_loss:0.358316,val_acc:0.865829,\n",
      "Epoch 29/59\n",
      "----------\n",
      "[epoch:29,batch:29]:acc: 0.918750,loss:0.197205\n",
      "[epoch:29,batch:59]:acc: 0.906250,loss:0.220729\n",
      "[epoch:29,batch:89]:acc: 0.906944,loss:0.219630\n",
      "[epoch:29,batch:119]:acc: 0.904687,loss:0.226422\n",
      "[epoch:29,batch:149]:acc: 0.904792,loss:0.226905\n",
      "[epoch:29,batch:179]:acc: 0.905382,loss:0.223516\n",
      "[epoch:29,batch:209]:acc: 0.904018,loss:0.227509\n",
      "[epoch:29,batch:239]:acc: 0.905078,loss:0.226417\n",
      "[epoch:29,batch:269]:acc: 0.903472,loss:0.228786\n",
      "[epoch:29,batch:299]:acc: 0.903958,loss:0.229012\n",
      "[epoch:29,batch:299]: val_loss:0.361951,val_acc:0.861643,val_total:4539\n",
      "[epoch:29,batch:329]:acc: 0.903314,loss:0.230553\n",
      "[epoch:29,batch:359]:acc: 0.902951,loss:0.230765\n",
      "[epoch:29,batch:389]:acc: 0.902404,loss:0.231464\n",
      "[epoch:29,batch:419]:acc: 0.902902,loss:0.230548\n",
      "[epoch:29,batch:449]:acc: 0.902778,loss:0.230869\n",
      "[epoch:29,batch:479]:acc: 0.902083,loss:0.231018\n",
      "[epoch:29,batch:509]:acc: 0.902267,loss:0.231568\n",
      "[epoch:29,batch:539]:acc: 0.902951,loss:0.230052\n",
      "[epoch:29,batch:569]:acc: 0.902138,loss:0.230742\n",
      "[epoch:29,batch:599]:acc: 0.901771,loss:0.231932\n",
      "[epoch:29,batch:599]: val_loss:0.360571,val_acc:0.863186,val_total:4539\n",
      "[epoch:29,batch:629]:acc: 0.902133,loss:0.231322\n",
      "[epoch:29,batch:659]:acc: 0.901847,loss:0.230642\n",
      "[epoch:29,batch:689]:acc: 0.902083,loss:0.230692\n",
      "[epoch:29,batch:719]:acc: 0.902170,loss:0.229930\n",
      "[epoch:29,batch:749]:acc: 0.902000,loss:0.229922\n",
      "[epoch:29,batch:779]:acc: 0.902043,loss:0.229565\n",
      "[epoch:29,batch:809]:acc: 0.902238,loss:0.229248\n",
      "[epoch:29,batch:839]:acc: 0.902158,loss:0.228931\n",
      "[epoch:29,batch:869]:acc: 0.902443,loss:0.228707\n",
      "[epoch:29,batch:899]:acc: 0.903125,loss:0.227470\n",
      "[epoch:29,batch:899]: val_loss:0.362142,val_acc:0.866050,val_total:4539\n",
      "[epoch:29,batch:929]:acc: 0.902117,loss:0.228254\n",
      "[epoch:29,batch:959]:acc: 0.902181,loss:0.228582\n",
      "[epoch:29,batch:989]:acc: 0.901768,loss:0.229122\n",
      "[epoch:29] :acc: 0.901851,loss:0.229191,lr:0.000001,patience:2\n",
      "[epoch:29]: val_loss:0.364771,val_acc:0.864508,\n",
      "Epoch 30/59\n",
      "----------\n",
      "lr desencd\n",
      "[epoch:30,batch:29]:acc: 0.920833,loss:0.210130\n",
      "[epoch:30,batch:59]:acc: 0.908854,loss:0.215884\n",
      "[epoch:30,batch:89]:acc: 0.901736,loss:0.228373\n",
      "[epoch:30,batch:119]:acc: 0.904427,loss:0.224963\n",
      "[epoch:30,batch:149]:acc: 0.905833,loss:0.223980\n",
      "[epoch:30,batch:179]:acc: 0.903472,loss:0.226006\n",
      "[epoch:30,batch:209]:acc: 0.900000,loss:0.232593\n",
      "[epoch:30,batch:239]:acc: 0.902214,loss:0.230084\n",
      "[epoch:30,batch:269]:acc: 0.901852,loss:0.230862\n",
      "[epoch:30,batch:299]:acc: 0.901667,loss:0.233112\n",
      "[epoch:30,batch:299]: val_loss:0.359417,val_acc:0.865169,val_total:4539\n",
      "[epoch:30,batch:329]:acc: 0.901705,loss:0.232931\n",
      "[epoch:30,batch:359]:acc: 0.901389,loss:0.234028\n",
      "[epoch:30,batch:389]:acc: 0.900801,loss:0.233577\n",
      "[epoch:30,batch:419]:acc: 0.900818,loss:0.235514\n",
      "[epoch:30,batch:449]:acc: 0.901736,loss:0.233534\n",
      "[epoch:30,batch:479]:acc: 0.902018,loss:0.232748\n",
      "[epoch:30,batch:509]:acc: 0.903125,loss:0.231960\n",
      "[epoch:30,batch:539]:acc: 0.902951,loss:0.231411\n",
      "[epoch:30,batch:569]:acc: 0.903015,loss:0.230600\n",
      "[epoch:30,batch:599]:acc: 0.902969,loss:0.231125\n",
      "[epoch:30,batch:599]: val_loss:0.362677,val_acc:0.863186,val_total:4539\n",
      "[epoch:30,batch:629]:acc: 0.902827,loss:0.230823\n",
      "[epoch:30,batch:659]:acc: 0.902746,loss:0.230976\n",
      "[epoch:30,batch:689]:acc: 0.903850,loss:0.229815\n",
      "[epoch:30,batch:719]:acc: 0.903993,loss:0.230312\n",
      "[epoch:30,batch:749]:acc: 0.903917,loss:0.229667\n",
      "[epoch:30,batch:779]:acc: 0.903766,loss:0.229828\n",
      "[epoch:30,batch:809]:acc: 0.903935,loss:0.230004\n",
      "[epoch:30,batch:839]:acc: 0.903534,loss:0.231067\n",
      "[epoch:30,batch:869]:acc: 0.903628,loss:0.231092\n",
      "[epoch:30,batch:899]:acc: 0.904340,loss:0.229479\n",
      "[epoch:30,batch:899]: val_loss:0.362520,val_acc:0.864728,val_total:4539\n",
      "[epoch:30,batch:929]:acc: 0.904704,loss:0.228925\n",
      "[epoch:30,batch:959]:acc: 0.904883,loss:0.228706\n",
      "[epoch:30,batch:989]:acc: 0.904609,loss:0.228487\n",
      "[epoch:30] :acc: 0.904562,loss:0.228975,lr:0.000000,patience:0\n",
      "[epoch:30]: val_loss:0.360097,val_acc:0.863847,\n",
      "Epoch 31/59\n",
      "----------\n",
      "[epoch:31,batch:29]:acc: 0.902083,loss:0.241973\n",
      "[epoch:31,batch:59]:acc: 0.904167,loss:0.237803\n",
      "[epoch:31,batch:89]:acc: 0.905903,loss:0.237211\n",
      "[epoch:31,batch:119]:acc: 0.905990,loss:0.233508\n",
      "[epoch:31,batch:149]:acc: 0.905833,loss:0.233675\n",
      "[epoch:31,batch:179]:acc: 0.909375,loss:0.225513\n",
      "[epoch:31,batch:209]:acc: 0.907738,loss:0.226358\n",
      "[epoch:31,batch:239]:acc: 0.907031,loss:0.225532\n",
      "[epoch:31,batch:269]:acc: 0.906713,loss:0.224669\n",
      "[epoch:31,batch:299]:acc: 0.904896,loss:0.226891\n",
      "[epoch:31,batch:299]: val_loss:0.360238,val_acc:0.864948,val_total:4539\n",
      "[epoch:31,batch:329]:acc: 0.904830,loss:0.227926\n",
      "[epoch:31,batch:359]:acc: 0.904687,loss:0.228058\n",
      "[epoch:31,batch:389]:acc: 0.904567,loss:0.228893\n",
      "[epoch:31,batch:419]:acc: 0.904762,loss:0.226822\n",
      "[epoch:31,batch:449]:acc: 0.904444,loss:0.227565\n",
      "[epoch:31,batch:479]:acc: 0.904818,loss:0.227749\n",
      "[epoch:31,batch:509]:acc: 0.904473,loss:0.227446\n",
      "[epoch:31,batch:539]:acc: 0.904630,loss:0.227114\n",
      "[epoch:31,batch:569]:acc: 0.904221,loss:0.227366\n",
      "[epoch:31,batch:599]:acc: 0.904531,loss:0.227119\n",
      "[epoch:31,batch:599]: val_loss:0.362410,val_acc:0.864067,val_total:4539\n",
      "[epoch:31,batch:629]:acc: 0.903919,loss:0.228119\n",
      "[epoch:31,batch:659]:acc: 0.903693,loss:0.229110\n",
      "[epoch:31,batch:689]:acc: 0.904167,loss:0.228659\n",
      "[epoch:31,batch:719]:acc: 0.904167,loss:0.228017\n",
      "[epoch:31,batch:749]:acc: 0.904625,loss:0.227490\n",
      "[epoch:31,batch:779]:acc: 0.905048,loss:0.226616\n",
      "[epoch:31,batch:809]:acc: 0.905247,loss:0.226126\n",
      "[epoch:31,batch:839]:acc: 0.904762,loss:0.226647\n",
      "[epoch:31,batch:869]:acc: 0.904562,loss:0.227229\n",
      "[epoch:31,batch:899]:acc: 0.904722,loss:0.227344\n",
      "[epoch:31,batch:899]: val_loss:0.361074,val_acc:0.862525,val_total:4539\n",
      "[epoch:31,batch:929]:acc: 0.905141,loss:0.226846\n",
      "[epoch:31,batch:959]:acc: 0.905339,loss:0.226366\n",
      "[epoch:31,batch:989]:acc: 0.905019,loss:0.227118\n",
      "[epoch:31] :acc: 0.905004,loss:0.227717,lr:0.000000,patience:1\n",
      "[epoch:31]: val_loss:0.365496,val_acc:0.864067,\n",
      "Epoch 32/59\n",
      "----------\n",
      "[epoch:32,batch:29]:acc: 0.906250,loss:0.224279\n",
      "[epoch:32,batch:59]:acc: 0.897917,loss:0.229739\n",
      "[epoch:32,batch:89]:acc: 0.900000,loss:0.229032\n",
      "[epoch:32,batch:119]:acc: 0.903385,loss:0.228881\n",
      "[epoch:32,batch:149]:acc: 0.906042,loss:0.226020\n",
      "[epoch:32,batch:179]:acc: 0.905035,loss:0.228762\n",
      "[epoch:32,batch:209]:acc: 0.904613,loss:0.232007\n",
      "[epoch:32,batch:239]:acc: 0.905729,loss:0.228929\n",
      "[epoch:32,batch:269]:acc: 0.902778,loss:0.231285\n",
      "[epoch:32,batch:299]:acc: 0.903646,loss:0.229467\n",
      "[epoch:32,batch:299]: val_loss:0.360838,val_acc:0.860322,val_total:4539\n",
      "[epoch:32,batch:329]:acc: 0.905019,loss:0.226681\n",
      "[epoch:32,batch:359]:acc: 0.904601,loss:0.227726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:32,batch:389]:acc: 0.904728,loss:0.229077\n",
      "[epoch:32,batch:419]:acc: 0.904911,loss:0.228349\n",
      "[epoch:32,batch:449]:acc: 0.904444,loss:0.228199\n",
      "[epoch:32,batch:479]:acc: 0.904232,loss:0.228036\n",
      "[epoch:32,batch:509]:acc: 0.904657,loss:0.227740\n",
      "[epoch:32,batch:539]:acc: 0.904398,loss:0.227630\n",
      "[epoch:32,batch:569]:acc: 0.903838,loss:0.227817\n",
      "[epoch:32,batch:599]:acc: 0.904427,loss:0.227396\n",
      "[epoch:32,batch:599]: val_loss:0.361058,val_acc:0.864508,val_total:4539\n",
      "[epoch:32,batch:629]:acc: 0.903919,loss:0.228571\n",
      "[epoch:32,batch:659]:acc: 0.904403,loss:0.228206\n",
      "[epoch:32,batch:689]:acc: 0.904212,loss:0.228284\n",
      "[epoch:32,batch:719]:acc: 0.904384,loss:0.227982\n",
      "[epoch:32,batch:749]:acc: 0.903542,loss:0.229471\n",
      "[epoch:32,batch:779]:acc: 0.903285,loss:0.230208\n",
      "[epoch:32,batch:809]:acc: 0.903395,loss:0.230083\n",
      "[epoch:32,batch:839]:acc: 0.903385,loss:0.229810\n",
      "[epoch:32,batch:869]:acc: 0.903592,loss:0.229240\n",
      "[epoch:32,batch:899]:acc: 0.903472,loss:0.229716\n",
      "[epoch:32,batch:899]: val_loss:0.361201,val_acc:0.863847,val_total:4539\n",
      "[epoch:32,batch:929]:acc: 0.902957,loss:0.230523\n",
      "[epoch:32,batch:959]:acc: 0.903255,loss:0.229937\n",
      "[epoch:32,batch:989]:acc: 0.903188,loss:0.229874\n",
      "[epoch:32] :acc: 0.903238,loss:0.230385,lr:0.000000,patience:2\n",
      "[epoch:32]: val_loss:0.362654,val_acc:0.863626,\n",
      "Epoch 33/59\n",
      "----------\n",
      "lr desencd\n",
      "[epoch:33,batch:29]:acc: 0.879167,loss:0.248798\n",
      "[epoch:33,batch:59]:acc: 0.900521,loss:0.219177\n",
      "[epoch:33,batch:89]:acc: 0.901736,loss:0.220757\n",
      "[epoch:33,batch:119]:acc: 0.902604,loss:0.224050\n",
      "[epoch:33,batch:149]:acc: 0.902292,loss:0.228028\n",
      "[epoch:33,batch:179]:acc: 0.903993,loss:0.226741\n",
      "[epoch:33,batch:209]:acc: 0.903571,loss:0.226259\n",
      "[epoch:33,batch:239]:acc: 0.902344,loss:0.231213\n",
      "[epoch:33,batch:269]:acc: 0.902315,loss:0.232792\n",
      "[epoch:33,batch:299]:acc: 0.902708,loss:0.231669\n",
      "[epoch:33,batch:299]: val_loss:0.361268,val_acc:0.865609,val_total:4539\n",
      "[epoch:33,batch:329]:acc: 0.901515,loss:0.234053\n",
      "[epoch:33,batch:359]:acc: 0.900955,loss:0.233936\n",
      "[epoch:33,batch:389]:acc: 0.901282,loss:0.233953\n",
      "[epoch:33,batch:419]:acc: 0.901563,loss:0.234236\n",
      "[epoch:33,batch:449]:acc: 0.900764,loss:0.234967\n",
      "[epoch:33,batch:479]:acc: 0.901172,loss:0.234599\n",
      "[epoch:33,batch:509]:acc: 0.900490,loss:0.235838\n",
      "[epoch:33,batch:539]:acc: 0.901100,loss:0.235024\n",
      "[epoch:33,batch:569]:acc: 0.901480,loss:0.234514\n",
      "[epoch:33,batch:599]:acc: 0.901406,loss:0.234790\n",
      "[epoch:33,batch:599]: val_loss:0.363678,val_acc:0.862304,val_total:4539\n",
      "[epoch:33,batch:629]:acc: 0.901389,loss:0.234887\n",
      "[epoch:33,batch:659]:acc: 0.902462,loss:0.233650\n",
      "[epoch:33,batch:689]:acc: 0.902627,loss:0.233308\n",
      "[epoch:33,batch:719]:acc: 0.902734,loss:0.232523\n",
      "[epoch:33,batch:749]:acc: 0.902625,loss:0.232271\n",
      "[epoch:33,batch:779]:acc: 0.903686,loss:0.230442\n",
      "[epoch:33,batch:809]:acc: 0.903549,loss:0.230990\n",
      "[epoch:33,batch:839]:acc: 0.903125,loss:0.230809\n",
      "[epoch:33,batch:869]:acc: 0.902874,loss:0.230571\n",
      "[epoch:33,batch:899]:acc: 0.903056,loss:0.230196\n",
      "[epoch:33,batch:899]: val_loss:0.361315,val_acc:0.864067,val_total:4539\n",
      "[epoch:33,batch:929]:acc: 0.902923,loss:0.230150\n",
      "[epoch:33,batch:959]:acc: 0.902962,loss:0.230298\n",
      "[epoch:33,batch:989]:acc: 0.902967,loss:0.230741\n",
      "[epoch:33] :acc: 0.902923,loss:0.231784,lr:0.000000,patience:0\n",
      "[epoch:33]: val_loss:0.362387,val_acc:0.863626,\n",
      "Epoch 34/59\n",
      "----------\n",
      "[epoch:34,batch:29]:acc: 0.904167,loss:0.237958\n",
      "[epoch:34,batch:59]:acc: 0.909896,loss:0.226376\n",
      "[epoch:34,batch:89]:acc: 0.910417,loss:0.223912\n",
      "[epoch:34,batch:119]:acc: 0.907031,loss:0.227596\n",
      "[epoch:34,batch:149]:acc: 0.904583,loss:0.232811\n",
      "[epoch:34,batch:179]:acc: 0.903472,loss:0.235137\n",
      "[epoch:34,batch:209]:acc: 0.904167,loss:0.231829\n",
      "[epoch:34,batch:239]:acc: 0.905469,loss:0.230115\n",
      "[epoch:34,batch:269]:acc: 0.905440,loss:0.229842\n",
      "[epoch:34,batch:299]:acc: 0.905521,loss:0.230032\n",
      "[epoch:34,batch:299]: val_loss:0.358226,val_acc:0.865389,val_total:4539\n",
      "save new model loss,now loss is  0.3582262098789215\n",
      "[epoch:34,batch:329]:acc: 0.905303,loss:0.229855\n",
      "[epoch:34,batch:359]:acc: 0.906510,loss:0.228088\n",
      "[epoch:34,batch:389]:acc: 0.905689,loss:0.228746\n",
      "[epoch:34,batch:419]:acc: 0.904836,loss:0.228324\n",
      "[epoch:34,batch:449]:acc: 0.904792,loss:0.228729\n",
      "[epoch:34,batch:479]:acc: 0.903451,loss:0.229534\n",
      "[epoch:34,batch:509]:acc: 0.902880,loss:0.229942\n",
      "[epoch:34,batch:539]:acc: 0.901620,loss:0.231638\n",
      "[epoch:34,batch:569]:acc: 0.902083,loss:0.231436\n",
      "[epoch:34,batch:599]:acc: 0.901667,loss:0.232339\n",
      "[epoch:34,batch:599]: val_loss:0.358768,val_acc:0.866490,val_total:4539\n",
      "[epoch:34,batch:629]:acc: 0.902381,loss:0.231333\n",
      "[epoch:34,batch:659]:acc: 0.902983,loss:0.230744\n",
      "[epoch:34,batch:689]:acc: 0.903261,loss:0.230211\n",
      "[epoch:34,batch:719]:acc: 0.902604,loss:0.231028\n",
      "[epoch:34,batch:749]:acc: 0.902958,loss:0.230756\n",
      "[epoch:34,batch:779]:acc: 0.902444,loss:0.231192\n",
      "[epoch:34,batch:809]:acc: 0.902623,loss:0.231305\n",
      "[epoch:34,batch:839]:acc: 0.902679,loss:0.230861\n",
      "[epoch:34,batch:869]:acc: 0.902550,loss:0.231863\n",
      "[epoch:34,batch:899]:acc: 0.902778,loss:0.232283\n",
      "[epoch:34,batch:899]: val_loss:0.360312,val_acc:0.862084,val_total:4539\n",
      "[epoch:34,batch:929]:acc: 0.903159,loss:0.231712\n",
      "[epoch:34,batch:959]:acc: 0.903190,loss:0.231639\n",
      "[epoch:34,batch:989]:acc: 0.903125,loss:0.231642\n",
      "[epoch:34] :acc: 0.903112,loss:0.231503,lr:0.000000,patience:0\n",
      "[epoch:34]: val_loss:0.360170,val_acc:0.865169,\n",
      "Epoch 35/59\n",
      "----------\n",
      "[epoch:35,batch:29]:acc: 0.889583,loss:0.270058\n",
      "[epoch:35,batch:59]:acc: 0.895833,loss:0.251463\n",
      "[epoch:35,batch:89]:acc: 0.896181,loss:0.247580\n",
      "[epoch:35,batch:119]:acc: 0.896094,loss:0.246664\n",
      "[epoch:35,batch:149]:acc: 0.899792,loss:0.237601\n",
      "[epoch:35,batch:179]:acc: 0.898785,loss:0.238467\n",
      "[epoch:35,batch:209]:acc: 0.899554,loss:0.238284\n",
      "[epoch:35,batch:239]:acc: 0.899870,loss:0.236567\n",
      "[epoch:35,batch:269]:acc: 0.899421,loss:0.236692\n",
      "[epoch:35,batch:299]:acc: 0.899792,loss:0.235161\n",
      "[epoch:35,batch:299]: val_loss:0.360118,val_acc:0.863847,val_total:4539\n",
      "[epoch:35,batch:329]:acc: 0.900473,loss:0.233520\n",
      "[epoch:35,batch:359]:acc: 0.900694,loss:0.233497\n",
      "[epoch:35,batch:389]:acc: 0.902083,loss:0.232380\n",
      "[epoch:35,batch:419]:acc: 0.901935,loss:0.232695\n",
      "[epoch:35,batch:449]:acc: 0.901250,loss:0.233076\n"
     ]
    }
   ],
   "source": [
    "TrainWithRawData('../model/InceptionV3/2018-11-03_acc_best.pth',60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Conda_Env_Pytorch]",
   "language": "python",
   "name": "conda-env-Conda_Env_Pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
