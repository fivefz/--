{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CropModels\n",
    "from CropDataset import MyDataSet,normalize_torch,normalize_05,normalize_dataset,preprocess,preprocess_hflip,preprocess_with_augmentation\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from tensorboardX import SummaryWriter\n",
    "import datetime\n",
    "import os\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from utils import RunningMean\n",
    "import utils\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "NB_CLASS=59\n",
    "SEED=888\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "IMAGE_SIZE=224    # 不同模型修改不同的Size\n",
    "IMAGE_TRAIN_PRE='../data/AgriculturalDisease_trainingset/images/'\n",
    "ANNOTATION_TRAIN='../data/AgriculturalDisease_trainingset/AgriculturalDisease_train_annotations_deleteNoise.json' #是否需要剔除两类异常类\n",
    "IMAGE_VAL_PRE='../data/AgriculturalDisease_validationset/images/'\n",
    "ANNOTATION_VAL='../data/AgriculturalDisease_validationset/AgriculturalDisease_validation_annotations_deleteNoise.json' #是否需要剔除两类异常类\n",
    "date=str(datetime.date.today())\n",
    "with open(ANNOTATION_TRAIN) as datafile1:\n",
    "    trainDataFram=pd.read_json(datafile1,orient='records')\n",
    "with open(ANNOTATION_VAL) as datafile2: #first check if it's a valid json file or not\n",
    "    validateDataFram =pd.read_json(datafile2,orient='records')    \n",
    "def getmodel():\n",
    "    print('[+] loading model... ', end='', flush=True)\n",
    "    model=CropModels.densenet201_finetune(NB_CLASS) # 需要修改为使用的model\n",
    "    model.cuda()\n",
    "    print('Done')\n",
    "    return model\n",
    "def train(epochNum):\n",
    "    writer=SummaryWriter('log/'+date+'/DesNet201/') # 创建 /log/日期/InceptionResnet的组织形式  不同模型需要修改不同名称\n",
    "    train_dataset=MyDataSet(json_Description=ANNOTATION_TRAIN,transform=preprocess_with_augmentation(normalize_torch,IMAGE_SIZE),path_pre=IMAGE_TRAIN_PRE)\n",
    "    val_dataset=MyDataSet(json_Description=ANNOTATION_VAL,transform=preprocess(normalize_torch,IMAGE_SIZE),path_pre=IMAGE_VAL_PRE)\n",
    "    train_dataLoader=DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE,num_workers=16,shuffle=True)\n",
    "    val_dataLoader=DataLoader(dataset=val_dataset,batch_size=BATCH_SIZE,num_workers=1,shuffle=False)\n",
    "    model=getmodel()\n",
    "    weight=torch.Tensor([1,3,3,3,3,4,2,3,3,3,3,3,3,3,3,3,2,3,3,3,2,3,4,2,3,1,1,3,2,2,1,3,3,1,3,2,3,3,3,3,2,1,3,2,3,3,3,1,3,3,4,4,3,2,2,3,1,1,3]).cuda()\n",
    "    criterion=nn.CrossEntropyLoss(weight=weight).cuda()\n",
    "#     lx, px = utils.predict(model,val_dataLoader)\n",
    "#     min_loss = criterion(Variable(px), Variable(lx)).item()\n",
    "    min_loss=4.1\n",
    "    print('min_loss is :%f'%(min_loss))\n",
    "    min_acc=0.80\n",
    "    patience=0\n",
    "    lr=0.0\n",
    "    momentum=0.9\n",
    "    for epoch in range(epochNum):\n",
    "        print('Epoch {}/{}'.format(epoch, epochNum - 1))\n",
    "        print('-' * 10)\n",
    "        if epoch==3 or epoch==4 or epoch==5:\n",
    "            lr=0.00003\n",
    "            momentum=0.95\n",
    "            print('set lr=:%f,momentum=%f'%(lr,momentum))\n",
    "        if epoch==6:\n",
    "            lr=1e-4\n",
    "            momentum=0.9\n",
    "            print('set lr=:%f,momentum=%f'%(lr,momentum))        \n",
    "        if patience==2:\n",
    "            patience=0\n",
    "            model.load_state_dict(torch.load('../model/DesNet201/'+date+'_loss_best.pth')['state_dict'])\n",
    "            lr=lr/5\n",
    "            print('loss has increased lr divide 10 lr now is :%f'%(lr))\n",
    "        if epoch==0 or epoch==1 or epoch==2: #第一轮首先训练全连接层\n",
    "            lr=1e-3\n",
    "            optimizer = torch.optim.Adam(model.fresh_params(),lr = lr,amsgrad=True,weight_decay=1e-4)\n",
    "#             optimizer=torch.optim.SGD(params=model.fresh_params(),lr=lr,momentum=0.9)\n",
    "\n",
    "            #optimizer = torch.optim.Adam(model.parameters(),lr = lr,amsgrad=True,weight_decay=1e-4)\n",
    "            #optimizer=torch.optim.SGD(params=model.fresh_params(),lr=lr,momentum=0.9)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(model.parameters(),lr = lr,amsgrad=True,weight_decay=1e-4)\n",
    "            \n",
    "             #optimizer=torch.optim.SGD(params=model.parameters(),lr=lr,momentum=momentum)\n",
    "#             optimizer=torch.optim.SGD(params=model.parameters(),lr=lr,momentum=momentum)\n",
    "        running_loss = RunningMean()\n",
    "        running_corrects = RunningMean()\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_dataLoader):\n",
    "            model.train(True)\n",
    "            n_batchsize=inputs.size(0)\n",
    "            inputs = Variable(inputs).cuda()\n",
    "            labels = Variable(labels).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            if isinstance(outputs,tuple):\n",
    "                loss=sum((criterion(o,labels)) for o in outputs)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "            running_loss.update(loss.item(),1)\n",
    "            running_corrects.update(torch.sum(preds == labels.data).data,n_batchsize)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx%30==29:\n",
    "                print('[epoch:%d,batch:%d]:acc: %f,loss:%f'%(epoch,batch_idx,running_corrects.value,running_loss.value))\n",
    "                if batch_idx%300==299: \n",
    "                    niter = epoch * len(train_dataset)/BATCH_SIZE + batch_idx\n",
    "                    writer.add_scalar('Train/Acc',running_corrects.value,niter)\n",
    "                    writer.add_scalar('Train/Loss',running_loss.value,niter)\n",
    "                    lx,px=utils.predict(model,val_dataLoader)\n",
    "                    log_loss = criterion(Variable(px), Variable(lx))\n",
    "                    log_loss = log_loss.item()\n",
    "                    _, preds = torch.max(px, dim=1)\n",
    "                    accuracy = torch.mean((preds == lx).float())\n",
    "                    writer.add_scalar('Val/Acc',accuracy,niter)\n",
    "                    writer.add_scalar('Val/Loss',log_loss,niter)\n",
    "                    print('[epoch:%d,batch:%d]: val_loss:%f,val_acc:%f,val_total:%d'%(epoch,batch_idx,log_loss,accuracy,len(val_dataset)))\n",
    "        print('[epoch:%d] :acc: %f,loss:%f,lr:%f,patience:%d'%(epoch,running_corrects.value,running_loss.value,lr,patience))       \n",
    "        lx,px=utils.predict(model,val_dataLoader)\n",
    "        log_loss = criterion(Variable(px), Variable(lx))\n",
    "        log_loss = log_loss.item()\n",
    "        _, preds = torch.max(px, dim=1)\n",
    "        accuracy = torch.mean((preds == lx).float())\n",
    "        writer.add_scalar('Val/Acc',accuracy,(epoch+1) * len(train_dataset)/BATCH_SIZE)\n",
    "        writer.add_scalar('Val/Loss',log_loss,(epoch+1) * len(train_dataset)/BATCH_SIZE)\n",
    "        print('[epoch:%d]: val_loss:%f,val_acc:%f,'%(epoch,log_loss,accuracy))\n",
    "        if  log_loss < min_loss:\n",
    "            utils.snapshot('../model/', 'DesNet201', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy })          \n",
    "            patience = 0\n",
    "            min_loss=log_loss\n",
    "            print('save new model loss,now loss is ',min_loss)\n",
    "        else:\n",
    "            patience += 1\n",
    "        if accuracy>min_acc:\n",
    "            utils.snapshot('../model/', 'DesNet201', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy },key='acc') \n",
    "            min_acc=accuracy\n",
    "            print('save new model acc,now acc is ',min_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] loading model... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lujunfeng/anaconda3/envs/Conda_Env_Pytorch/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "min_loss is :4.100000\n",
      "Epoch 0/59\n",
      "----------\n",
      "[epoch:0,batch:29]:acc: 0.126042,loss:3.843178\n",
      "[epoch:0,batch:59]:acc: 0.281250,loss:3.374204\n",
      "[epoch:0,batch:89]:acc: 0.362847,loss:3.044162\n",
      "[epoch:0,batch:119]:acc: 0.418490,loss:2.799988\n",
      "[epoch:0,batch:149]:acc: 0.455208,loss:2.601840\n",
      "[epoch:0,batch:179]:acc: 0.482465,loss:2.449963\n",
      "[epoch:0,batch:209]:acc: 0.507738,loss:2.317951\n",
      "[epoch:0,batch:239]:acc: 0.530208,loss:2.194325\n",
      "[epoch:0,batch:269]:acc: 0.546065,loss:2.098324\n",
      "[epoch:0,batch:299]:acc: 0.560104,loss:2.018067\n",
      "[epoch:0,batch:299]: val_loss:1.105027,val_acc:0.722185,val_total:4539\n",
      "[epoch:0,batch:329]:acc: 0.573769,loss:1.938710\n",
      "[epoch:0,batch:359]:acc: 0.582031,loss:1.880606\n",
      "[epoch:0,batch:389]:acc: 0.591186,loss:1.821571\n",
      "[epoch:0,batch:419]:acc: 0.601265,loss:1.763964\n",
      "[epoch:0,batch:449]:acc: 0.608750,loss:1.714544\n",
      "[epoch:0,batch:479]:acc: 0.612891,loss:1.676296\n",
      "[epoch:0,batch:509]:acc: 0.619240,loss:1.635641\n",
      "[epoch:0,batch:539]:acc: 0.624421,loss:1.598293\n",
      "[epoch:0,batch:569]:acc: 0.630482,loss:1.566029\n",
      "[epoch:0,batch:599]:acc: 0.636771,loss:1.532338\n",
      "[epoch:0,batch:599]: val_loss:0.816030,val_acc:0.757876,val_total:4539\n",
      "[epoch:0,batch:629]:acc: 0.641617,loss:1.503093\n",
      "[epoch:0,batch:659]:acc: 0.646686,loss:1.473819\n",
      "[epoch:0,batch:689]:acc: 0.649366,loss:1.452272\n",
      "[epoch:0,batch:719]:acc: 0.651476,loss:1.432606\n",
      "[epoch:0,batch:749]:acc: 0.654958,loss:1.410218\n",
      "[epoch:0,batch:779]:acc: 0.658574,loss:1.388194\n",
      "[epoch:0,batch:809]:acc: 0.661073,loss:1.370256\n",
      "[epoch:0,batch:839]:acc: 0.663579,loss:1.352864\n",
      "[epoch:0,batch:869]:acc: 0.665769,loss:1.336450\n",
      "[epoch:0,batch:899]:acc: 0.667951,loss:1.321344\n",
      "[epoch:0,batch:899]: val_loss:0.711568,val_acc:0.778365,val_total:4539\n",
      "[epoch:0,batch:929]:acc: 0.670430,loss:1.305430\n",
      "[epoch:0,batch:959]:acc: 0.673210,loss:1.290005\n",
      "[epoch:0,batch:989]:acc: 0.675126,loss:1.276676\n",
      "[epoch:0] :acc: 0.675190,loss:1.275536,lr:0.001000,patience:0\n",
      "[epoch:0]: val_loss:0.717158,val_acc:0.772417,\n",
      "save new model loss,now loss is  0.7171582579612732\n",
      "Epoch 1/59\n",
      "----------\n",
      "[epoch:1,batch:29]:acc: 0.753125,loss:0.846255\n",
      "[epoch:1,batch:59]:acc: 0.766146,loss:0.800249\n",
      "[epoch:1,batch:89]:acc: 0.762153,loss:0.791263\n",
      "[epoch:1,batch:119]:acc: 0.763021,loss:0.778797\n",
      "[epoch:1,batch:149]:acc: 0.759167,loss:0.770568\n",
      "[epoch:1,batch:179]:acc: 0.760417,loss:0.759770\n",
      "[epoch:1,batch:209]:acc: 0.761607,loss:0.760428\n",
      "[epoch:1,batch:239]:acc: 0.761589,loss:0.753668\n",
      "[epoch:1,batch:269]:acc: 0.761921,loss:0.750145\n",
      "[epoch:1,batch:299]:acc: 0.760833,loss:0.749927\n",
      "[epoch:1,batch:299]: val_loss:0.704888,val_acc:0.770434,val_total:4539\n",
      "[epoch:1,batch:329]:acc: 0.759848,loss:0.749094\n",
      "[epoch:1,batch:359]:acc: 0.761632,loss:0.745511\n",
      "[epoch:1,batch:389]:acc: 0.760016,loss:0.746769\n",
      "[epoch:1,batch:419]:acc: 0.760714,loss:0.747383\n",
      "[epoch:1,batch:449]:acc: 0.761806,loss:0.747771\n",
      "[epoch:1,batch:479]:acc: 0.762109,loss:0.743813\n",
      "[epoch:1,batch:509]:acc: 0.762377,loss:0.741771\n",
      "[epoch:1,batch:539]:acc: 0.762674,loss:0.741820\n",
      "[epoch:1,batch:569]:acc: 0.762390,loss:0.740571\n",
      "[epoch:1,batch:599]:acc: 0.761563,loss:0.740590\n",
      "[epoch:1,batch:599]: val_loss:0.653434,val_acc:0.778145,val_total:4539\n",
      "[epoch:1,batch:629]:acc: 0.761359,loss:0.740098\n",
      "[epoch:1,batch:659]:acc: 0.761222,loss:0.739694\n",
      "[epoch:1,batch:689]:acc: 0.760417,loss:0.741457\n",
      "[epoch:1,batch:719]:acc: 0.760026,loss:0.740477\n",
      "[epoch:1,batch:749]:acc: 0.760667,loss:0.739063\n",
      "[epoch:1,batch:779]:acc: 0.761819,loss:0.737202\n",
      "[epoch:1,batch:809]:acc: 0.761304,loss:0.738247\n",
      "[epoch:1,batch:839]:acc: 0.760528,loss:0.739087\n",
      "[epoch:1,batch:869]:acc: 0.760093,loss:0.739036\n",
      "[epoch:1,batch:899]:acc: 0.759826,loss:0.737288\n",
      "[epoch:1,batch:899]: val_loss:0.629674,val_acc:0.786957,val_total:4539\n",
      "[epoch:1,batch:929]:acc: 0.760114,loss:0.735323\n",
      "[epoch:1,batch:959]:acc: 0.761133,loss:0.733186\n",
      "[epoch:1,batch:989]:acc: 0.760732,loss:0.731995\n",
      "[epoch:1] :acc: 0.760759,loss:0.732033,lr:0.001000,patience:0\n",
      "[epoch:1]: val_loss:0.629680,val_acc:0.796651,\n",
      "save new model loss,now loss is  0.6296802163124084\n",
      "Epoch 2/59\n",
      "----------\n",
      "[epoch:2,batch:29]:acc: 0.743750,loss:0.753214\n",
      "[epoch:2,batch:59]:acc: 0.761979,loss:0.701159\n",
      "[epoch:2,batch:89]:acc: 0.765625,loss:0.686651\n",
      "[epoch:2,batch:119]:acc: 0.766406,loss:0.688363\n",
      "[epoch:2,batch:149]:acc: 0.765417,loss:0.694167\n",
      "[epoch:2,batch:179]:acc: 0.766493,loss:0.692637\n",
      "[epoch:2,batch:209]:acc: 0.768155,loss:0.682194\n",
      "[epoch:2,batch:239]:acc: 0.769010,loss:0.677696\n",
      "[epoch:2,batch:269]:acc: 0.768634,loss:0.675221\n",
      "[epoch:2,batch:299]:acc: 0.768229,loss:0.679388\n",
      "[epoch:2,batch:299]: val_loss:0.625548,val_acc:0.779907,val_total:4539\n",
      "[epoch:2,batch:329]:acc: 0.770455,loss:0.671562\n",
      "[epoch:2,batch:359]:acc: 0.770573,loss:0.671662\n",
      "[epoch:2,batch:389]:acc: 0.771955,loss:0.666432\n",
      "[epoch:2,batch:419]:acc: 0.772991,loss:0.663834\n",
      "[epoch:2,batch:449]:acc: 0.773125,loss:0.664903\n",
      "[epoch:2,batch:479]:acc: 0.772656,loss:0.664120\n",
      "[epoch:2,batch:509]:acc: 0.773591,loss:0.661541\n",
      "[epoch:2,batch:539]:acc: 0.772627,loss:0.660191\n",
      "[epoch:2,batch:569]:acc: 0.774452,loss:0.656018\n",
      "[epoch:2,batch:599]:acc: 0.774167,loss:0.655488\n",
      "[epoch:2,batch:599]: val_loss:0.608337,val_acc:0.792465,val_total:4539\n",
      "[epoch:2,batch:629]:acc: 0.774901,loss:0.654252\n",
      "[epoch:2,batch:659]:acc: 0.773769,loss:0.657664\n",
      "[epoch:2,batch:689]:acc: 0.773822,loss:0.657508\n",
      "[epoch:2,batch:719]:acc: 0.773958,loss:0.656290\n",
      "[epoch:2,batch:749]:acc: 0.774125,loss:0.654999\n",
      "[epoch:2,batch:779]:acc: 0.773718,loss:0.656191\n",
      "[epoch:2,batch:809]:acc: 0.773611,loss:0.655753\n",
      "[epoch:2,batch:839]:acc: 0.773177,loss:0.655579\n",
      "[epoch:2,batch:869]:acc: 0.773527,loss:0.654564\n",
      "[epoch:2,batch:899]:acc: 0.773472,loss:0.653757\n",
      "[epoch:2,batch:899]: val_loss:0.628293,val_acc:0.769332,val_total:4539\n",
      "[epoch:2,batch:929]:acc: 0.773656,loss:0.654131\n",
      "[epoch:2,batch:959]:acc: 0.774089,loss:0.654343\n",
      "[epoch:2,batch:989]:acc: 0.773895,loss:0.656926\n",
      "[epoch:2] :acc: 0.773718,loss:0.657011,lr:0.001000,patience:0\n",
      "[epoch:2]: val_loss:0.600287,val_acc:0.796651,\n",
      "save new model loss,now loss is  0.6002874374389648\n",
      "Epoch 3/59\n",
      "----------\n",
      "set lr=:0.000030,momentum=0.950000\n",
      "[epoch:3,batch:29]:acc: 0.794792,loss:0.576875\n",
      "[epoch:3,batch:59]:acc: 0.794792,loss:0.552863\n",
      "[epoch:3,batch:89]:acc: 0.803125,loss:0.535309\n",
      "[epoch:3,batch:119]:acc: 0.806510,loss:0.528060\n",
      "[epoch:3,batch:149]:acc: 0.808958,loss:0.529358\n",
      "[epoch:3,batch:179]:acc: 0.810243,loss:0.524846\n",
      "[epoch:3,batch:209]:acc: 0.808929,loss:0.522385\n",
      "[epoch:3,batch:239]:acc: 0.809766,loss:0.519433\n",
      "[epoch:3,batch:269]:acc: 0.809491,loss:0.519317\n",
      "[epoch:3,batch:299]:acc: 0.808958,loss:0.516566\n",
      "[epoch:3,batch:299]: val_loss:0.502082,val_acc:0.817140,val_total:4539\n",
      "[epoch:3,batch:329]:acc: 0.808996,loss:0.515321\n",
      "[epoch:3,batch:359]:acc: 0.811372,loss:0.510601\n",
      "[epoch:3,batch:389]:acc: 0.813462,loss:0.506988\n",
      "[epoch:3,batch:419]:acc: 0.815699,loss:0.502355\n",
      "[epoch:3,batch:449]:acc: 0.817153,loss:0.499093\n",
      "[epoch:3,batch:479]:acc: 0.816732,loss:0.497565\n",
      "[epoch:3,batch:509]:acc: 0.817831,loss:0.494795\n",
      "[epoch:3,batch:539]:acc: 0.818866,loss:0.491402\n",
      "[epoch:3,batch:569]:acc: 0.819518,loss:0.489242\n",
      "[epoch:3,batch:599]:acc: 0.820156,loss:0.486520\n",
      "[epoch:3,batch:599]: val_loss:0.485579,val_acc:0.823089,val_total:4539\n",
      "[epoch:3,batch:629]:acc: 0.820486,loss:0.483562\n",
      "[epoch:3,batch:659]:acc: 0.819934,loss:0.484673\n",
      "[epoch:3,batch:689]:acc: 0.820154,loss:0.481802\n",
      "[epoch:3,batch:719]:acc: 0.820877,loss:0.479262\n",
      "[epoch:3,batch:749]:acc: 0.821583,loss:0.478021\n",
      "[epoch:3,batch:779]:acc: 0.821955,loss:0.476346\n",
      "[epoch:3,batch:809]:acc: 0.822377,loss:0.475171\n",
      "[epoch:3,batch:839]:acc: 0.822917,loss:0.475074\n",
      "[epoch:3,batch:869]:acc: 0.823312,loss:0.474533\n",
      "[epoch:3,batch:899]:acc: 0.823194,loss:0.473806\n",
      "[epoch:3,batch:899]: val_loss:0.446507,val_acc:0.833003,val_total:4539\n",
      "[epoch:3,batch:929]:acc: 0.823488,loss:0.473028\n",
      "[epoch:3,batch:959]:acc: 0.824023,loss:0.471181\n",
      "[epoch:3,batch:989]:acc: 0.824716,loss:0.470330\n",
      "[epoch:3] :acc: 0.824731,loss:0.469933,lr:0.000030,patience:0\n",
      "[epoch:3]: val_loss:0.454378,val_acc:0.829698,\n",
      "save new model loss,now loss is  0.4543784260749817\n",
      "save new model acc,now acc is  tensor(0.8297, device='cuda:0')\n",
      "Epoch 4/59\n",
      "----------\n",
      "set lr=:0.000030,momentum=0.950000\n",
      "[epoch:4,batch:29]:acc: 0.860417,loss:0.368895\n",
      "[epoch:4,batch:59]:acc: 0.860938,loss:0.373594\n",
      "[epoch:4,batch:89]:acc: 0.860069,loss:0.368964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:4,batch:119]:acc: 0.859896,loss:0.371331\n",
      "[epoch:4,batch:149]:acc: 0.861667,loss:0.363234\n",
      "[epoch:4,batch:179]:acc: 0.863889,loss:0.360007\n",
      "[epoch:4,batch:209]:acc: 0.863244,loss:0.360731\n",
      "[epoch:4,batch:239]:acc: 0.862370,loss:0.363692\n",
      "[epoch:4,batch:269]:acc: 0.861806,loss:0.363287\n",
      "[epoch:4,batch:299]:acc: 0.859375,loss:0.369406\n",
      "[epoch:4,batch:299]: val_loss:0.463146,val_acc:0.837850,val_total:4539\n",
      "[epoch:4,batch:329]:acc: 0.858239,loss:0.371994\n",
      "[epoch:4,batch:359]:acc: 0.858333,loss:0.370625\n",
      "[epoch:4,batch:389]:acc: 0.856571,loss:0.372786\n",
      "[epoch:4,batch:419]:acc: 0.856176,loss:0.372134\n",
      "[epoch:4,batch:449]:acc: 0.853889,loss:0.376210\n",
      "[epoch:4,batch:479]:acc: 0.853646,loss:0.376872\n",
      "[epoch:4,batch:509]:acc: 0.853309,loss:0.377677\n",
      "[epoch:4,batch:539]:acc: 0.853762,loss:0.376284\n",
      "[epoch:4,batch:569]:acc: 0.854002,loss:0.378753\n",
      "[epoch:4,batch:599]:acc: 0.853750,loss:0.378609\n",
      "[epoch:4,batch:599]: val_loss:0.444005,val_acc:0.838731,val_total:4539\n",
      "[epoch:4,batch:629]:acc: 0.853919,loss:0.376196\n",
      "[epoch:4,batch:659]:acc: 0.853646,loss:0.374722\n",
      "[epoch:4,batch:689]:acc: 0.852899,loss:0.375661\n",
      "[epoch:4,batch:719]:acc: 0.852170,loss:0.378279\n",
      "[epoch:4,batch:749]:acc: 0.852625,loss:0.379119\n",
      "[epoch:4,batch:779]:acc: 0.853085,loss:0.377466\n",
      "[epoch:4,batch:809]:acc: 0.852353,loss:0.378934\n",
      "[epoch:4,batch:839]:acc: 0.852009,loss:0.379025\n",
      "[epoch:4,batch:869]:acc: 0.852155,loss:0.378695\n",
      "[epoch:4,batch:899]:acc: 0.851806,loss:0.378813\n",
      "[epoch:4,batch:899]: val_loss:0.436586,val_acc:0.837629,val_total:4539\n",
      "[epoch:4,batch:929]:acc: 0.851781,loss:0.377899\n",
      "[epoch:4,batch:959]:acc: 0.851790,loss:0.378054\n",
      "[epoch:4,batch:989]:acc: 0.852020,loss:0.377688\n",
      "[epoch:4] :acc: 0.851814,loss:0.378142,lr:0.000030,patience:0\n",
      "[epoch:4]: val_loss:0.429094,val_acc:0.842697,\n",
      "save new model loss,now loss is  0.42909419536590576\n",
      "save new model acc,now acc is  tensor(0.8427, device='cuda:0')\n",
      "Epoch 5/59\n",
      "----------\n",
      "set lr=:0.000030,momentum=0.950000\n",
      "[epoch:5,batch:29]:acc: 0.881250,loss:0.315610\n",
      "[epoch:5,batch:59]:acc: 0.870833,loss:0.319674\n",
      "[epoch:5,batch:89]:acc: 0.863194,loss:0.333108\n",
      "[epoch:5,batch:119]:acc: 0.866146,loss:0.322614\n",
      "[epoch:5,batch:149]:acc: 0.865625,loss:0.328012\n",
      "[epoch:5,batch:179]:acc: 0.866667,loss:0.324311\n",
      "[epoch:5,batch:209]:acc: 0.866815,loss:0.323606\n",
      "[epoch:5,batch:239]:acc: 0.866667,loss:0.324530\n",
      "[epoch:5,batch:269]:acc: 0.867361,loss:0.322866\n",
      "[epoch:5,batch:299]:acc: 0.868021,loss:0.318562\n",
      "[epoch:5,batch:299]: val_loss:0.454033,val_acc:0.842697,val_total:4539\n",
      "[epoch:5,batch:329]:acc: 0.866572,loss:0.322131\n",
      "[epoch:5,batch:359]:acc: 0.867708,loss:0.322906\n",
      "[epoch:5,batch:389]:acc: 0.867548,loss:0.324393\n",
      "[epoch:5,batch:419]:acc: 0.867485,loss:0.323427\n",
      "[epoch:5,batch:449]:acc: 0.868333,loss:0.321759\n",
      "[epoch:5,batch:479]:acc: 0.869076,loss:0.321124\n",
      "[epoch:5,batch:509]:acc: 0.869118,loss:0.319565\n",
      "[epoch:5,batch:539]:acc: 0.870197,loss:0.318444\n",
      "[epoch:5,batch:569]:acc: 0.870066,loss:0.320165\n",
      "[epoch:5,batch:599]:acc: 0.869740,loss:0.320997\n",
      "[epoch:5,batch:599]: val_loss:0.432510,val_acc:0.845561,val_total:4539\n",
      "[epoch:5,batch:629]:acc: 0.869742,loss:0.320134\n",
      "[epoch:5,batch:659]:acc: 0.869271,loss:0.320345\n",
      "[epoch:5,batch:689]:acc: 0.869611,loss:0.319200\n",
      "[epoch:5,batch:719]:acc: 0.870139,loss:0.317049\n",
      "[epoch:5,batch:749]:acc: 0.869625,loss:0.316942\n",
      "[epoch:5,batch:779]:acc: 0.869151,loss:0.317404\n",
      "[epoch:5,batch:809]:acc: 0.869252,loss:0.317914\n",
      "[epoch:5,batch:839]:acc: 0.869643,loss:0.318785\n",
      "[epoch:5,batch:869]:acc: 0.869684,loss:0.318080\n",
      "[epoch:5,batch:899]:acc: 0.870486,loss:0.317612\n",
      "[epoch:5,batch:899]: val_loss:0.421418,val_acc:0.849526,val_total:4539\n",
      "[epoch:5,batch:929]:acc: 0.870800,loss:0.317150\n",
      "[epoch:5,batch:959]:acc: 0.870280,loss:0.318934\n",
      "[epoch:5,batch:989]:acc: 0.869886,loss:0.320450\n",
      "[epoch:5] :acc: 0.869817,loss:0.320653,lr:0.000030,patience:0\n",
      "[epoch:5]: val_loss:0.436055,val_acc:0.843358,\n",
      "save new model acc,now acc is  tensor(0.8434, device='cuda:0')\n",
      "Epoch 6/59\n",
      "----------\n",
      "set lr=:0.000100,momentum=0.900000\n",
      "[epoch:6,batch:29]:acc: 0.864583,loss:0.354020\n",
      "[epoch:6,batch:59]:acc: 0.861979,loss:0.341366\n",
      "[epoch:6,batch:89]:acc: 0.858333,loss:0.361823\n",
      "[epoch:6,batch:119]:acc: 0.853906,loss:0.370279\n",
      "[epoch:6,batch:149]:acc: 0.848333,loss:0.381717\n",
      "[epoch:6,batch:179]:acc: 0.847569,loss:0.391639\n",
      "[epoch:6,batch:209]:acc: 0.843452,loss:0.403305\n",
      "[epoch:6,batch:239]:acc: 0.842839,loss:0.411643\n",
      "[epoch:6,batch:269]:acc: 0.838657,loss:0.421744\n",
      "[epoch:6,batch:299]:acc: 0.838437,loss:0.424180\n",
      "[epoch:6,batch:299]: val_loss:0.549456,val_acc:0.813836,val_total:4539\n",
      "[epoch:6,batch:329]:acc: 0.836932,loss:0.428061\n",
      "[epoch:6,batch:359]:acc: 0.836719,loss:0.428447\n",
      "[epoch:6,batch:389]:acc: 0.837580,loss:0.427423\n",
      "[epoch:6,batch:419]:acc: 0.837872,loss:0.426383\n",
      "[epoch:6,batch:449]:acc: 0.838958,loss:0.425486\n",
      "[epoch:6,batch:479]:acc: 0.839714,loss:0.424422\n",
      "[epoch:6,batch:509]:acc: 0.838909,loss:0.425813\n",
      "[epoch:6,batch:539]:acc: 0.837963,loss:0.426720\n",
      "[epoch:6,batch:569]:acc: 0.838432,loss:0.424144\n",
      "[epoch:6,batch:599]:acc: 0.837917,loss:0.425713\n",
      "[epoch:6,batch:599]: val_loss:0.505638,val_acc:0.823750,val_total:4539\n",
      "[epoch:6,batch:629]:acc: 0.837202,loss:0.428072\n",
      "[epoch:6,batch:659]:acc: 0.837500,loss:0.428155\n",
      "[epoch:6,batch:689]:acc: 0.837138,loss:0.426754\n",
      "[epoch:6,batch:719]:acc: 0.837196,loss:0.425845\n",
      "[epoch:6,batch:749]:acc: 0.837583,loss:0.424012\n",
      "[epoch:6,batch:779]:acc: 0.837260,loss:0.423896\n",
      "[epoch:6,batch:809]:acc: 0.837037,loss:0.424398\n",
      "[epoch:6,batch:839]:acc: 0.837016,loss:0.424467\n",
      "[epoch:6,batch:869]:acc: 0.835812,loss:0.427246\n",
      "[epoch:6,batch:899]:acc: 0.835243,loss:0.428431\n",
      "[epoch:6,batch:899]: val_loss:0.478780,val_acc:0.834765,val_total:4539\n",
      "[epoch:6,batch:929]:acc: 0.835013,loss:0.428405\n",
      "[epoch:6,batch:959]:acc: 0.835156,loss:0.428145\n",
      "[epoch:6,batch:989]:acc: 0.835101,loss:0.429163\n",
      "[epoch:6] :acc: 0.835010,loss:0.429420,lr:0.000100,patience:1\n",
      "[epoch:6]: val_loss:0.495802,val_acc:0.816700,\n",
      "Epoch 7/59\n",
      "----------\n",
      "loss has increased lr divide 10 lr now is :0.000020\n",
      "[epoch:7,batch:29]:acc: 0.885417,loss:0.295460\n",
      "[epoch:7,batch:59]:acc: 0.885417,loss:0.293312\n",
      "[epoch:7,batch:89]:acc: 0.884028,loss:0.296585\n",
      "[epoch:7,batch:119]:acc: 0.883333,loss:0.291506\n",
      "[epoch:7,batch:149]:acc: 0.886458,loss:0.287221\n",
      "[epoch:7,batch:179]:acc: 0.884896,loss:0.289248\n",
      "[epoch:7,batch:209]:acc: 0.884673,loss:0.288907\n",
      "[epoch:7,batch:239]:acc: 0.884245,loss:0.287946\n",
      "[epoch:7,batch:269]:acc: 0.885880,loss:0.284763\n",
      "[epoch:7,batch:299]:acc: 0.885104,loss:0.286103\n",
      "[epoch:7,batch:299]: val_loss:0.416263,val_acc:0.850408,val_total:4539\n",
      "[epoch:7,batch:329]:acc: 0.882860,loss:0.287626\n",
      "[epoch:7,batch:359]:acc: 0.882465,loss:0.290289\n",
      "[epoch:7,batch:389]:acc: 0.879808,loss:0.294414\n",
      "[epoch:7,batch:419]:acc: 0.878646,loss:0.294150\n",
      "[epoch:7,batch:449]:acc: 0.878750,loss:0.295507\n",
      "[epoch:7,batch:479]:acc: 0.878060,loss:0.299486\n",
      "[epoch:7,batch:509]:acc: 0.877696,loss:0.301584\n",
      "[epoch:7,batch:539]:acc: 0.877546,loss:0.302436\n",
      "[epoch:7,batch:569]:acc: 0.878125,loss:0.301253\n",
      "[epoch:7,batch:599]:acc: 0.877865,loss:0.301898\n",
      "[epoch:7,batch:599]: val_loss:0.404831,val_acc:0.849306,val_total:4539\n",
      "[epoch:7,batch:629]:acc: 0.878522,loss:0.300914\n",
      "[epoch:7,batch:659]:acc: 0.878362,loss:0.301722\n",
      "[epoch:7,batch:689]:acc: 0.878306,loss:0.301253\n",
      "[epoch:7,batch:719]:acc: 0.877604,loss:0.302191\n",
      "[epoch:7,batch:749]:acc: 0.877542,loss:0.301922\n",
      "[epoch:7,batch:779]:acc: 0.878325,loss:0.301161\n",
      "[epoch:7,batch:809]:acc: 0.878356,loss:0.300399\n",
      "[epoch:7,batch:839]:acc: 0.877976,loss:0.301262\n",
      "[epoch:7,batch:869]:acc: 0.877838,loss:0.300429\n",
      "[epoch:7,batch:899]:acc: 0.878056,loss:0.300538\n",
      "[epoch:7,batch:899]: val_loss:0.408663,val_acc:0.855475,val_total:4539\n",
      "[epoch:7,batch:929]:acc: 0.877453,loss:0.301458\n",
      "[epoch:7,batch:959]:acc: 0.877051,loss:0.302159\n",
      "[epoch:7,batch:989]:acc: 0.876831,loss:0.302697\n",
      "[epoch:7] :acc: 0.876817,loss:0.304506,lr:0.000020,patience:0\n",
      "[epoch:7]: val_loss:0.409927,val_acc:0.857237,\n",
      "save new model loss,now loss is  0.4099271893501282\n",
      "save new model acc,now acc is  tensor(0.8572, device='cuda:0')\n",
      "Epoch 8/59\n",
      "----------\n",
      "[epoch:8,batch:29]:acc: 0.887500,loss:0.267285\n",
      "[epoch:8,batch:59]:acc: 0.894271,loss:0.250802\n",
      "[epoch:8,batch:89]:acc: 0.890972,loss:0.260775\n",
      "[epoch:8,batch:119]:acc: 0.889844,loss:0.262502\n",
      "[epoch:8,batch:149]:acc: 0.888125,loss:0.263712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:8,batch:179]:acc: 0.889236,loss:0.260248\n",
      "[epoch:8,batch:209]:acc: 0.891220,loss:0.256016\n",
      "[epoch:8,batch:239]:acc: 0.889062,loss:0.258743\n",
      "[epoch:8,batch:269]:acc: 0.889236,loss:0.260323\n",
      "[epoch:8,batch:299]:acc: 0.890208,loss:0.259771\n",
      "[epoch:8,batch:299]: val_loss:0.419151,val_acc:0.855254,val_total:4539\n",
      "[epoch:8,batch:329]:acc: 0.890152,loss:0.258388\n",
      "[epoch:8,batch:359]:acc: 0.891146,loss:0.256639\n",
      "[epoch:8,batch:389]:acc: 0.891346,loss:0.257104\n",
      "[epoch:8,batch:419]:acc: 0.890253,loss:0.259702\n",
      "[epoch:8,batch:449]:acc: 0.889931,loss:0.259357\n",
      "[epoch:8,batch:479]:acc: 0.889974,loss:0.260558\n",
      "[epoch:8,batch:509]:acc: 0.890012,loss:0.260686\n",
      "[epoch:8,batch:539]:acc: 0.889757,loss:0.260496\n",
      "[epoch:8,batch:569]:acc: 0.890022,loss:0.259752\n",
      "[epoch:8,batch:599]:acc: 0.889635,loss:0.261353\n",
      "[epoch:8,batch:599]: val_loss:0.422214,val_acc:0.846883,val_total:4539\n",
      "[epoch:8,batch:629]:acc: 0.889534,loss:0.261662\n",
      "[epoch:8,batch:659]:acc: 0.889725,loss:0.261258\n",
      "[epoch:8,batch:689]:acc: 0.888678,loss:0.261701\n",
      "[epoch:8,batch:719]:acc: 0.888585,loss:0.262480\n",
      "[epoch:8,batch:749]:acc: 0.887583,loss:0.263815\n",
      "[epoch:8,batch:779]:acc: 0.887580,loss:0.265541\n",
      "[epoch:8,batch:809]:acc: 0.887924,loss:0.263934\n",
      "[epoch:8,batch:839]:acc: 0.888021,loss:0.263212\n",
      "[epoch:8,batch:869]:acc: 0.887751,loss:0.263426\n",
      "[epoch:8,batch:899]:acc: 0.887465,loss:0.264073\n",
      "[epoch:8,batch:899]: val_loss:0.433983,val_acc:0.848645,val_total:4539\n",
      "[epoch:8,batch:929]:acc: 0.887231,loss:0.264360\n",
      "[epoch:8,batch:959]:acc: 0.886914,loss:0.264719\n",
      "[epoch:8,batch:989]:acc: 0.886395,loss:0.265976\n",
      "[epoch:8] :acc: 0.886465,loss:0.265795,lr:0.000020,patience:0\n",
      "[epoch:8]: val_loss:0.433097,val_acc:0.850187,\n",
      "Epoch 9/59\n",
      "----------\n",
      "[epoch:9,batch:29]:acc: 0.898958,loss:0.244736\n",
      "[epoch:9,batch:59]:acc: 0.907813,loss:0.217088\n",
      "[epoch:9,batch:89]:acc: 0.905208,loss:0.221513\n",
      "[epoch:9,batch:119]:acc: 0.905990,loss:0.214538\n",
      "[epoch:9,batch:149]:acc: 0.905417,loss:0.217456\n",
      "[epoch:9,batch:179]:acc: 0.902431,loss:0.220963\n",
      "[epoch:9,batch:209]:acc: 0.902827,loss:0.219945\n",
      "[epoch:9,batch:239]:acc: 0.905208,loss:0.218095\n",
      "[epoch:9,batch:269]:acc: 0.905324,loss:0.219149\n",
      "[epoch:9,batch:299]:acc: 0.903854,loss:0.220367\n",
      "[epoch:9,batch:299]: val_loss:0.430256,val_acc:0.849747,val_total:4539\n",
      "[epoch:9,batch:329]:acc: 0.904545,loss:0.221376\n",
      "[epoch:9,batch:359]:acc: 0.902951,loss:0.226136\n",
      "[epoch:9,batch:389]:acc: 0.903365,loss:0.225729\n",
      "[epoch:9,batch:419]:acc: 0.902009,loss:0.227699\n",
      "[epoch:9,batch:449]:acc: 0.901319,loss:0.228159\n",
      "[epoch:9,batch:479]:acc: 0.900391,loss:0.229338\n",
      "[epoch:9,batch:509]:acc: 0.900184,loss:0.229047\n",
      "[epoch:9,batch:539]:acc: 0.899769,loss:0.228356\n",
      "[epoch:9,batch:569]:acc: 0.900164,loss:0.228527\n",
      "[epoch:9,batch:599]:acc: 0.900000,loss:0.228380\n",
      "[epoch:9,batch:599]: val_loss:0.425788,val_acc:0.853051,val_total:4539\n",
      "[epoch:9,batch:629]:acc: 0.899653,loss:0.229868\n",
      "[epoch:9,batch:659]:acc: 0.898674,loss:0.231787\n",
      "[epoch:9,batch:689]:acc: 0.898777,loss:0.232458\n",
      "[epoch:9,batch:719]:acc: 0.898134,loss:0.233908\n",
      "[epoch:9,batch:749]:acc: 0.897750,loss:0.234727\n",
      "[epoch:9,batch:779]:acc: 0.897997,loss:0.234916\n",
      "[epoch:9,batch:809]:acc: 0.897492,loss:0.235342\n",
      "[epoch:9,batch:839]:acc: 0.898214,loss:0.234539\n",
      "[epoch:9,batch:869]:acc: 0.898204,loss:0.235241\n",
      "[epoch:9,batch:899]:acc: 0.898681,loss:0.235030\n",
      "[epoch:9,batch:899]: val_loss:0.436897,val_acc:0.851068,val_total:4539\n",
      "[epoch:9,batch:929]:acc: 0.898118,loss:0.236149\n",
      "[epoch:9,batch:959]:acc: 0.897656,loss:0.236755\n",
      "[epoch:9,batch:989]:acc: 0.897380,loss:0.237281\n",
      "[epoch:9] :acc: 0.897311,loss:0.237223,lr:0.000020,patience:1\n",
      "[epoch:9]: val_loss:0.435264,val_acc:0.849967,\n",
      "Epoch 10/59\n",
      "----------\n",
      "loss has increased lr divide 10 lr now is :0.000004\n",
      "[epoch:10,batch:29]:acc: 0.895833,loss:0.239456\n",
      "[epoch:10,batch:59]:acc: 0.897917,loss:0.242326\n",
      "[epoch:10,batch:89]:acc: 0.897222,loss:0.248958\n",
      "[epoch:10,batch:119]:acc: 0.900521,loss:0.248414\n",
      "[epoch:10,batch:149]:acc: 0.902708,loss:0.244534\n",
      "[epoch:10,batch:179]:acc: 0.902431,loss:0.243213\n",
      "[epoch:10,batch:209]:acc: 0.902530,loss:0.243416\n",
      "[epoch:10,batch:239]:acc: 0.900911,loss:0.243902\n",
      "[epoch:10,batch:269]:acc: 0.901736,loss:0.241813\n",
      "[epoch:10,batch:299]:acc: 0.899896,loss:0.243260\n",
      "[epoch:10,batch:299]: val_loss:0.399580,val_acc:0.860542,val_total:4539\n",
      "[epoch:10,batch:329]:acc: 0.901610,loss:0.240378\n",
      "[epoch:10,batch:359]:acc: 0.903125,loss:0.239415\n",
      "[epoch:10,batch:389]:acc: 0.903045,loss:0.238447\n",
      "[epoch:10,batch:419]:acc: 0.902083,loss:0.238792\n",
      "[epoch:10,batch:449]:acc: 0.902917,loss:0.237780\n",
      "[epoch:10,batch:479]:acc: 0.903906,loss:0.237717\n",
      "[epoch:10,batch:509]:acc: 0.903983,loss:0.238980\n",
      "[epoch:10,batch:539]:acc: 0.904167,loss:0.239294\n",
      "[epoch:10,batch:569]:acc: 0.903344,loss:0.240363\n",
      "[epoch:10,batch:599]:acc: 0.902604,loss:0.242538\n",
      "[epoch:10,batch:599]: val_loss:0.398030,val_acc:0.861423,val_total:4539\n",
      "[epoch:10,batch:629]:acc: 0.902034,loss:0.243498\n",
      "[epoch:10,batch:659]:acc: 0.901563,loss:0.244495\n",
      "[epoch:10,batch:689]:acc: 0.901268,loss:0.245433\n",
      "[epoch:10,batch:719]:acc: 0.900955,loss:0.244943\n",
      "[epoch:10,batch:749]:acc: 0.900583,loss:0.245286\n",
      "[epoch:10,batch:779]:acc: 0.900361,loss:0.245390\n",
      "[epoch:10,batch:809]:acc: 0.900386,loss:0.245930\n",
      "[epoch:10,batch:839]:acc: 0.899851,loss:0.246503\n",
      "[epoch:10,batch:869]:acc: 0.899964,loss:0.246141\n",
      "[epoch:10,batch:899]:acc: 0.900174,loss:0.245458\n",
      "[epoch:10,batch:899]: val_loss:0.395468,val_acc:0.862304,val_total:4539\n",
      "[epoch:10,batch:929]:acc: 0.900202,loss:0.244411\n",
      "[epoch:10,batch:959]:acc: 0.899935,loss:0.244448\n",
      "[epoch:10,batch:989]:acc: 0.899874,loss:0.244000\n",
      "[epoch:10] :acc: 0.899864,loss:0.244197,lr:0.000004,patience:0\n",
      "[epoch:10]: val_loss:0.397214,val_acc:0.861864,\n",
      "save new model loss,now loss is  0.39721372723579407\n",
      "save new model acc,now acc is  tensor(0.8619, device='cuda:0')\n",
      "Epoch 11/59\n",
      "----------\n",
      "[epoch:11,batch:29]:acc: 0.893750,loss:0.255068\n",
      "[epoch:11,batch:59]:acc: 0.900000,loss:0.232283\n",
      "[epoch:11,batch:89]:acc: 0.899306,loss:0.230133\n",
      "[epoch:11,batch:119]:acc: 0.902604,loss:0.227271\n",
      "[epoch:11,batch:149]:acc: 0.902083,loss:0.227796\n",
      "[epoch:11,batch:179]:acc: 0.903125,loss:0.227487\n",
      "[epoch:11,batch:209]:acc: 0.903571,loss:0.226810\n",
      "[epoch:11,batch:239]:acc: 0.903906,loss:0.226083\n",
      "[epoch:11,batch:269]:acc: 0.904167,loss:0.224172\n",
      "[epoch:11,batch:299]:acc: 0.904479,loss:0.222763\n",
      "[epoch:11,batch:299]: val_loss:0.393642,val_acc:0.859220,val_total:4539\n",
      "[epoch:11,batch:329]:acc: 0.905777,loss:0.220059\n",
      "[epoch:11,batch:359]:acc: 0.905642,loss:0.221560\n",
      "[epoch:11,batch:389]:acc: 0.906971,loss:0.219337\n",
      "[epoch:11,batch:419]:acc: 0.907366,loss:0.219140\n",
      "[epoch:11,batch:449]:acc: 0.905833,loss:0.221065\n",
      "[epoch:11,batch:479]:acc: 0.906250,loss:0.222032\n",
      "[epoch:11,batch:509]:acc: 0.906127,loss:0.222744\n",
      "[epoch:11,batch:539]:acc: 0.906308,loss:0.223486\n",
      "[epoch:11,batch:569]:acc: 0.906360,loss:0.223756\n",
      "[epoch:11,batch:599]:acc: 0.906667,loss:0.223537\n",
      "[epoch:11,batch:599]: val_loss:0.400635,val_acc:0.860762,val_total:4539\n",
      "[epoch:11,batch:629]:acc: 0.906845,loss:0.222874\n",
      "[epoch:11,batch:659]:acc: 0.906723,loss:0.223592\n",
      "[epoch:11,batch:689]:acc: 0.906069,loss:0.223934\n",
      "[epoch:11,batch:719]:acc: 0.905425,loss:0.225887\n",
      "[epoch:11,batch:749]:acc: 0.906083,loss:0.225502\n",
      "[epoch:11,batch:779]:acc: 0.906490,loss:0.226343\n",
      "[epoch:11,batch:809]:acc: 0.906211,loss:0.227123\n",
      "[epoch:11,batch:839]:acc: 0.905990,loss:0.226798\n",
      "[epoch:11,batch:869]:acc: 0.905316,loss:0.227638\n",
      "[epoch:11,batch:899]:acc: 0.905069,loss:0.227726\n",
      "[epoch:11,batch:899]: val_loss:0.402401,val_acc:0.857237,val_total:4539\n",
      "[epoch:11,batch:929]:acc: 0.904872,loss:0.228265\n",
      "[epoch:11,batch:959]:acc: 0.904948,loss:0.227988\n",
      "[epoch:11,batch:989]:acc: 0.904893,loss:0.228039\n",
      "[epoch:11] :acc: 0.904846,loss:0.228010,lr:0.000004,patience:0\n",
      "[epoch:11]: val_loss:0.398537,val_acc:0.860322,\n",
      "Epoch 12/59\n",
      "----------\n",
      "[epoch:12,batch:29]:acc: 0.897917,loss:0.244633\n",
      "[epoch:12,batch:59]:acc: 0.900521,loss:0.237425\n",
      "[epoch:12,batch:89]:acc: 0.902083,loss:0.227427\n",
      "[epoch:12,batch:119]:acc: 0.904427,loss:0.221421\n",
      "[epoch:12,batch:149]:acc: 0.903958,loss:0.224901\n",
      "[epoch:12,batch:179]:acc: 0.906944,loss:0.219207\n",
      "[epoch:12,batch:209]:acc: 0.906696,loss:0.219262\n",
      "[epoch:12,batch:239]:acc: 0.908724,loss:0.219408\n",
      "[epoch:12,batch:269]:acc: 0.907292,loss:0.220845\n",
      "[epoch:12,batch:299]:acc: 0.906042,loss:0.222591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:12,batch:299]: val_loss:0.402198,val_acc:0.859440,val_total:4539\n",
      "[epoch:12,batch:329]:acc: 0.907576,loss:0.219628\n",
      "[epoch:12,batch:359]:acc: 0.906076,loss:0.221036\n",
      "[epoch:12,batch:389]:acc: 0.906330,loss:0.220782\n",
      "[epoch:12,batch:419]:acc: 0.905729,loss:0.220070\n",
      "[epoch:12,batch:449]:acc: 0.905903,loss:0.220211\n",
      "[epoch:12,batch:479]:acc: 0.906315,loss:0.219689\n",
      "[epoch:12,batch:509]:acc: 0.906740,loss:0.218932\n",
      "[epoch:12,batch:539]:acc: 0.907523,loss:0.217917\n",
      "[epoch:12,batch:569]:acc: 0.907950,loss:0.216861\n",
      "[epoch:12,batch:599]:acc: 0.907760,loss:0.216363\n",
      "[epoch:12,batch:599]: val_loss:0.402418,val_acc:0.862965,val_total:4539\n",
      "[epoch:12,batch:629]:acc: 0.907143,loss:0.218130\n",
      "[epoch:12,batch:659]:acc: 0.907055,loss:0.218231\n",
      "[epoch:12,batch:689]:acc: 0.907971,loss:0.217140\n",
      "[epoch:12,batch:719]:acc: 0.908420,loss:0.216532\n",
      "[epoch:12,batch:749]:acc: 0.908583,loss:0.216596\n",
      "[epoch:12,batch:779]:acc: 0.909375,loss:0.215824\n",
      "[epoch:12,batch:809]:acc: 0.909877,loss:0.215152\n",
      "[epoch:12,batch:839]:acc: 0.909710,loss:0.215493\n",
      "[epoch:12,batch:869]:acc: 0.909950,loss:0.215214\n",
      "[epoch:12,batch:899]:acc: 0.910035,loss:0.215486\n",
      "[epoch:12,batch:899]: val_loss:0.401915,val_acc:0.860983,val_total:4539\n",
      "[epoch:12,batch:929]:acc: 0.910047,loss:0.216029\n",
      "[epoch:12,batch:959]:acc: 0.910547,loss:0.215462\n",
      "[epoch:12,batch:989]:acc: 0.910417,loss:0.215677\n",
      "[epoch:12] :acc: 0.910395,loss:0.215733,lr:0.000004,patience:1\n",
      "[epoch:12]: val_loss:0.403174,val_acc:0.861643,\n",
      "Epoch 13/59\n",
      "----------\n",
      "loss has increased lr divide 10 lr now is :0.000001\n",
      "[epoch:13,batch:29]:acc: 0.905208,loss:0.223700\n",
      "[epoch:13,batch:59]:acc: 0.904167,loss:0.227364\n",
      "[epoch:13,batch:89]:acc: 0.902778,loss:0.230421\n",
      "[epoch:13,batch:119]:acc: 0.904167,loss:0.230919\n",
      "[epoch:13,batch:149]:acc: 0.902708,loss:0.229651\n",
      "[epoch:13,batch:179]:acc: 0.903646,loss:0.228358\n",
      "[epoch:13,batch:209]:acc: 0.903869,loss:0.233226\n",
      "[epoch:13,batch:239]:acc: 0.905729,loss:0.233749\n",
      "[epoch:13,batch:269]:acc: 0.906134,loss:0.234191\n",
      "[epoch:13,batch:299]:acc: 0.906458,loss:0.233348\n",
      "[epoch:13,batch:299]: val_loss:0.395717,val_acc:0.861423,val_total:4539\n",
      "[epoch:13,batch:329]:acc: 0.905871,loss:0.232304\n",
      "[epoch:13,batch:359]:acc: 0.907292,loss:0.228949\n",
      "[epoch:13,batch:389]:acc: 0.907292,loss:0.229042\n",
      "[epoch:13,batch:419]:acc: 0.907515,loss:0.227872\n",
      "[epoch:13,batch:449]:acc: 0.907569,loss:0.227681\n",
      "[epoch:13,batch:479]:acc: 0.906901,loss:0.229420\n",
      "[epoch:13,batch:509]:acc: 0.906556,loss:0.229849\n",
      "[epoch:13,batch:539]:acc: 0.906887,loss:0.229472\n",
      "[epoch:13,batch:569]:acc: 0.906305,loss:0.229851\n",
      "[epoch:13,batch:599]:acc: 0.905365,loss:0.230346\n",
      "[epoch:13,batch:599]: val_loss:0.396614,val_acc:0.862304,val_total:4539\n",
      "[epoch:13,batch:629]:acc: 0.905903,loss:0.228217\n",
      "[epoch:13,batch:659]:acc: 0.906818,loss:0.226547\n",
      "[epoch:13,batch:689]:acc: 0.907111,loss:0.225628\n",
      "[epoch:13,batch:719]:acc: 0.906944,loss:0.225975\n",
      "[epoch:13,batch:749]:acc: 0.907708,loss:0.224819\n",
      "[epoch:13,batch:779]:acc: 0.907292,loss:0.224855\n",
      "[epoch:13,batch:809]:acc: 0.906674,loss:0.225986\n",
      "[epoch:13,batch:839]:acc: 0.906510,loss:0.225702\n",
      "[epoch:13,batch:869]:acc: 0.906609,loss:0.225144\n",
      "[epoch:13,batch:899]:acc: 0.905729,loss:0.225594\n",
      "[epoch:13,batch:899]: val_loss:0.392768,val_acc:0.860322,val_total:4539\n",
      "[epoch:13,batch:929]:acc: 0.905712,loss:0.225507\n",
      "[epoch:13,batch:959]:acc: 0.905566,loss:0.225438\n",
      "[epoch:13,batch:989]:acc: 0.905745,loss:0.225169\n",
      "[epoch:13] :acc: 0.905666,loss:0.225621,lr:0.000001,patience:0\n",
      "[epoch:13]: val_loss:0.396496,val_acc:0.860983,\n",
      "save new model loss,now loss is  0.3964957594871521\n",
      "Epoch 14/59\n",
      "----------\n",
      "[epoch:14,batch:29]:acc: 0.914583,loss:0.223119\n",
      "[epoch:14,batch:59]:acc: 0.904687,loss:0.226371\n",
      "[epoch:14,batch:89]:acc: 0.908333,loss:0.222360\n",
      "[epoch:14,batch:119]:acc: 0.908854,loss:0.219555\n",
      "[epoch:14,batch:149]:acc: 0.911250,loss:0.216557\n",
      "[epoch:14,batch:179]:acc: 0.910764,loss:0.215563\n",
      "[epoch:14,batch:209]:acc: 0.910119,loss:0.214106\n",
      "[epoch:14,batch:239]:acc: 0.909245,loss:0.216334\n",
      "[epoch:14,batch:269]:acc: 0.909491,loss:0.213480\n",
      "[epoch:14,batch:299]:acc: 0.909792,loss:0.213500\n",
      "[epoch:14,batch:299]: val_loss:0.394795,val_acc:0.860542,val_total:4539\n",
      "[epoch:14,batch:329]:acc: 0.908523,loss:0.216770\n",
      "[epoch:14,batch:359]:acc: 0.908160,loss:0.217225\n",
      "[epoch:14,batch:389]:acc: 0.907051,loss:0.220186\n",
      "[epoch:14,batch:419]:acc: 0.905952,loss:0.222206\n",
      "[epoch:14,batch:449]:acc: 0.906181,loss:0.222749\n",
      "[epoch:14,batch:479]:acc: 0.906250,loss:0.222332\n",
      "[epoch:14,batch:509]:acc: 0.906127,loss:0.222935\n",
      "[epoch:14,batch:539]:acc: 0.905903,loss:0.222731\n",
      "[epoch:14,batch:569]:acc: 0.906250,loss:0.223269\n",
      "[epoch:14,batch:599]:acc: 0.905833,loss:0.224264\n",
      "[epoch:14,batch:599]: val_loss:0.393480,val_acc:0.860542,val_total:4539\n",
      "[epoch:14,batch:629]:acc: 0.906448,loss:0.223740\n",
      "[epoch:14,batch:659]:acc: 0.905871,loss:0.223758\n",
      "[epoch:14,batch:689]:acc: 0.905842,loss:0.224446\n",
      "[epoch:14,batch:719]:acc: 0.905035,loss:0.225042\n",
      "[epoch:14,batch:749]:acc: 0.905458,loss:0.225310\n",
      "[epoch:14,batch:779]:acc: 0.905609,loss:0.224719\n",
      "[epoch:14,batch:809]:acc: 0.905594,loss:0.224442\n",
      "[epoch:14,batch:839]:acc: 0.905357,loss:0.224677\n",
      "[epoch:14,batch:869]:acc: 0.904921,loss:0.225086\n",
      "[epoch:14,batch:899]:acc: 0.904410,loss:0.225581\n",
      "[epoch:14,batch:899]: val_loss:0.393581,val_acc:0.861643,val_total:4539\n",
      "[epoch:14,batch:929]:acc: 0.904503,loss:0.224781\n",
      "[epoch:14,batch:959]:acc: 0.904687,loss:0.224693\n",
      "[epoch:14,batch:989]:acc: 0.904545,loss:0.224957\n",
      "[epoch:14] :acc: 0.904594,loss:0.224915,lr:0.000001,patience:0\n",
      "[epoch:14]: val_loss:0.400213,val_acc:0.859440,\n",
      "Epoch 15/59\n",
      "----------\n",
      "[epoch:15,batch:29]:acc: 0.900000,loss:0.211125\n",
      "[epoch:15,batch:59]:acc: 0.905208,loss:0.218771\n",
      "[epoch:15,batch:89]:acc: 0.905903,loss:0.215831\n",
      "[epoch:15,batch:119]:acc: 0.908073,loss:0.215657\n",
      "[epoch:15,batch:149]:acc: 0.910000,loss:0.212420\n",
      "[epoch:15,batch:179]:acc: 0.908854,loss:0.210158\n",
      "[epoch:15,batch:209]:acc: 0.909524,loss:0.210248\n",
      "[epoch:15,batch:239]:acc: 0.910417,loss:0.208510\n",
      "[epoch:15,batch:269]:acc: 0.911921,loss:0.205256\n",
      "[epoch:15,batch:299]:acc: 0.912917,loss:0.203888\n",
      "[epoch:15,batch:299]: val_loss:0.395120,val_acc:0.860762,val_total:4539\n",
      "[epoch:15,batch:329]:acc: 0.912973,loss:0.203486\n",
      "[epoch:15,batch:359]:acc: 0.914410,loss:0.202915\n",
      "[epoch:15,batch:389]:acc: 0.914343,loss:0.203728\n",
      "[epoch:15,batch:419]:acc: 0.914509,loss:0.203641\n",
      "[epoch:15,batch:449]:acc: 0.912569,loss:0.206815\n",
      "[epoch:15,batch:479]:acc: 0.912435,loss:0.207175\n",
      "[epoch:15,batch:509]:acc: 0.912071,loss:0.208008\n",
      "[epoch:15,batch:539]:acc: 0.911400,loss:0.209609\n",
      "[epoch:15,batch:569]:acc: 0.910965,loss:0.209943\n",
      "[epoch:15,batch:599]:acc: 0.910365,loss:0.211085\n",
      "[epoch:15,batch:599]: val_loss:0.396505,val_acc:0.860322,val_total:4539\n",
      "[epoch:15,batch:629]:acc: 0.910317,loss:0.212236\n",
      "[epoch:15,batch:659]:acc: 0.910275,loss:0.212580\n",
      "[epoch:15,batch:689]:acc: 0.910236,loss:0.212543\n",
      "[epoch:15,batch:719]:acc: 0.909635,loss:0.212873\n",
      "[epoch:15,batch:749]:acc: 0.909542,loss:0.212984\n",
      "[epoch:15,batch:779]:acc: 0.909535,loss:0.213446\n",
      "[epoch:15,batch:809]:acc: 0.909799,loss:0.213116\n",
      "[epoch:15,batch:839]:acc: 0.909896,loss:0.213523\n",
      "[epoch:15,batch:869]:acc: 0.909698,loss:0.214188\n",
      "[epoch:15,batch:899]:acc: 0.909410,loss:0.215015\n",
      "[epoch:15,batch:899]: val_loss:0.400838,val_acc:0.859881,val_total:4539\n",
      "[epoch:15,batch:929]:acc: 0.908905,loss:0.215499\n",
      "[epoch:15,batch:959]:acc: 0.908561,loss:0.215862\n",
      "[epoch:15,batch:989]:acc: 0.908617,loss:0.215735\n",
      "[epoch:15] :acc: 0.908535,loss:0.215780,lr:0.000001,patience:1\n",
      "[epoch:15]: val_loss:0.397103,val_acc:0.860542,\n",
      "Epoch 16/59\n",
      "----------\n",
      "loss has increased lr divide 10 lr now is :0.000000\n",
      "[epoch:16,batch:29]:acc: 0.909375,loss:0.228681\n",
      "[epoch:16,batch:59]:acc: 0.914062,loss:0.212784\n",
      "[epoch:16,batch:89]:acc: 0.913889,loss:0.220661\n",
      "[epoch:16,batch:119]:acc: 0.911979,loss:0.221348\n",
      "[epoch:16,batch:149]:acc: 0.910625,loss:0.222064\n",
      "[epoch:16,batch:179]:acc: 0.908681,loss:0.227686\n",
      "[epoch:16,batch:209]:acc: 0.907143,loss:0.229285\n",
      "[epoch:16,batch:239]:acc: 0.907682,loss:0.228087\n",
      "[epoch:16,batch:269]:acc: 0.907639,loss:0.229476\n",
      "[epoch:16,batch:299]:acc: 0.907500,loss:0.228094\n",
      "[epoch:16,batch:299]: val_loss:0.394168,val_acc:0.862525,val_total:4539\n",
      "[epoch:16,batch:329]:acc: 0.906913,loss:0.226970\n",
      "[epoch:16,batch:359]:acc: 0.907465,loss:0.225224\n",
      "[epoch:16,batch:389]:acc: 0.907051,loss:0.226276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:16,batch:419]:acc: 0.907143,loss:0.226320\n",
      "[epoch:16,batch:449]:acc: 0.906528,loss:0.226836\n",
      "[epoch:16,batch:479]:acc: 0.907487,loss:0.225074\n",
      "[epoch:16,batch:509]:acc: 0.906618,loss:0.225553\n",
      "[epoch:16,batch:539]:acc: 0.906539,loss:0.225344\n",
      "[epoch:16,batch:569]:acc: 0.906469,loss:0.224922\n",
      "[epoch:16,batch:599]:acc: 0.907344,loss:0.223520\n",
      "[epoch:16,batch:599]: val_loss:0.393805,val_acc:0.863406,val_total:4539\n",
      "[epoch:16,batch:629]:acc: 0.907490,loss:0.223531\n",
      "[epoch:16,batch:659]:acc: 0.906913,loss:0.223354\n",
      "[epoch:16,batch:689]:acc: 0.907609,loss:0.222699\n",
      "[epoch:16,batch:719]:acc: 0.907118,loss:0.222636\n",
      "[epoch:16,batch:749]:acc: 0.907375,loss:0.222531\n",
      "[epoch:16,batch:779]:acc: 0.907292,loss:0.222081\n",
      "[epoch:16,batch:809]:acc: 0.907137,loss:0.222190\n",
      "[epoch:16,batch:839]:acc: 0.907180,loss:0.221808\n",
      "[epoch:16,batch:869]:acc: 0.906358,loss:0.222707\n",
      "[epoch:16,batch:899]:acc: 0.906319,loss:0.222691\n",
      "[epoch:16,batch:899]: val_loss:0.394395,val_acc:0.862745,val_total:4539\n",
      "[epoch:16,batch:929]:acc: 0.906116,loss:0.222719\n",
      "[epoch:16,batch:959]:acc: 0.905827,loss:0.223119\n",
      "[epoch:16,batch:989]:acc: 0.905871,loss:0.223389\n",
      "[epoch:16] :acc: 0.905792,loss:0.223540,lr:0.000000,patience:0\n",
      "[epoch:16]: val_loss:0.401132,val_acc:0.861203,\n",
      "Epoch 17/59\n",
      "----------\n",
      "[epoch:17,batch:29]:acc: 0.906250,loss:0.222277\n",
      "[epoch:17,batch:59]:acc: 0.912500,loss:0.214839\n",
      "[epoch:17,batch:89]:acc: 0.912500,loss:0.218822\n",
      "[epoch:17,batch:119]:acc: 0.912240,loss:0.214933\n",
      "[epoch:17,batch:149]:acc: 0.912500,loss:0.213027\n",
      "[epoch:17,batch:179]:acc: 0.911979,loss:0.214839\n",
      "[epoch:17,batch:209]:acc: 0.909821,loss:0.218544\n",
      "[epoch:17,batch:239]:acc: 0.908203,loss:0.219199\n",
      "[epoch:17,batch:269]:acc: 0.907292,loss:0.219180\n",
      "[epoch:17,batch:299]:acc: 0.905000,loss:0.221048\n",
      "[epoch:17,batch:299]: val_loss:0.395675,val_acc:0.860101,val_total:4539\n",
      "[epoch:17,batch:329]:acc: 0.906250,loss:0.217617\n",
      "[epoch:17,batch:359]:acc: 0.905295,loss:0.219183\n",
      "[epoch:17,batch:389]:acc: 0.906090,loss:0.218853\n",
      "[epoch:17,batch:419]:acc: 0.905580,loss:0.219472\n",
      "[epoch:17,batch:449]:acc: 0.905972,loss:0.219242\n",
      "[epoch:17,batch:479]:acc: 0.907422,loss:0.217475\n",
      "[epoch:17,batch:509]:acc: 0.906985,loss:0.217484\n",
      "[epoch:17,batch:539]:acc: 0.906771,loss:0.218296\n",
      "[epoch:17,batch:569]:acc: 0.906743,loss:0.218681\n",
      "[epoch:17,batch:599]:acc: 0.906198,loss:0.219292\n",
      "[epoch:17,batch:599]: val_loss:0.393692,val_acc:0.862965,val_total:4539\n",
      "[epoch:17,batch:629]:acc: 0.906994,loss:0.217912\n",
      "[epoch:17,batch:659]:acc: 0.906581,loss:0.218424\n",
      "[epoch:17,batch:689]:acc: 0.906748,loss:0.218728\n",
      "[epoch:17,batch:719]:acc: 0.906988,loss:0.219214\n",
      "[epoch:17,batch:749]:acc: 0.907167,loss:0.218995\n",
      "[epoch:17,batch:779]:acc: 0.906971,loss:0.219475\n",
      "[epoch:17,batch:809]:acc: 0.907176,loss:0.219377\n",
      "[epoch:17,batch:839]:acc: 0.907440,loss:0.219994\n",
      "[epoch:17,batch:869]:acc: 0.907507,loss:0.220232\n",
      "[epoch:17,batch:899]:acc: 0.907569,loss:0.219774\n",
      "[epoch:17,batch:899]: val_loss:0.396683,val_acc:0.860322,val_total:4539\n",
      "[epoch:17,batch:929]:acc: 0.907191,loss:0.220604\n",
      "[epoch:17,batch:959]:acc: 0.907194,loss:0.220235\n",
      "[epoch:17,batch:989]:acc: 0.907355,loss:0.219856\n",
      "[epoch:17] :acc: 0.907368,loss:0.220156,lr:0.000000,patience:1\n",
      "[epoch:17]: val_loss:0.398457,val_acc:0.859000,\n",
      "Epoch 18/59\n",
      "----------\n",
      "loss has increased lr divide 10 lr now is :0.000000\n",
      "[epoch:18,batch:29]:acc: 0.900000,loss:0.236795\n",
      "[epoch:18,batch:59]:acc: 0.901042,loss:0.230553\n",
      "[epoch:18,batch:89]:acc: 0.903472,loss:0.227914\n",
      "[epoch:18,batch:119]:acc: 0.906250,loss:0.218741\n",
      "[epoch:18,batch:149]:acc: 0.903333,loss:0.221723\n",
      "[epoch:18,batch:179]:acc: 0.902778,loss:0.221738\n",
      "[epoch:18,batch:209]:acc: 0.903869,loss:0.221931\n",
      "[epoch:18,batch:239]:acc: 0.903646,loss:0.219704\n",
      "[epoch:18,batch:269]:acc: 0.902894,loss:0.220738\n",
      "[epoch:18,batch:299]:acc: 0.903958,loss:0.221146\n",
      "[epoch:18,batch:299]: val_loss:0.394667,val_acc:0.862304,val_total:4539\n",
      "[epoch:18,batch:329]:acc: 0.902936,loss:0.223956\n",
      "[epoch:18,batch:359]:acc: 0.903906,loss:0.223281\n",
      "[epoch:18,batch:389]:acc: 0.905369,loss:0.220919\n"
     ]
    }
   ],
   "source": [
    "#train(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reuseTrain(path,epochNum):\n",
    "    writer=SummaryWriter('log/'+date+'/DesNet201/') # 创建 /log/日期/InceptionResnet的组织形式  不同模型需要修改不同名称\n",
    "    train_dataset=MyDataSet(json_Description=ANNOTATION_TRAIN,transform=preprocess_with_augmentation(normalize_torch,IMAGE_SIZE),path_pre=IMAGE_TRAIN_PRE)\n",
    "    val_dataset=MyDataSet(json_Description=ANNOTATION_VAL,transform=preprocess(normalize_torch,IMAGE_SIZE),path_pre=IMAGE_VAL_PRE)\n",
    "    train_dataLoader=DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE,num_workers=16,shuffle=True)\n",
    "    val_dataLoader=DataLoader(dataset=val_dataset,batch_size=BATCH_SIZE,num_workers=1,shuffle=False)\n",
    "    model=getmodel()\n",
    "    weight=torch.Tensor([1,3,3,3,3,4,2,3,3,3,3,3,3,3,3,3,2,3,3,3,2,3,4,2,3,1,1,3,2,2,1,3,3,1,3,2,3,3,3,3,2,1,3,2,3,3,3,1,3,3,4,4,3,2,2,3,1,1,3]).cuda()\n",
    "    criterion=nn.CrossEntropyLoss(weight=weight).cuda()\n",
    "#     lx, px = utils.predict(model,val_dataLoader)\n",
    "#     min_loss = criterion(Variable(px), Variable(lx)).item()\n",
    "    min_loss=4.1\n",
    "    print('min_loss is :%f'%(min_loss))\n",
    "    min_acc=0.80\n",
    "    patience=0\n",
    "    lr=0.0\n",
    "    momentum=0.0\n",
    "    for epoch in range(epochNum):\n",
    "        print('Epoch {}/{}'.format(epoch, epochNum - 1))\n",
    "        print('-' * 10)\n",
    "        if epoch==3:\n",
    "            lr=1e-4\n",
    "            momentum=0.9\n",
    "            print('set lr=:%f,momentum=%f'%(lr,momentum))\n",
    "        if patience==2:\n",
    "            patience=0\n",
    "            model.load_state_dict(torch.load('../model/DesNet201/'+date+'_loss_best.pth')['state_dict'])\n",
    "            lr=lr/5\n",
    "            print('loss has increased lr divide 10 lr now is :%f'%(lr))\n",
    "        if epoch==0 or epoch==1 or epoch==2: #第一轮首先训练全连接层\n",
    "            lr=1e-3\n",
    "#             optimizer=torch.optim.SGD(params=model.fresh_params(),lr=lr,momentum=0.9)\n",
    "            optimizer = torch.optim.Adam(model.fresh_params(),lr = lr,amsgrad=True,weight_decay=1e-4)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(model.parameters(),lr = lr,amsgrad=True,weight_decay=1e-4)\n",
    "#             optimizer=torch.optim.SGD(params=model.parameters(),lr=lr,momentum=momentum)\n",
    "        running_loss = RunningMean()\n",
    "        running_corrects = RunningMean()\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_dataLoader):\n",
    "            model.train(True)\n",
    "            n_batchsize=inputs.size(0)\n",
    "            inputs = Variable(inputs).cuda()\n",
    "            labels = Variable(labels).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            if isinstance(outputs,tuple):\n",
    "                loss=sum((criterion(o,labels)) for o in outputs)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "            running_loss.update(loss.item(),1)\n",
    "            running_corrects.update(torch.sum(preds == labels.data).data,n_batchsize)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx%30==29:\n",
    "                print('[epoch:%d,batch:%d]:acc: %f,loss:%f'%(epoch,batch_idx,running_corrects.value,running_loss.value))\n",
    "                if batch_idx%300==299: \n",
    "                    niter = epoch * len(train_dataset)/BATCH_SIZE + batch_idx\n",
    "                    writer.add_scalar('Train/Acc',running_corrects.value,niter)\n",
    "                    writer.add_scalar('Train/Loss',running_loss.value,niter)\n",
    "                    lx,px=utils.predict(model,val_dataLoader)\n",
    "                    log_loss = criterion(Variable(px), Variable(lx))\n",
    "                    log_loss = log_loss.item()\n",
    "                    _, preds = torch.max(px, dim=1)\n",
    "                    accuracy = torch.mean((preds == lx).float())\n",
    "                    writer.add_scalar('Val/Acc',accuracy,niter)\n",
    "                    writer.add_scalar('Val/Loss',log_loss,niter)\n",
    "                    print('[epoch:%d,batch:%d]: val_loss:%f,val_acc:%f,val_total:%d'%(epoch,batch_idx,log_loss,accuracy,len(val_dataset)))\n",
    "        print('[epoch:%d] :acc: %f,loss:%f,lr:%f,patience:%d'%(epoch,running_corrects.value,running_loss.value,lr,patience))       \n",
    "        lx,px=utils.predict(model,val_dataLoader)\n",
    "        log_loss = criterion(Variable(px), Variable(lx))\n",
    "        log_loss = log_loss.item()\n",
    "        _, preds = torch.max(px, dim=1)\n",
    "        accuracy = torch.mean((preds == lx).float())\n",
    "        writer.add_scalar('Val/Acc',accuracy,(epoch+1) * len(train_dataset)/BATCH_SIZE)\n",
    "        writer.add_scalar('Val/Loss',log_loss,(epoch+1) * len(train_dataset)/BATCH_SIZE)\n",
    "        print('[epoch:%d]: val_loss:%f,val_acc:%f,'%(epoch,log_loss,accuracy))\n",
    "        if  log_loss < min_loss:\n",
    "            utils.snapshot('../model/', 'DesNet201', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy })          \n",
    "            patience = 0\n",
    "            min_loss=log_loss\n",
    "            print('save new model loss,now loss is ',min_loss)\n",
    "        else:\n",
    "            patience += 1\n",
    "        if accuracy>min_acc:\n",
    "            utils.snapshot('../model/', 'DesNet201', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy },key='acc') \n",
    "            min_acc=accuracy\n",
    "            print('save new model acc,now acc is ',min_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainWithRawData(path,epochNum):\n",
    "    writer=SummaryWriter('log/'+date+'/DesNet201/') # 创建 /log/日期/InceptionResnet的组织形式\n",
    "    train_dataset=MyDataSet(json_Description=ANNOTATION_TRAIN,transform=preprocess(normalize_torch,IMAGE_SIZE),path_pre=IMAGE_TRAIN_PRE)\n",
    "    val_dataset=MyDataSet(json_Description=ANNOTATION_VAL,transform=preprocess(normalize_torch,IMAGE_SIZE),path_pre=IMAGE_VAL_PRE)\n",
    "    train_dataLoader=DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE,num_workers=16,shuffle=True)\n",
    "    val_dataLoader=DataLoader(dataset=val_dataset,batch_size=BATCH_SIZE,num_workers=1,shuffle=False)\n",
    "    model=getmodel()\n",
    "    criterion=nn.CrossEntropyLoss().cuda()\n",
    "    modelParams=torch.load(path)\n",
    "    model.load_state_dict(modelParams['state_dict'])\n",
    "    min_loss=modelParams['val_loss']\n",
    "    print('min_loss is :%f'%(min_loss))\n",
    "    print('val_correct is %f'%(modelParams['val_correct']))\n",
    "    min_acc=max(modelParams['val_correct'],0.81)\n",
    "    optinizerSave=modelParams['optimizer']\n",
    "    patience=0\n",
    "    lr=1e-4\n",
    "    momentum=0.9\n",
    "    beginepoch=modelParams['epoch']\n",
    "    for epoch in range(beginepoch,epochNum):\n",
    "        print('Epoch {}/{}'.format(epoch, epochNum - 1))\n",
    "        print('-' * 10)\n",
    "        if patience==3:\n",
    "            patience=0\n",
    "            model.load_state_dict(torch.load('../model/DesNet201/'+date+'_loss_best.pth')['state_dict'])\n",
    "            lr=lr/5\n",
    "            print('lr desencd')\n",
    "        if epoch==beginepoch:\n",
    "            optimizer=torch.optim.SGD(params=model.parameters(),lr=lr,momentum=momentum)\n",
    "#             optimizer.load_state_dict(optinizerSave)\n",
    "#             lr=optimizer['lr']\n",
    "#             momentum=optimizer['momentum']\n",
    "            print('begin lr is ',lr)\n",
    "            \n",
    "        else:\n",
    "            optimizer=torch.optim.SGD(params=model.parameters(),lr=lr,momentum=momentum)\n",
    "                   \n",
    "        running_loss = RunningMean()\n",
    "        running_corrects = RunningMean()\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_dataLoader):\n",
    "            model.train(True)\n",
    "            n_batchsize=inputs.size(0)\n",
    "            inputs = Variable(inputs).cuda()\n",
    "            labels = Variable(labels).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            if isinstance(outputs,tuple):\n",
    "                loss=sum((criterion(o,labels)) for o in outputs)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "            running_loss.update(loss.item(),1)\n",
    "            running_corrects.update(torch.sum(preds == labels.data).data,n_batchsize)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx%30==29:\n",
    "                print('[epoch:%d,batch:%d]:acc: %f,loss:%f'%(epoch,batch_idx,running_corrects.value,running_loss.value))\n",
    "                if batch_idx%300==299: \n",
    "                    niter = epoch * len(train_dataset)/BATCH_SIZE + batch_idx\n",
    "                    writer.add_scalar('Train/Acc',running_corrects.value,niter)\n",
    "                    writer.add_scalar('Train/Loss',running_loss.value,niter)\n",
    "                    lx,px=utils.predict(model,val_dataLoader)\n",
    "                    log_loss = criterion(Variable(px), Variable(lx))\n",
    "                    log_loss = log_loss.item()\n",
    "                    _, preds = torch.max(px, dim=1)\n",
    "                    accuracy = torch.mean((preds == lx).float())\n",
    "                    writer.add_scalar('Val/Acc',accuracy,niter)\n",
    "                    writer.add_scalar('Val/Loss',log_loss,niter)\n",
    "                    print('[epoch:%d,batch:%d]: val_loss:%f,val_acc:%f,val_total:%d'%(epoch,batch_idx,log_loss,accuracy,len(val_dataset)))\n",
    "                    if  log_loss < min_loss:\n",
    "                        utils.snapshot('../model/', 'DesNet201', {\n",
    "                               'epoch': epoch + 1,\n",
    "                               'state_dict': model.state_dict(),\n",
    "                               'optimizer': optimizer.state_dict(),\n",
    "                               'val_loss': log_loss,\n",
    "                               'val_correct':accuracy })          \n",
    "\n",
    "                        min_loss=log_loss\n",
    "                        print('save new model loss,now loss is ',min_loss)\n",
    "\n",
    "                    if accuracy>min_acc:\n",
    "                        utils.snapshot('../model/', 'DesNet201', {\n",
    "                               'epoch': epoch + 1,\n",
    "                               'state_dict': model.state_dict(),\n",
    "                               'optimizer': optimizer.state_dict(),\n",
    "                               'val_loss': log_loss,\n",
    "                               'val_correct':accuracy },key='acc') \n",
    "                        min_acc=accuracy\n",
    "                        print('save new model acc,now acc is ',min_acc)\n",
    "        print('[epoch:%d] :acc: %f,loss:%f,lr:%f,patience:%d'%(epoch,running_corrects.value,running_loss.value,lr,patience))         \n",
    "        lx,px=utils.predict(model,val_dataLoader)\n",
    "        log_loss = criterion(Variable(px), Variable(lx))\n",
    "        log_loss = log_loss.item()\n",
    "        _, preds = torch.max(px, dim=1)\n",
    "        accuracy = torch.mean((preds == lx).float())\n",
    "        writer.add_scalar('Val/Acc',accuracy,(epoch+1) * len(train_dataset)/BATCH_SIZE)\n",
    "        writer.add_scalar('Val/Loss',log_loss,(epoch+1) * len(train_dataset)/BATCH_SIZE)\n",
    "        print('[epoch:%d]: val_loss:%f,val_acc:%f,'%(epoch,log_loss,accuracy))\n",
    "        if  log_loss < min_loss:\n",
    "            utils.snapshot('../model/', 'DesNet201', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy })          \n",
    "            patience = 0\n",
    "            min_loss=log_loss\n",
    "            print('save new model loss,now loss is ',min_loss)\n",
    "        else:\n",
    "            patience += 1\n",
    "        if accuracy>min_acc:\n",
    "            utils.snapshot('../model/', 'DesNet201', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy },key='acc') \n",
    "            min_acc=accuracy\n",
    "            print('save new model acc,now acc is ',min_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] loading model... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lujunfeng/anaconda3/envs/Conda_Env_Pytorch/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "min_loss is :0.397214\n",
      "val_correct is 0.861864\n",
      "Epoch 11/59\n",
      "----------\n",
      "begin lr is  0.0001\n",
      "[epoch:11,batch:29]:acc: 0.890625,loss:0.245192\n",
      "[epoch:11,batch:59]:acc: 0.899479,loss:0.227566\n",
      "[epoch:11,batch:89]:acc: 0.904861,loss:0.223650\n",
      "[epoch:11,batch:119]:acc: 0.906510,loss:0.223769\n",
      "[epoch:11,batch:149]:acc: 0.907083,loss:0.223205\n",
      "[epoch:11,batch:179]:acc: 0.906597,loss:0.224726\n",
      "[epoch:11,batch:209]:acc: 0.906101,loss:0.224138\n",
      "[epoch:11,batch:239]:acc: 0.906901,loss:0.220967\n",
      "[epoch:11,batch:269]:acc: 0.907986,loss:0.219209\n",
      "[epoch:11,batch:299]:acc: 0.908542,loss:0.217059\n",
      "[epoch:11,batch:299]: val_loss:0.354680,val_acc:0.866490,val_total:4539\n",
      "save new model loss,now loss is  0.35468021035194397\n",
      "save new model acc,now acc is  tensor(0.8665, device='cuda:0')\n",
      "[epoch:11,batch:329]:acc: 0.909186,loss:0.216487\n",
      "[epoch:11,batch:359]:acc: 0.909549,loss:0.215935\n",
      "[epoch:11,batch:389]:acc: 0.909856,loss:0.214716\n",
      "[epoch:11,batch:419]:acc: 0.910565,loss:0.213926\n",
      "[epoch:11,batch:449]:acc: 0.911944,loss:0.212439\n",
      "[epoch:11,batch:479]:acc: 0.911784,loss:0.212859\n",
      "[epoch:11,batch:509]:acc: 0.912929,loss:0.212083\n",
      "[epoch:11,batch:539]:acc: 0.912789,loss:0.212015\n",
      "[epoch:11,batch:569]:acc: 0.912939,loss:0.212157\n",
      "[epoch:11,batch:599]:acc: 0.912969,loss:0.211957\n",
      "[epoch:11,batch:599]: val_loss:0.352699,val_acc:0.868914,val_total:4539\n",
      "save new model loss,now loss is  0.3526992201805115\n",
      "save new model acc,now acc is  tensor(0.8689, device='cuda:0')\n",
      "[epoch:11,batch:629]:acc: 0.912996,loss:0.211939\n",
      "[epoch:11,batch:659]:acc: 0.913873,loss:0.211322\n",
      "[epoch:11,batch:689]:acc: 0.914130,loss:0.210488\n",
      "[epoch:11,batch:719]:acc: 0.913281,loss:0.211919\n",
      "[epoch:11,batch:749]:acc: 0.913500,loss:0.210989\n",
      "[epoch:11,batch:779]:acc: 0.913902,loss:0.210976\n",
      "[epoch:11,batch:809]:acc: 0.914043,loss:0.210624\n",
      "[epoch:11,batch:839]:acc: 0.914174,loss:0.210858\n",
      "[epoch:11,batch:869]:acc: 0.914224,loss:0.210857\n",
      "[epoch:11,batch:899]:acc: 0.913958,loss:0.211457\n",
      "[epoch:11,batch:899]: val_loss:0.352159,val_acc:0.867812,val_total:4539\n",
      "save new model loss,now loss is  0.35215893387794495\n",
      "[epoch:11,batch:929]:acc: 0.914247,loss:0.211232\n",
      "[epoch:11,batch:959]:acc: 0.914453,loss:0.210904\n",
      "[epoch:11,batch:989]:acc: 0.914236,loss:0.211117\n",
      "[epoch:11] :acc: 0.914210,loss:0.211171,lr:0.000100,patience:0\n",
      "[epoch:11]: val_loss:0.351260,val_acc:0.868694,\n",
      "save new model loss,now loss is  0.35125967860221863\n",
      "Epoch 12/59\n",
      "----------\n",
      "[epoch:12,batch:29]:acc: 0.923958,loss:0.177096\n",
      "[epoch:12,batch:59]:acc: 0.924479,loss:0.179885\n",
      "[epoch:12,batch:89]:acc: 0.922222,loss:0.185257\n",
      "[epoch:12,batch:119]:acc: 0.923698,loss:0.184944\n",
      "[epoch:12,batch:149]:acc: 0.925000,loss:0.185392\n",
      "[epoch:12,batch:179]:acc: 0.926389,loss:0.185321\n",
      "[epoch:12,batch:209]:acc: 0.928571,loss:0.182912\n",
      "[epoch:12,batch:239]:acc: 0.927214,loss:0.183479\n",
      "[epoch:12,batch:269]:acc: 0.926620,loss:0.182996\n",
      "[epoch:12,batch:299]:acc: 0.925729,loss:0.183481\n",
      "[epoch:12,batch:299]: val_loss:0.352406,val_acc:0.868253,val_total:4539\n",
      "[epoch:12,batch:329]:acc: 0.925189,loss:0.184669\n",
      "[epoch:12,batch:359]:acc: 0.925174,loss:0.184094\n",
      "[epoch:12,batch:389]:acc: 0.925561,loss:0.183815\n",
      "[epoch:12,batch:419]:acc: 0.924926,loss:0.184402\n",
      "[epoch:12,batch:449]:acc: 0.924722,loss:0.184976\n",
      "[epoch:12,batch:479]:acc: 0.924674,loss:0.185096\n",
      "[epoch:12,batch:509]:acc: 0.924877,loss:0.184644\n",
      "[epoch:12,batch:539]:acc: 0.924884,loss:0.184479\n",
      "[epoch:12,batch:569]:acc: 0.924836,loss:0.184536\n",
      "[epoch:12,batch:599]:acc: 0.924219,loss:0.185420\n",
      "[epoch:12,batch:599]: val_loss:0.351361,val_acc:0.870676,val_total:4539\n",
      "save new model acc,now acc is  tensor(0.8707, device='cuda:0')\n",
      "[epoch:12,batch:629]:acc: 0.924058,loss:0.185902\n",
      "[epoch:12,batch:659]:acc: 0.924242,loss:0.185404\n",
      "[epoch:12,batch:689]:acc: 0.924049,loss:0.186190\n",
      "[epoch:12,batch:719]:acc: 0.923828,loss:0.186619\n",
      "[epoch:12,batch:749]:acc: 0.924083,loss:0.186091\n",
      "[epoch:12,batch:779]:acc: 0.924359,loss:0.185526\n",
      "[epoch:12,batch:809]:acc: 0.924807,loss:0.185471\n",
      "[epoch:12,batch:839]:acc: 0.924554,loss:0.186346\n",
      "[epoch:12,batch:869]:acc: 0.924964,loss:0.185860\n",
      "[epoch:12,batch:899]:acc: 0.925035,loss:0.185901\n",
      "[epoch:12,batch:899]: val_loss:0.348847,val_acc:0.870015,val_total:4539\n",
      "save new model loss,now loss is  0.348846971988678\n",
      "[epoch:12,batch:929]:acc: 0.924933,loss:0.185926\n",
      "[epoch:12,batch:959]:acc: 0.924837,loss:0.186004\n",
      "[epoch:12,batch:989]:acc: 0.924306,loss:0.186556\n",
      "[epoch:12] :acc: 0.924268,loss:0.186969,lr:0.000100,patience:0\n",
      "[epoch:12]: val_loss:0.352776,val_acc:0.867812,\n",
      "Epoch 13/59\n",
      "----------\n",
      "[epoch:13,batch:29]:acc: 0.931250,loss:0.172473\n",
      "[epoch:13,batch:59]:acc: 0.945312,loss:0.153496\n",
      "[epoch:13,batch:89]:acc: 0.940625,loss:0.164741\n",
      "[epoch:13,batch:119]:acc: 0.937240,loss:0.168203\n",
      "[epoch:13,batch:149]:acc: 0.936667,loss:0.167421\n",
      "[epoch:13,batch:179]:acc: 0.937500,loss:0.168006\n",
      "[epoch:13,batch:209]:acc: 0.936756,loss:0.167146\n",
      "[epoch:13,batch:239]:acc: 0.936458,loss:0.166936\n",
      "[epoch:13,batch:269]:acc: 0.935185,loss:0.168048\n",
      "[epoch:13,batch:299]:acc: 0.935729,loss:0.168329\n",
      "[epoch:13,batch:299]: val_loss:0.353476,val_acc:0.866931,val_total:4539\n",
      "[epoch:13,batch:329]:acc: 0.935133,loss:0.168856\n",
      "[epoch:13,batch:359]:acc: 0.934375,loss:0.169817\n",
      "[epoch:13,batch:389]:acc: 0.935417,loss:0.168925\n",
      "[epoch:13,batch:419]:acc: 0.935342,loss:0.168880\n",
      "[epoch:13,batch:449]:acc: 0.934236,loss:0.170218\n",
      "[epoch:13,batch:479]:acc: 0.933659,loss:0.170612\n",
      "[epoch:13,batch:509]:acc: 0.933946,loss:0.169394\n",
      "[epoch:13,batch:539]:acc: 0.933738,loss:0.169934\n",
      "[epoch:13,batch:569]:acc: 0.934594,loss:0.168976\n",
      "[epoch:13,batch:599]:acc: 0.934115,loss:0.170189\n",
      "[epoch:13,batch:599]: val_loss:0.352645,val_acc:0.869795,val_total:4539\n",
      "[epoch:13,batch:629]:acc: 0.933829,loss:0.170514\n",
      "[epoch:13,batch:659]:acc: 0.934422,loss:0.170463\n",
      "[epoch:13,batch:689]:acc: 0.934375,loss:0.170486\n",
      "[epoch:13,batch:719]:acc: 0.934158,loss:0.170281\n",
      "[epoch:13,batch:749]:acc: 0.934292,loss:0.170196\n",
      "[epoch:13,batch:779]:acc: 0.934095,loss:0.170123\n",
      "[epoch:13,batch:809]:acc: 0.934221,loss:0.169916\n",
      "[epoch:13,batch:839]:acc: 0.934077,loss:0.170153\n",
      "[epoch:13,batch:869]:acc: 0.933585,loss:0.170741\n",
      "[epoch:13,batch:899]:acc: 0.933472,loss:0.170711\n",
      "[epoch:13,batch:899]: val_loss:0.350882,val_acc:0.870236,val_total:4539\n",
      "[epoch:13,batch:929]:acc: 0.933737,loss:0.170328\n",
      "[epoch:13,batch:959]:acc: 0.933789,loss:0.170001\n",
      "[epoch:13,batch:989]:acc: 0.933996,loss:0.170216\n",
      "[epoch:13] :acc: 0.934010,loss:0.170243,lr:0.000100,patience:1\n",
      "[epoch:13]: val_loss:0.349974,val_acc:0.871117,\n",
      "save new model acc,now acc is  tensor(0.8711, device='cuda:0')\n",
      "Epoch 14/59\n",
      "----------\n",
      "[epoch:14,batch:29]:acc: 0.947917,loss:0.145180\n",
      "[epoch:14,batch:59]:acc: 0.939583,loss:0.158001\n",
      "[epoch:14,batch:89]:acc: 0.941319,loss:0.155587\n",
      "[epoch:14,batch:119]:acc: 0.941406,loss:0.155737\n",
      "[epoch:14,batch:149]:acc: 0.941250,loss:0.155895\n",
      "[epoch:14,batch:179]:acc: 0.941840,loss:0.155058\n",
      "[epoch:14,batch:209]:acc: 0.940923,loss:0.157614\n",
      "[epoch:14,batch:239]:acc: 0.941016,loss:0.157514\n",
      "[epoch:14,batch:269]:acc: 0.940509,loss:0.159143\n",
      "[epoch:14,batch:299]:acc: 0.940833,loss:0.159134\n",
      "[epoch:14,batch:299]: val_loss:0.353905,val_acc:0.869134,val_total:4539\n",
      "[epoch:14,batch:329]:acc: 0.941193,loss:0.159324\n",
      "[epoch:14,batch:359]:acc: 0.942448,loss:0.157722\n",
      "[epoch:14,batch:389]:acc: 0.943029,loss:0.156753\n",
      "[epoch:14,batch:419]:acc: 0.943750,loss:0.155662\n",
      "[epoch:14,batch:449]:acc: 0.943611,loss:0.155445\n",
      "[epoch:14,batch:479]:acc: 0.943164,loss:0.155857\n",
      "[epoch:14,batch:509]:acc: 0.942953,loss:0.155549\n",
      "[epoch:14,batch:539]:acc: 0.943056,loss:0.155652\n",
      "[epoch:14,batch:569]:acc: 0.943805,loss:0.154972\n",
      "[epoch:14,batch:599]:acc: 0.943438,loss:0.154882\n",
      "[epoch:14,batch:599]: val_loss:0.352479,val_acc:0.871337,val_total:4539\n",
      "save new model acc,now acc is  tensor(0.8713, device='cuda:0')\n",
      "[epoch:14,batch:629]:acc: 0.943948,loss:0.154197\n",
      "[epoch:14,batch:659]:acc: 0.944176,loss:0.154388\n",
      "[epoch:14,batch:689]:acc: 0.944520,loss:0.153875\n",
      "[epoch:14,batch:719]:acc: 0.944531,loss:0.153793\n",
      "[epoch:14,batch:749]:acc: 0.944208,loss:0.154428\n",
      "[epoch:14,batch:779]:acc: 0.944151,loss:0.154336\n",
      "[epoch:14,batch:809]:acc: 0.944329,loss:0.153879\n",
      "[epoch:14,batch:839]:acc: 0.944457,loss:0.153302\n",
      "[epoch:14,batch:869]:acc: 0.944289,loss:0.153536\n",
      "[epoch:14,batch:899]:acc: 0.944097,loss:0.153331\n",
      "[epoch:14,batch:899]: val_loss:0.352371,val_acc:0.869134,val_total:4539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:14,batch:929]:acc: 0.943851,loss:0.153369\n",
      "[epoch:14,batch:959]:acc: 0.943848,loss:0.153175\n",
      "[epoch:14,batch:989]:acc: 0.944034,loss:0.152785\n",
      "[epoch:14] :acc: 0.944005,loss:0.152864,lr:0.000100,patience:2\n",
      "[epoch:14]: val_loss:0.356526,val_acc:0.869134,\n",
      "Epoch 15/59\n",
      "----------\n",
      "lr desencd\n",
      "[epoch:15,batch:29]:acc: 0.940625,loss:0.158389\n",
      "[epoch:15,batch:59]:acc: 0.935937,loss:0.163992\n",
      "[epoch:15,batch:89]:acc: 0.937153,loss:0.161790\n",
      "[epoch:15,batch:119]:acc: 0.939844,loss:0.160240\n",
      "[epoch:15,batch:149]:acc: 0.942500,loss:0.158944\n",
      "[epoch:15,batch:179]:acc: 0.944965,loss:0.157009\n",
      "[epoch:15,batch:209]:acc: 0.944196,loss:0.158443\n",
      "[epoch:15,batch:239]:acc: 0.944531,loss:0.158671\n",
      "[epoch:15,batch:269]:acc: 0.944907,loss:0.158040\n",
      "[epoch:15,batch:299]:acc: 0.943750,loss:0.159701\n",
      "[epoch:15,batch:299]: val_loss:0.349220,val_acc:0.869795,val_total:4539\n",
      "[epoch:15,batch:329]:acc: 0.943466,loss:0.160415\n",
      "[epoch:15,batch:359]:acc: 0.943924,loss:0.158921\n",
      "[epoch:15,batch:389]:acc: 0.942548,loss:0.160225\n",
      "[epoch:15,batch:419]:acc: 0.942634,loss:0.160083\n",
      "[epoch:15,batch:449]:acc: 0.942361,loss:0.160774\n",
      "[epoch:15,batch:479]:acc: 0.941927,loss:0.160836\n",
      "[epoch:15,batch:509]:acc: 0.941544,loss:0.160742\n",
      "[epoch:15,batch:539]:acc: 0.942072,loss:0.160178\n",
      "[epoch:15,batch:569]:acc: 0.941502,loss:0.160326\n",
      "[epoch:15,batch:599]:acc: 0.942292,loss:0.159464\n",
      "[epoch:15,batch:599]: val_loss:0.350373,val_acc:0.869795,val_total:4539\n",
      "[epoch:15,batch:629]:acc: 0.942956,loss:0.158751\n",
      "[epoch:15,batch:659]:acc: 0.942424,loss:0.159182\n",
      "[epoch:15,batch:689]:acc: 0.941848,loss:0.160184\n",
      "[epoch:15,batch:719]:acc: 0.941146,loss:0.160826\n",
      "[epoch:15,batch:749]:acc: 0.941042,loss:0.161022\n",
      "[epoch:15,batch:779]:acc: 0.941587,loss:0.160223\n",
      "[epoch:15,batch:809]:acc: 0.941281,loss:0.161121\n",
      "[epoch:15,batch:839]:acc: 0.941369,loss:0.161592\n",
      "[epoch:15,batch:869]:acc: 0.941128,loss:0.161765\n",
      "[epoch:15,batch:899]:acc: 0.940903,loss:0.161818\n",
      "[epoch:15,batch:899]: val_loss:0.349989,val_acc:0.868473,val_total:4539\n",
      "[epoch:15,batch:929]:acc: 0.940793,loss:0.162144\n",
      "[epoch:15,batch:959]:acc: 0.940430,loss:0.162404\n",
      "[epoch:15,batch:989]:acc: 0.940436,loss:0.162386\n",
      "[epoch:15] :acc: 0.940347,loss:0.162546,lr:0.000020,patience:0\n",
      "[epoch:15]: val_loss:0.353395,val_acc:0.869354,\n",
      "Epoch 16/59\n",
      "----------\n",
      "[epoch:16,batch:29]:acc: 0.932292,loss:0.176845\n",
      "[epoch:16,batch:59]:acc: 0.937500,loss:0.178495\n",
      "[epoch:16,batch:89]:acc: 0.932292,loss:0.182703\n",
      "[epoch:16,batch:119]:acc: 0.934896,loss:0.175184\n",
      "[epoch:16,batch:149]:acc: 0.934792,loss:0.174194\n",
      "[epoch:16,batch:179]:acc: 0.935590,loss:0.173254\n",
      "[epoch:16,batch:209]:acc: 0.935863,loss:0.175946\n",
      "[epoch:16,batch:239]:acc: 0.935547,loss:0.175871\n",
      "[epoch:16,batch:269]:acc: 0.935532,loss:0.174662\n",
      "[epoch:16,batch:299]:acc: 0.936562,loss:0.173505\n",
      "[epoch:16,batch:299]: val_loss:0.350834,val_acc:0.867372,val_total:4539\n",
      "[epoch:16,batch:329]:acc: 0.935417,loss:0.174233\n",
      "[epoch:16,batch:359]:acc: 0.935417,loss:0.172841\n",
      "[epoch:16,batch:389]:acc: 0.936218,loss:0.172027\n",
      "[epoch:16,batch:419]:acc: 0.936235,loss:0.171129\n",
      "[epoch:16,batch:449]:acc: 0.937083,loss:0.170151\n",
      "[epoch:16,batch:479]:acc: 0.937044,loss:0.170478\n",
      "[epoch:16,batch:509]:acc: 0.937806,loss:0.169232\n",
      "[epoch:16,batch:539]:acc: 0.938137,loss:0.168172\n",
      "[epoch:16,batch:569]:acc: 0.937774,loss:0.168721\n",
      "[epoch:16,batch:599]:acc: 0.937760,loss:0.168297\n",
      "[epoch:16,batch:599]: val_loss:0.349105,val_acc:0.870676,val_total:4539\n",
      "[epoch:16,batch:629]:acc: 0.937351,loss:0.168663\n",
      "[epoch:16,batch:659]:acc: 0.936932,loss:0.168527\n",
      "[epoch:16,batch:689]:acc: 0.937047,loss:0.168130\n",
      "[epoch:16,batch:719]:acc: 0.937413,loss:0.167771\n",
      "[epoch:16,batch:749]:acc: 0.937333,loss:0.167819\n",
      "[epoch:16,batch:779]:acc: 0.937059,loss:0.168330\n",
      "[epoch:16,batch:809]:acc: 0.936728,loss:0.168611\n",
      "[epoch:16,batch:839]:acc: 0.936756,loss:0.168563\n",
      "[epoch:16,batch:869]:acc: 0.936889,loss:0.168302\n",
      "[epoch:16,batch:899]:acc: 0.937153,loss:0.167444\n",
      "[epoch:16,batch:899]: val_loss:0.348354,val_acc:0.869354,val_total:4539\n",
      "save new model loss,now loss is  0.3483544588088989\n",
      "[epoch:16,batch:929]:acc: 0.937130,loss:0.167391\n",
      "[epoch:16,batch:959]:acc: 0.936816,loss:0.168017\n",
      "[epoch:16,batch:989]:acc: 0.936963,loss:0.167589\n",
      "[epoch:16] :acc: 0.936879,loss:0.167636,lr:0.000020,patience:1\n",
      "[epoch:16]: val_loss:0.350113,val_acc:0.868473,\n",
      "Epoch 17/59\n",
      "----------\n",
      "[epoch:17,batch:29]:acc: 0.935417,loss:0.165522\n",
      "[epoch:17,batch:59]:acc: 0.939063,loss:0.159727\n",
      "[epoch:17,batch:89]:acc: 0.941319,loss:0.158272\n",
      "[epoch:17,batch:119]:acc: 0.943229,loss:0.154833\n",
      "[epoch:17,batch:149]:acc: 0.939792,loss:0.158579\n",
      "[epoch:17,batch:179]:acc: 0.940278,loss:0.158198\n",
      "[epoch:17,batch:209]:acc: 0.940327,loss:0.158745\n",
      "[epoch:17,batch:239]:acc: 0.940365,loss:0.158523\n",
      "[epoch:17,batch:269]:acc: 0.937847,loss:0.161409\n",
      "[epoch:17,batch:299]:acc: 0.937292,loss:0.162494\n",
      "[epoch:17,batch:299]: val_loss:0.351335,val_acc:0.870897,val_total:4539\n",
      "[epoch:17,batch:329]:acc: 0.936837,loss:0.162887\n",
      "[epoch:17,batch:359]:acc: 0.937066,loss:0.162582\n",
      "[epoch:17,batch:389]:acc: 0.936859,loss:0.163549\n",
      "[epoch:17,batch:419]:acc: 0.936905,loss:0.163814\n",
      "[epoch:17,batch:449]:acc: 0.936944,loss:0.163246\n",
      "[epoch:17,batch:479]:acc: 0.937500,loss:0.162896\n",
      "[epoch:17,batch:509]:acc: 0.937132,loss:0.163010\n",
      "[epoch:17,batch:539]:acc: 0.937095,loss:0.163800\n",
      "[epoch:17,batch:569]:acc: 0.937336,loss:0.163295\n",
      "[epoch:17,batch:599]:acc: 0.937500,loss:0.163093\n",
      "[epoch:17,batch:599]: val_loss:0.351039,val_acc:0.867592,val_total:4539\n",
      "[epoch:17,batch:629]:acc: 0.937252,loss:0.163570\n",
      "[epoch:17,batch:659]:acc: 0.937121,loss:0.164239\n",
      "[epoch:17,batch:689]:acc: 0.937591,loss:0.163405\n",
      "[epoch:17,batch:719]:acc: 0.937587,loss:0.163234\n",
      "[epoch:17,batch:749]:acc: 0.937875,loss:0.162823\n",
      "[epoch:17,batch:779]:acc: 0.937901,loss:0.162740\n",
      "[epoch:17,batch:809]:acc: 0.937963,loss:0.163093\n",
      "[epoch:17,batch:839]:acc: 0.938021,loss:0.163075\n",
      "[epoch:17,batch:869]:acc: 0.937249,loss:0.164265\n",
      "[epoch:17,batch:899]:acc: 0.937153,loss:0.164333\n",
      "[epoch:17,batch:899]: val_loss:0.349477,val_acc:0.870676,val_total:4539\n",
      "[epoch:17,batch:929]:acc: 0.936828,loss:0.164587\n",
      "[epoch:17,batch:959]:acc: 0.936914,loss:0.164649\n",
      "[epoch:17,batch:989]:acc: 0.936869,loss:0.164663\n",
      "[epoch:17] :acc: 0.936816,loss:0.164859,lr:0.000020,patience:2\n",
      "[epoch:17]: val_loss:0.350163,val_acc:0.869134,\n",
      "Epoch 18/59\n",
      "----------\n",
      "lr desencd\n",
      "[epoch:18,batch:29]:acc: 0.947917,loss:0.151298\n",
      "[epoch:18,batch:59]:acc: 0.953646,loss:0.144081\n",
      "[epoch:18,batch:89]:acc: 0.948958,loss:0.150020\n",
      "[epoch:18,batch:119]:acc: 0.948177,loss:0.150301\n",
      "[epoch:18,batch:149]:acc: 0.948125,loss:0.149909\n",
      "[epoch:18,batch:179]:acc: 0.947569,loss:0.150135\n",
      "[epoch:18,batch:209]:acc: 0.947768,loss:0.150267\n",
      "[epoch:18,batch:239]:acc: 0.946615,loss:0.151768\n",
      "[epoch:18,batch:269]:acc: 0.947222,loss:0.150583\n",
      "[epoch:18,batch:299]:acc: 0.946875,loss:0.151248\n",
      "[epoch:18,batch:299]: val_loss:0.347857,val_acc:0.869795,val_total:4539\n",
      "save new model loss,now loss is  0.3478569984436035\n",
      "[epoch:18,batch:329]:acc: 0.946686,loss:0.150859\n",
      "[epoch:18,batch:359]:acc: 0.946528,loss:0.152277\n",
      "[epoch:18,batch:389]:acc: 0.944952,loss:0.153716\n",
      "[epoch:18,batch:419]:acc: 0.944792,loss:0.154390\n",
      "[epoch:18,batch:449]:acc: 0.945278,loss:0.153836\n",
      "[epoch:18,batch:479]:acc: 0.944141,loss:0.155717\n",
      "[epoch:18,batch:509]:acc: 0.943750,loss:0.156357\n",
      "[epoch:18,batch:539]:acc: 0.943924,loss:0.156520\n",
      "[epoch:18,batch:569]:acc: 0.944024,loss:0.155901\n",
      "[epoch:18,batch:599]:acc: 0.943698,loss:0.156524\n",
      "[epoch:18,batch:599]: val_loss:0.348541,val_acc:0.870015,val_total:4539\n",
      "[epoch:18,batch:629]:acc: 0.943254,loss:0.156626\n",
      "[epoch:18,batch:659]:acc: 0.943134,loss:0.156890\n",
      "[epoch:18,batch:689]:acc: 0.943342,loss:0.156965\n",
      "[epoch:18,batch:719]:acc: 0.943056,loss:0.157236\n",
      "[epoch:18,batch:749]:acc: 0.943167,loss:0.156868\n",
      "[epoch:18,batch:779]:acc: 0.943389,loss:0.156195\n",
      "[epoch:18,batch:809]:acc: 0.943133,loss:0.155891\n",
      "[epoch:18,batch:839]:acc: 0.943415,loss:0.155932\n",
      "[epoch:18,batch:869]:acc: 0.943319,loss:0.155928\n",
      "[epoch:18,batch:899]:acc: 0.943681,loss:0.155775\n",
      "[epoch:18,batch:899]: val_loss:0.347713,val_acc:0.869354,val_total:4539\n",
      "save new model loss,now loss is  0.3477131426334381\n",
      "[epoch:18,batch:929]:acc: 0.943683,loss:0.155979\n",
      "[epoch:18,batch:959]:acc: 0.943392,loss:0.156545\n",
      "[epoch:18,batch:989]:acc: 0.943087,loss:0.156817\n",
      "[epoch:18] :acc: 0.943059,loss:0.157639,lr:0.000004,patience:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:18]: val_loss:0.350135,val_acc:0.866711,\n",
      "Epoch 19/59\n",
      "----------\n",
      "[epoch:19,batch:29]:acc: 0.946875,loss:0.160242\n",
      "[epoch:19,batch:59]:acc: 0.945833,loss:0.154699\n",
      "[epoch:19,batch:89]:acc: 0.945139,loss:0.158312\n",
      "[epoch:19,batch:119]:acc: 0.942708,loss:0.160962\n",
      "[epoch:19,batch:149]:acc: 0.941250,loss:0.161392\n",
      "[epoch:19,batch:179]:acc: 0.940799,loss:0.161638\n",
      "[epoch:19,batch:209]:acc: 0.941518,loss:0.160719\n",
      "[epoch:19,batch:239]:acc: 0.940625,loss:0.161731\n",
      "[epoch:19,batch:269]:acc: 0.939699,loss:0.162266\n",
      "[epoch:19,batch:299]:acc: 0.940000,loss:0.161013\n",
      "[epoch:19,batch:299]: val_loss:0.347885,val_acc:0.870236,val_total:4539\n",
      "[epoch:19,batch:329]:acc: 0.940436,loss:0.160528\n",
      "[epoch:19,batch:359]:acc: 0.940885,loss:0.159478\n",
      "[epoch:19,batch:389]:acc: 0.941587,loss:0.158397\n",
      "[epoch:19,batch:419]:acc: 0.940253,loss:0.160680\n",
      "[epoch:19,batch:449]:acc: 0.940139,loss:0.160988\n",
      "[epoch:19,batch:479]:acc: 0.939844,loss:0.162000\n",
      "[epoch:19,batch:509]:acc: 0.940196,loss:0.161810\n",
      "[epoch:19,batch:539]:acc: 0.939352,loss:0.162810\n",
      "[epoch:19,batch:569]:acc: 0.940186,loss:0.161896\n",
      "[epoch:19,batch:599]:acc: 0.940052,loss:0.162621\n",
      "[epoch:19,batch:599]: val_loss:0.349666,val_acc:0.871778,val_total:4539\n",
      "save new model acc,now acc is  tensor(0.8718, device='cuda:0')\n",
      "[epoch:19,batch:629]:acc: 0.939534,loss:0.162847\n",
      "[epoch:19,batch:659]:acc: 0.939773,loss:0.162291\n",
      "[epoch:19,batch:689]:acc: 0.939493,loss:0.162732\n",
      "[epoch:19,batch:719]:acc: 0.940061,loss:0.162030\n",
      "[epoch:19,batch:749]:acc: 0.939708,loss:0.162781\n",
      "[epoch:19,batch:779]:acc: 0.939663,loss:0.162955\n",
      "[epoch:19,batch:809]:acc: 0.939622,loss:0.162588\n",
      "[epoch:19,batch:839]:acc: 0.939137,loss:0.162896\n",
      "[epoch:19,batch:869]:acc: 0.939260,loss:0.162963\n",
      "[epoch:19,batch:899]:acc: 0.938924,loss:0.163535\n",
      "[epoch:19,batch:899]: val_loss:0.348617,val_acc:0.870676,val_total:4539\n",
      "[epoch:19,batch:929]:acc: 0.938676,loss:0.163727\n",
      "[epoch:19,batch:959]:acc: 0.938086,loss:0.164389\n",
      "[epoch:19,batch:989]:acc: 0.937879,loss:0.165045\n",
      "[epoch:19] :acc: 0.937857,loss:0.165264,lr:0.000004,patience:1\n",
      "[epoch:19]: val_loss:0.349263,val_acc:0.871778,\n",
      "Epoch 20/59\n",
      "----------\n",
      "[epoch:20,batch:29]:acc: 0.931250,loss:0.171634\n",
      "[epoch:20,batch:59]:acc: 0.933854,loss:0.168386\n",
      "[epoch:20,batch:89]:acc: 0.934722,loss:0.169710\n",
      "[epoch:20,batch:119]:acc: 0.939323,loss:0.161927\n",
      "[epoch:20,batch:149]:acc: 0.940000,loss:0.163963\n",
      "[epoch:20,batch:179]:acc: 0.939757,loss:0.165429\n",
      "[epoch:20,batch:209]:acc: 0.938988,loss:0.164076\n",
      "[epoch:20,batch:239]:acc: 0.941927,loss:0.159855\n",
      "[epoch:20,batch:269]:acc: 0.942593,loss:0.159074\n",
      "[epoch:20,batch:299]:acc: 0.940729,loss:0.161718\n",
      "[epoch:20,batch:299]: val_loss:0.348862,val_acc:0.871778,val_total:4539\n",
      "[epoch:20,batch:329]:acc: 0.941098,loss:0.161462\n",
      "[epoch:20,batch:359]:acc: 0.940538,loss:0.162351\n",
      "[epoch:20,batch:389]:acc: 0.941106,loss:0.161353\n",
      "[epoch:20,batch:419]:acc: 0.940327,loss:0.162211\n",
      "[epoch:20,batch:449]:acc: 0.939375,loss:0.162987\n",
      "[epoch:20,batch:479]:acc: 0.938932,loss:0.164374\n",
      "[epoch:20,batch:509]:acc: 0.939154,loss:0.163970\n",
      "[epoch:20,batch:539]:acc: 0.939525,loss:0.163586\n",
      "[epoch:20,batch:569]:acc: 0.939474,loss:0.163330\n",
      "[epoch:20,batch:599]:acc: 0.939375,loss:0.163297\n",
      "[epoch:20,batch:599]: val_loss:0.349405,val_acc:0.869354,val_total:4539\n",
      "[epoch:20,batch:629]:acc: 0.939038,loss:0.163947\n",
      "[epoch:20,batch:659]:acc: 0.938021,loss:0.164792\n",
      "[epoch:20,batch:689]:acc: 0.938315,loss:0.164177\n",
      "[epoch:20,batch:719]:acc: 0.938368,loss:0.164787\n",
      "[epoch:20,batch:749]:acc: 0.938208,loss:0.164592\n",
      "[epoch:20,batch:779]:acc: 0.938301,loss:0.164487\n",
      "[epoch:20,batch:809]:acc: 0.938503,loss:0.164640\n",
      "[epoch:20,batch:839]:acc: 0.938690,loss:0.163831\n",
      "[epoch:20,batch:869]:acc: 0.938901,loss:0.163825\n",
      "[epoch:20,batch:899]:acc: 0.938958,loss:0.163588\n",
      "[epoch:20,batch:899]: val_loss:0.349660,val_acc:0.869795,val_total:4539\n",
      "[epoch:20,batch:929]:acc: 0.938878,loss:0.164162\n",
      "[epoch:20,batch:959]:acc: 0.938932,loss:0.164266\n",
      "[epoch:20,batch:989]:acc: 0.938668,loss:0.164694\n",
      "[epoch:20] :acc: 0.938645,loss:0.164752,lr:0.000004,patience:2\n",
      "[epoch:20]: val_loss:0.350138,val_acc:0.869134,\n",
      "Epoch 21/59\n",
      "----------\n",
      "lr desencd\n",
      "[epoch:21,batch:29]:acc: 0.939583,loss:0.160458\n",
      "[epoch:21,batch:59]:acc: 0.944792,loss:0.157788\n",
      "[epoch:21,batch:89]:acc: 0.941667,loss:0.159880\n",
      "[epoch:21,batch:119]:acc: 0.943229,loss:0.159259\n",
      "[epoch:21,batch:149]:acc: 0.943125,loss:0.159469\n",
      "[epoch:21,batch:179]:acc: 0.942014,loss:0.160838\n",
      "[epoch:21,batch:209]:acc: 0.941220,loss:0.161472\n",
      "[epoch:21,batch:239]:acc: 0.939063,loss:0.163454\n",
      "[epoch:21,batch:269]:acc: 0.939120,loss:0.164428\n",
      "[epoch:21,batch:299]:acc: 0.938958,loss:0.164011\n",
      "[epoch:21,batch:299]: val_loss:0.350770,val_acc:0.869134,val_total:4539\n",
      "[epoch:21,batch:329]:acc: 0.939773,loss:0.163263\n",
      "[epoch:21,batch:359]:acc: 0.940191,loss:0.162755\n",
      "[epoch:21,batch:389]:acc: 0.939663,loss:0.163926\n",
      "[epoch:21,batch:419]:acc: 0.939063,loss:0.164480\n",
      "[epoch:21,batch:449]:acc: 0.940417,loss:0.162377\n",
      "[epoch:21,batch:479]:acc: 0.940885,loss:0.161837\n",
      "[epoch:21,batch:509]:acc: 0.941544,loss:0.161802\n",
      "[epoch:21,batch:539]:acc: 0.940799,loss:0.162430\n",
      "[epoch:21,batch:569]:acc: 0.939857,loss:0.162806\n",
      "[epoch:21,batch:599]:acc: 0.939792,loss:0.162809\n",
      "[epoch:21,batch:599]: val_loss:0.348190,val_acc:0.871337,val_total:4539\n",
      "[epoch:21,batch:629]:acc: 0.939187,loss:0.164219\n",
      "[epoch:21,batch:659]:acc: 0.939015,loss:0.164258\n",
      "[epoch:21,batch:689]:acc: 0.938859,loss:0.164580\n",
      "[epoch:21,batch:719]:acc: 0.938759,loss:0.164471\n",
      "[epoch:21,batch:749]:acc: 0.938625,loss:0.164290\n",
      "[epoch:21,batch:779]:acc: 0.938782,loss:0.163945\n",
      "[epoch:21,batch:809]:acc: 0.938850,loss:0.164148\n",
      "[epoch:21,batch:839]:acc: 0.938951,loss:0.164160\n",
      "[epoch:21,batch:869]:acc: 0.939152,loss:0.163923\n",
      "[epoch:21,batch:899]:acc: 0.939549,loss:0.163714\n",
      "[epoch:21,batch:899]: val_loss:0.350589,val_acc:0.871337,val_total:4539\n",
      "[epoch:21,batch:929]:acc: 0.939651,loss:0.163349\n",
      "[epoch:21,batch:959]:acc: 0.939518,loss:0.163331\n",
      "[epoch:21,batch:989]:acc: 0.939489,loss:0.163133\n",
      "[epoch:21] :acc: 0.939528,loss:0.163219,lr:0.000001,patience:0\n",
      "[epoch:21]: val_loss:0.351995,val_acc:0.870897,\n",
      "Epoch 22/59\n",
      "----------\n",
      "[epoch:22,batch:29]:acc: 0.930208,loss:0.186203\n",
      "[epoch:22,batch:59]:acc: 0.931250,loss:0.176568\n",
      "[epoch:22,batch:89]:acc: 0.930556,loss:0.175987\n",
      "[epoch:22,batch:119]:acc: 0.935417,loss:0.170885\n",
      "[epoch:22,batch:149]:acc: 0.936875,loss:0.170059\n",
      "[epoch:22,batch:179]:acc: 0.935764,loss:0.170191\n",
      "[epoch:22,batch:209]:acc: 0.935119,loss:0.169646\n",
      "[epoch:22,batch:239]:acc: 0.935937,loss:0.169606\n",
      "[epoch:22,batch:269]:acc: 0.936227,loss:0.168557\n",
      "[epoch:22,batch:299]:acc: 0.937292,loss:0.167331\n",
      "[epoch:22,batch:299]: val_loss:0.348631,val_acc:0.869795,val_total:4539\n",
      "[epoch:22,batch:329]:acc: 0.938920,loss:0.165220\n",
      "[epoch:22,batch:359]:acc: 0.938715,loss:0.165676\n",
      "[epoch:22,batch:389]:acc: 0.940144,loss:0.163217\n",
      "[epoch:22,batch:419]:acc: 0.940997,loss:0.162287\n",
      "[epoch:22,batch:449]:acc: 0.940069,loss:0.163676\n",
      "[epoch:22,batch:479]:acc: 0.939974,loss:0.164152\n",
      "[epoch:22,batch:509]:acc: 0.940257,loss:0.163895\n",
      "[epoch:22,batch:539]:acc: 0.939988,loss:0.164150\n",
      "[epoch:22,batch:569]:acc: 0.939857,loss:0.164188\n",
      "[epoch:22,batch:599]:acc: 0.940729,loss:0.162717\n",
      "[epoch:22,batch:599]: val_loss:0.349595,val_acc:0.868694,val_total:4539\n",
      "[epoch:22,batch:629]:acc: 0.940675,loss:0.162334\n",
      "[epoch:22,batch:659]:acc: 0.940767,loss:0.162374\n",
      "[epoch:22,batch:689]:acc: 0.940534,loss:0.162640\n",
      "[epoch:22,batch:719]:acc: 0.939844,loss:0.163601\n",
      "[epoch:22,batch:749]:acc: 0.940375,loss:0.163354\n",
      "[epoch:22,batch:779]:acc: 0.940345,loss:0.163404\n",
      "[epoch:22,batch:809]:acc: 0.940278,loss:0.163547\n",
      "[epoch:22,batch:839]:acc: 0.939881,loss:0.163658\n",
      "[epoch:22,batch:869]:acc: 0.939727,loss:0.163718\n",
      "[epoch:22,batch:899]:acc: 0.939757,loss:0.163572\n",
      "[epoch:22,batch:899]: val_loss:0.349534,val_acc:0.869354,val_total:4539\n",
      "[epoch:22,batch:929]:acc: 0.939987,loss:0.163541\n",
      "[epoch:22,batch:959]:acc: 0.940267,loss:0.163545\n",
      "[epoch:22,batch:989]:acc: 0.940467,loss:0.163712\n",
      "[epoch:22] :acc: 0.940411,loss:0.163753,lr:0.000001,patience:1\n",
      "[epoch:22]: val_loss:0.350338,val_acc:0.870015,\n",
      "Epoch 23/59\n",
      "----------\n",
      "[epoch:23,batch:29]:acc: 0.923958,loss:0.180237\n",
      "[epoch:23,batch:59]:acc: 0.931250,loss:0.178099\n",
      "[epoch:23,batch:89]:acc: 0.933333,loss:0.173633\n",
      "[epoch:23,batch:119]:acc: 0.936198,loss:0.167739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:23,batch:149]:acc: 0.935000,loss:0.170019\n",
      "[epoch:23,batch:179]:acc: 0.938715,loss:0.165384\n",
      "[epoch:23,batch:209]:acc: 0.938393,loss:0.164417\n",
      "[epoch:23,batch:239]:acc: 0.938932,loss:0.162809\n",
      "[epoch:23,batch:269]:acc: 0.936921,loss:0.165972\n",
      "[epoch:23,batch:299]:acc: 0.936458,loss:0.167109\n",
      "[epoch:23,batch:299]: val_loss:0.349482,val_acc:0.869354,val_total:4539\n",
      "[epoch:23,batch:329]:acc: 0.936932,loss:0.166577\n",
      "[epoch:23,batch:359]:acc: 0.935851,loss:0.167590\n",
      "[epoch:23,batch:389]:acc: 0.936058,loss:0.167469\n",
      "[epoch:23,batch:419]:acc: 0.935789,loss:0.167634\n",
      "[epoch:23,batch:449]:acc: 0.935972,loss:0.167277\n",
      "[epoch:23,batch:479]:acc: 0.936719,loss:0.165895\n",
      "[epoch:23,batch:509]:acc: 0.937255,loss:0.165263\n",
      "[epoch:23,batch:539]:acc: 0.937674,loss:0.164901\n",
      "[epoch:23,batch:569]:acc: 0.937884,loss:0.164390\n",
      "[epoch:23,batch:599]:acc: 0.938073,loss:0.163721\n",
      "[epoch:23,batch:599]: val_loss:0.350466,val_acc:0.870015,val_total:4539\n",
      "[epoch:23,batch:629]:acc: 0.937897,loss:0.164197\n",
      "[epoch:23,batch:659]:acc: 0.937689,loss:0.164010\n",
      "[epoch:23,batch:689]:acc: 0.938134,loss:0.163198\n",
      "[epoch:23,batch:719]:acc: 0.938194,loss:0.162830\n",
      "[epoch:23,batch:749]:acc: 0.938083,loss:0.163019\n",
      "[epoch:23,batch:779]:acc: 0.938421,loss:0.162623\n",
      "[epoch:23,batch:809]:acc: 0.938735,loss:0.162232\n",
      "[epoch:23,batch:839]:acc: 0.938839,loss:0.161982\n",
      "[epoch:23,batch:869]:acc: 0.938757,loss:0.162122\n",
      "[epoch:23,batch:899]:acc: 0.938854,loss:0.162324\n",
      "[epoch:23,batch:899]: val_loss:0.349908,val_acc:0.871117,val_total:4539\n",
      "[epoch:23,batch:929]:acc: 0.938474,loss:0.163087\n",
      "[epoch:23,batch:959]:acc: 0.938835,loss:0.162441\n",
      "[epoch:23,batch:989]:acc: 0.938920,loss:0.162206\n",
      "[epoch:23] :acc: 0.938897,loss:0.162269,lr:0.000001,patience:2\n",
      "[epoch:23]: val_loss:0.349703,val_acc:0.868694,\n",
      "Epoch 24/59\n",
      "----------\n",
      "lr desencd\n",
      "[epoch:24,batch:29]:acc: 0.931250,loss:0.173897\n",
      "[epoch:24,batch:59]:acc: 0.938021,loss:0.169367\n",
      "[epoch:24,batch:89]:acc: 0.935069,loss:0.168146\n",
      "[epoch:24,batch:119]:acc: 0.934375,loss:0.168745\n",
      "[epoch:24,batch:149]:acc: 0.932917,loss:0.170410\n",
      "[epoch:24,batch:179]:acc: 0.934375,loss:0.169293\n",
      "[epoch:24,batch:209]:acc: 0.934226,loss:0.170722\n",
      "[epoch:24,batch:239]:acc: 0.934245,loss:0.169836\n",
      "[epoch:24,batch:269]:acc: 0.934838,loss:0.168804\n",
      "[epoch:24,batch:299]:acc: 0.936042,loss:0.168295\n",
      "[epoch:24,batch:299]: val_loss:0.348930,val_acc:0.868914,val_total:4539\n",
      "[epoch:24,batch:329]:acc: 0.935133,loss:0.169441\n",
      "[epoch:24,batch:359]:acc: 0.936285,loss:0.167767\n",
      "[epoch:24,batch:389]:acc: 0.936378,loss:0.167697\n",
      "[epoch:24,batch:419]:acc: 0.936756,loss:0.167549\n",
      "[epoch:24,batch:449]:acc: 0.936806,loss:0.166993\n",
      "[epoch:24,batch:479]:acc: 0.936979,loss:0.166681\n",
      "[epoch:24,batch:509]:acc: 0.936887,loss:0.166645\n",
      "[epoch:24,batch:539]:acc: 0.937442,loss:0.166116\n",
      "[epoch:24,batch:569]:acc: 0.937281,loss:0.166412\n",
      "[epoch:24,batch:599]:acc: 0.937292,loss:0.166659\n",
      "[epoch:24,batch:599]: val_loss:0.349490,val_acc:0.869354,val_total:4539\n",
      "[epoch:24,batch:629]:acc: 0.938095,loss:0.165271\n",
      "[epoch:24,batch:659]:acc: 0.938447,loss:0.164547\n",
      "[epoch:24,batch:689]:acc: 0.938587,loss:0.164080\n",
      "[epoch:24,batch:719]:acc: 0.938715,loss:0.164219\n",
      "[epoch:24,batch:749]:acc: 0.938792,loss:0.163770\n",
      "[epoch:24,batch:779]:acc: 0.938702,loss:0.163672\n",
      "[epoch:24,batch:809]:acc: 0.938272,loss:0.164075\n",
      "[epoch:24,batch:839]:acc: 0.938244,loss:0.164157\n",
      "[epoch:24,batch:869]:acc: 0.938434,loss:0.163776\n",
      "[epoch:24,batch:899]:acc: 0.938229,loss:0.164205\n",
      "[epoch:24,batch:899]: val_loss:0.350320,val_acc:0.870456,val_total:4539\n",
      "[epoch:24,batch:929]:acc: 0.938340,loss:0.164135\n",
      "[epoch:24,batch:959]:acc: 0.938216,loss:0.164027\n",
      "[epoch:24,batch:989]:acc: 0.938415,loss:0.163605\n",
      "[epoch:24] :acc: 0.938424,loss:0.164265,lr:0.000000,patience:0\n",
      "[epoch:24]: val_loss:0.349774,val_acc:0.870456,\n",
      "Epoch 25/59\n",
      "----------\n",
      "[epoch:25,batch:29]:acc: 0.930208,loss:0.171404\n",
      "[epoch:25,batch:59]:acc: 0.933333,loss:0.171202\n",
      "[epoch:25,batch:89]:acc: 0.934028,loss:0.166659\n",
      "[epoch:25,batch:119]:acc: 0.935677,loss:0.166163\n",
      "[epoch:25,batch:149]:acc: 0.939792,loss:0.162061\n",
      "[epoch:25,batch:179]:acc: 0.939931,loss:0.161849\n",
      "[epoch:25,batch:209]:acc: 0.940179,loss:0.161130\n",
      "[epoch:25,batch:239]:acc: 0.941406,loss:0.159627\n",
      "[epoch:25,batch:269]:acc: 0.941898,loss:0.158857\n",
      "[epoch:25,batch:299]:acc: 0.941562,loss:0.159512\n",
      "[epoch:25,batch:299]: val_loss:0.348546,val_acc:0.870897,val_total:4539\n",
      "[epoch:25,batch:329]:acc: 0.940436,loss:0.160706\n",
      "[epoch:25,batch:359]:acc: 0.939931,loss:0.161022\n",
      "[epoch:25,batch:389]:acc: 0.940465,loss:0.161616\n",
      "[epoch:25,batch:419]:acc: 0.940253,loss:0.162166\n",
      "[epoch:25,batch:449]:acc: 0.940000,loss:0.162028\n",
      "[epoch:25,batch:479]:acc: 0.939583,loss:0.161165\n",
      "[epoch:25,batch:509]:acc: 0.939461,loss:0.161461\n",
      "[epoch:25,batch:539]:acc: 0.939583,loss:0.161312\n",
      "[epoch:25,batch:569]:acc: 0.939474,loss:0.161518\n",
      "[epoch:25,batch:599]:acc: 0.939375,loss:0.162273\n",
      "[epoch:25,batch:599]: val_loss:0.348874,val_acc:0.870236,val_total:4539\n",
      "[epoch:25,batch:629]:acc: 0.939484,loss:0.161898\n",
      "[epoch:25,batch:659]:acc: 0.939205,loss:0.161963\n",
      "[epoch:25,batch:689]:acc: 0.938768,loss:0.162611\n",
      "[epoch:25,batch:719]:acc: 0.938021,loss:0.163408\n",
      "[epoch:25,batch:749]:acc: 0.938292,loss:0.163445\n",
      "[epoch:25,batch:779]:acc: 0.938381,loss:0.163406\n",
      "[epoch:25,batch:809]:acc: 0.938233,loss:0.163897\n",
      "[epoch:25,batch:839]:acc: 0.938244,loss:0.163835\n",
      "[epoch:25,batch:869]:acc: 0.938147,loss:0.164174\n",
      "[epoch:25,batch:899]:acc: 0.938090,loss:0.164549\n",
      "[epoch:25,batch:899]: val_loss:0.348211,val_acc:0.870456,val_total:4539\n",
      "[epoch:25,batch:929]:acc: 0.938172,loss:0.164046\n",
      "[epoch:25,batch:959]:acc: 0.938444,loss:0.163902\n",
      "[epoch:25,batch:989]:acc: 0.938163,loss:0.164188\n",
      "[epoch:25] :acc: 0.938203,loss:0.164122,lr:0.000000,patience:1\n",
      "[epoch:25]: val_loss:0.350051,val_acc:0.868473,\n",
      "Epoch 26/59\n",
      "----------\n",
      "[epoch:26,batch:29]:acc: 0.936458,loss:0.164917\n",
      "[epoch:26,batch:59]:acc: 0.936458,loss:0.161174\n",
      "[epoch:26,batch:89]:acc: 0.936458,loss:0.160592\n",
      "[epoch:26,batch:119]:acc: 0.938802,loss:0.157902\n",
      "[epoch:26,batch:149]:acc: 0.939375,loss:0.157204\n",
      "[epoch:26,batch:179]:acc: 0.939583,loss:0.156178\n",
      "[epoch:26,batch:209]:acc: 0.939732,loss:0.157168\n",
      "[epoch:26,batch:239]:acc: 0.941276,loss:0.155828\n",
      "[epoch:26,batch:269]:acc: 0.942014,loss:0.154442\n",
      "[epoch:26,batch:299]:acc: 0.943229,loss:0.152770\n",
      "[epoch:26,batch:299]: val_loss:0.349829,val_acc:0.869354,val_total:4539\n",
      "[epoch:26,batch:329]:acc: 0.943371,loss:0.153030\n",
      "[epoch:26,batch:359]:acc: 0.942708,loss:0.153320\n",
      "[epoch:26,batch:389]:acc: 0.943429,loss:0.152947\n",
      "[epoch:26,batch:419]:acc: 0.943080,loss:0.153287\n",
      "[epoch:26,batch:449]:acc: 0.942292,loss:0.155448\n",
      "[epoch:26,batch:479]:acc: 0.941667,loss:0.156314\n",
      "[epoch:26,batch:509]:acc: 0.941299,loss:0.156708\n",
      "[epoch:26,batch:539]:acc: 0.940509,loss:0.157790\n",
      "[epoch:26,batch:569]:acc: 0.940186,loss:0.158298\n",
      "[epoch:26,batch:599]:acc: 0.940052,loss:0.158516\n",
      "[epoch:26,batch:599]: val_loss:0.349736,val_acc:0.871337,val_total:4539\n",
      "[epoch:26,batch:629]:acc: 0.940129,loss:0.158959\n",
      "[epoch:26,batch:659]:acc: 0.939678,loss:0.159832\n",
      "[epoch:26,batch:689]:acc: 0.939583,loss:0.159916\n",
      "[epoch:26,batch:719]:acc: 0.939149,loss:0.160214\n",
      "[epoch:26,batch:749]:acc: 0.939083,loss:0.160543\n",
      "[epoch:26,batch:779]:acc: 0.938862,loss:0.161010\n",
      "[epoch:26,batch:809]:acc: 0.938619,loss:0.161378\n",
      "[epoch:26,batch:839]:acc: 0.938616,loss:0.161509\n",
      "[epoch:26,batch:869]:acc: 0.938254,loss:0.162535\n",
      "[epoch:26,batch:899]:acc: 0.938368,loss:0.162933\n",
      "[epoch:26,batch:899]: val_loss:0.349259,val_acc:0.870676,val_total:4539\n",
      "[epoch:26,batch:929]:acc: 0.938273,loss:0.163151\n",
      "[epoch:26,batch:959]:acc: 0.938346,loss:0.163418\n",
      "[epoch:26,batch:989]:acc: 0.938131,loss:0.163431\n",
      "[epoch:26] :acc: 0.938046,loss:0.163584,lr:0.000000,patience:2\n",
      "[epoch:26]: val_loss:0.348095,val_acc:0.869575,\n",
      "Epoch 27/59\n",
      "----------\n",
      "lr desencd\n",
      "[epoch:27,batch:29]:acc: 0.928125,loss:0.170742\n",
      "[epoch:27,batch:59]:acc: 0.940104,loss:0.159973\n",
      "[epoch:27,batch:89]:acc: 0.941319,loss:0.162629\n",
      "[epoch:27,batch:119]:acc: 0.941927,loss:0.163239\n",
      "[epoch:27,batch:149]:acc: 0.941250,loss:0.164902\n",
      "[epoch:27,batch:179]:acc: 0.941146,loss:0.165556\n",
      "[epoch:27,batch:209]:acc: 0.939881,loss:0.167345\n",
      "[epoch:27,batch:239]:acc: 0.939844,loss:0.165744\n",
      "[epoch:27,batch:269]:acc: 0.939120,loss:0.165445\n",
      "[epoch:27,batch:299]:acc: 0.938750,loss:0.166201\n",
      "[epoch:27,batch:299]: val_loss:0.348414,val_acc:0.870236,val_total:4539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:27,batch:329]:acc: 0.938731,loss:0.165815\n",
      "[epoch:27,batch:359]:acc: 0.939236,loss:0.165240\n",
      "[epoch:27,batch:389]:acc: 0.938702,loss:0.165697\n",
      "[epoch:27,batch:419]:acc: 0.938616,loss:0.165371\n",
      "[epoch:27,batch:449]:acc: 0.938681,loss:0.165224\n",
      "[epoch:27,batch:479]:acc: 0.938281,loss:0.164935\n",
      "[epoch:27,batch:509]:acc: 0.937990,loss:0.165564\n",
      "[epoch:27,batch:539]:acc: 0.938831,loss:0.164843\n",
      "[epoch:27,batch:569]:acc: 0.938871,loss:0.164867\n",
      "[epoch:27,batch:599]:acc: 0.939688,loss:0.163945\n",
      "[epoch:27,batch:599]: val_loss:0.348760,val_acc:0.870236,val_total:4539\n",
      "[epoch:27,batch:629]:acc: 0.939484,loss:0.163792\n",
      "[epoch:27,batch:659]:acc: 0.939110,loss:0.163916\n",
      "[epoch:27,batch:689]:acc: 0.939176,loss:0.163289\n",
      "[epoch:27,batch:719]:acc: 0.938585,loss:0.163906\n",
      "[epoch:27,batch:749]:acc: 0.938958,loss:0.163441\n",
      "[epoch:27,batch:779]:acc: 0.939143,loss:0.163278\n",
      "[epoch:27,batch:809]:acc: 0.939429,loss:0.162837\n",
      "[epoch:27,batch:839]:acc: 0.939025,loss:0.163202\n",
      "[epoch:27,batch:869]:acc: 0.938757,loss:0.163681\n",
      "[epoch:27,batch:899]:acc: 0.938993,loss:0.163421\n",
      "[epoch:27,batch:899]: val_loss:0.349750,val_acc:0.870236,val_total:4539\n",
      "[epoch:27,batch:929]:acc: 0.938978,loss:0.163258\n",
      "[epoch:27,batch:959]:acc: 0.938672,loss:0.163331\n",
      "[epoch:27,batch:989]:acc: 0.938889,loss:0.163292\n",
      "[epoch:27] :acc: 0.938803,loss:0.163621,lr:0.000000,patience:0\n",
      "[epoch:27]: val_loss:0.348975,val_acc:0.871558,\n",
      "Epoch 28/59\n",
      "----------\n",
      "[epoch:28,batch:29]:acc: 0.938542,loss:0.164458\n",
      "[epoch:28,batch:59]:acc: 0.943750,loss:0.156145\n",
      "[epoch:28,batch:89]:acc: 0.942361,loss:0.158431\n",
      "[epoch:28,batch:119]:acc: 0.941406,loss:0.156747\n",
      "[epoch:28,batch:149]:acc: 0.941458,loss:0.157837\n",
      "[epoch:28,batch:179]:acc: 0.940972,loss:0.158536\n",
      "[epoch:28,batch:209]:acc: 0.938393,loss:0.161824\n",
      "[epoch:28,batch:239]:acc: 0.938411,loss:0.162581\n",
      "[epoch:28,batch:269]:acc: 0.938426,loss:0.163687\n",
      "[epoch:28,batch:299]:acc: 0.937813,loss:0.164935\n",
      "[epoch:28,batch:299]: val_loss:0.348473,val_acc:0.870676,val_total:4539\n",
      "[epoch:28,batch:329]:acc: 0.938352,loss:0.163455\n",
      "[epoch:28,batch:359]:acc: 0.938194,loss:0.163847\n",
      "[epoch:28,batch:389]:acc: 0.938862,loss:0.163027\n",
      "[epoch:28,batch:419]:acc: 0.938839,loss:0.162902\n",
      "[epoch:28,batch:449]:acc: 0.939028,loss:0.162137\n",
      "[epoch:28,batch:479]:acc: 0.939844,loss:0.161349\n",
      "[epoch:28,batch:509]:acc: 0.939461,loss:0.161593\n",
      "[epoch:28,batch:539]:acc: 0.939352,loss:0.162190\n",
      "[epoch:28,batch:569]:acc: 0.938706,loss:0.162874\n",
      "[epoch:28,batch:599]:acc: 0.938698,loss:0.163358\n",
      "[epoch:28,batch:599]: val_loss:0.350137,val_acc:0.870236,val_total:4539\n",
      "[epoch:28,batch:629]:acc: 0.938889,loss:0.162708\n",
      "[epoch:28,batch:659]:acc: 0.939110,loss:0.162854\n",
      "[epoch:28,batch:689]:acc: 0.939312,loss:0.162469\n",
      "[epoch:28,batch:719]:acc: 0.939019,loss:0.162820\n",
      "[epoch:28,batch:749]:acc: 0.939208,loss:0.162667\n",
      "[epoch:28,batch:779]:acc: 0.939263,loss:0.163069\n",
      "[epoch:28,batch:809]:acc: 0.939583,loss:0.163012\n",
      "[epoch:28,batch:839]:acc: 0.939844,loss:0.162837\n",
      "[epoch:28,batch:869]:acc: 0.939727,loss:0.163084\n",
      "[epoch:28,batch:899]:acc: 0.939444,loss:0.163457\n",
      "[epoch:28,batch:899]: val_loss:0.349903,val_acc:0.870015,val_total:4539\n",
      "[epoch:28,batch:929]:acc: 0.939079,loss:0.163695\n",
      "[epoch:28,batch:959]:acc: 0.939160,loss:0.163814\n",
      "[epoch:28,batch:989]:acc: 0.939268,loss:0.163661\n",
      "[epoch:28] :acc: 0.939212,loss:0.164315,lr:0.000000,patience:1\n",
      "[epoch:28]: val_loss:0.349228,val_acc:0.870676,\n",
      "Epoch 29/59\n",
      "----------\n",
      "[epoch:29,batch:29]:acc: 0.933333,loss:0.168398\n",
      "[epoch:29,batch:59]:acc: 0.933854,loss:0.172100\n",
      "[epoch:29,batch:89]:acc: 0.934722,loss:0.168440\n",
      "[epoch:29,batch:119]:acc: 0.939063,loss:0.160805\n",
      "[epoch:29,batch:149]:acc: 0.936042,loss:0.165825\n",
      "[epoch:29,batch:179]:acc: 0.935764,loss:0.168630\n",
      "[epoch:29,batch:209]:acc: 0.936458,loss:0.167026\n",
      "[epoch:29,batch:239]:acc: 0.936849,loss:0.165740\n",
      "[epoch:29,batch:269]:acc: 0.935417,loss:0.168166\n",
      "[epoch:29,batch:299]:acc: 0.936146,loss:0.166688\n",
      "[epoch:29,batch:299]: val_loss:0.349960,val_acc:0.870015,val_total:4539\n",
      "[epoch:29,batch:329]:acc: 0.936364,loss:0.166952\n",
      "[epoch:29,batch:359]:acc: 0.937326,loss:0.166035\n",
      "[epoch:29,batch:389]:acc: 0.938622,loss:0.164047\n",
      "[epoch:29,batch:419]:acc: 0.938765,loss:0.163438\n",
      "[epoch:29,batch:449]:acc: 0.938750,loss:0.163921\n",
      "[epoch:29,batch:479]:acc: 0.939518,loss:0.163002\n",
      "[epoch:29,batch:509]:acc: 0.938603,loss:0.163944\n",
      "[epoch:29,batch:539]:acc: 0.938889,loss:0.163516\n",
      "[epoch:29,batch:569]:acc: 0.939200,loss:0.163127\n",
      "[epoch:29,batch:599]:acc: 0.939271,loss:0.163124\n",
      "[epoch:29,batch:599]: val_loss:0.349436,val_acc:0.871337,val_total:4539\n",
      "[epoch:29,batch:629]:acc: 0.939633,loss:0.162365\n",
      "[epoch:29,batch:659]:acc: 0.939205,loss:0.162851\n",
      "[epoch:29,batch:689]:acc: 0.939085,loss:0.163124\n",
      "[epoch:29,batch:719]:acc: 0.939106,loss:0.162253\n",
      "[epoch:29,batch:749]:acc: 0.939542,loss:0.161867\n",
      "[epoch:29,batch:779]:acc: 0.939984,loss:0.161547\n",
      "[epoch:29,batch:809]:acc: 0.939545,loss:0.162370\n",
      "[epoch:29,batch:839]:acc: 0.939397,loss:0.162785\n",
      "[epoch:29,batch:869]:acc: 0.939080,loss:0.163049\n"
     ]
    }
   ],
   "source": [
    "TrainWithRawData('../model/DesNet201/2018-11-01_acc_best.pth',60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_loss is :0.349666\n",
      "val_correct is 0.871778\n"
     ]
    }
   ],
   "source": [
    "modelParams=torch.load('../model/DesNet201/2018-11-01_acc_best.pth')\n",
    "min_loss=modelParams['val_loss']\n",
    "print('min_loss is :%f'%(min_loss))\n",
    "print('val_correct is %f'%(modelParams['val_correct']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Conda_Env_Pytorch]",
   "language": "python",
   "name": "conda-env-Conda_Env_Pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
