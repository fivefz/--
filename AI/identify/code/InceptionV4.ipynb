{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CropModels\n",
    "from CropDataset import MyDataSet,normalize_torch,normalize_05,normalize_dataset,preprocess,preprocess_hflip,preprocess_with_augmentation\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from tensorboardX import SummaryWriter\n",
    "import datetime\n",
    "import os\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from utils import RunningMean\n",
    "import utils\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "NB_CLASS=59\n",
    "SEED=888\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "IMAGE_SIZE=299    # 不同模型修改不同的Size\n",
    "IMAGE_TRAIN_PRE='../data/AgriculturalDisease_trainingset/images/'\n",
    "ANNOTATION_TRAIN='../data/AgriculturalDisease_trainingset/AgriculturalDisease_train_annotations_deleteNoise.json' #是否需要剔除两类异常类\n",
    "IMAGE_VAL_PRE='../data/AgriculturalDisease_validationset/images/'\n",
    "ANNOTATION_VAL='../data/AgriculturalDisease_validationset/AgriculturalDisease_validation_annotations_deleteNoise.json' #是否需要剔除两类异常类\n",
    "date=str(datetime.date.today())\n",
    "with open(ANNOTATION_TRAIN) as datafile1:\n",
    "    trainDataFram=pd.read_json(datafile1,orient='records')\n",
    "with open(ANNOTATION_VAL) as datafile2: #first check if it's a valid json file or not\n",
    "    validateDataFram =pd.read_json(datafile2,orient='records')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getmodel():\n",
    "    print('[+] loading model... ', end='', flush=True)\n",
    "    model=CropModels.inceptionv4_finetune(NB_CLASS)\n",
    "    model.cuda()\n",
    "    print('Done')\n",
    "    return model\n",
    "def train(epochNum):\n",
    "    writer=SummaryWriter('log/'+date+'/InceptionV4/') # 创建 /log/日期/InceptionResnet的组织形式  不同模型需要修改不同名称\n",
    "    train_dataset=MyDataSet(json_Description=ANNOTATION_TRAIN,transform=preprocess_with_augmentation(normalize_05,IMAGE_SIZE),path_pre=IMAGE_TRAIN_PRE)\n",
    "    val_dataset=MyDataSet(json_Description=ANNOTATION_VAL,transform=preprocess(normalize_05,IMAGE_SIZE),path_pre=IMAGE_VAL_PRE)\n",
    "    train_dataLoader=DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE,num_workers=16,shuffle=True)\n",
    "    val_dataLoader=DataLoader(dataset=val_dataset,batch_size=BATCH_SIZE,num_workers=1,shuffle=False)\n",
    "    model=getmodel()\n",
    "    weight=torch.Tensor([1,3,3,3,3,4,2,3,3,3,3,3,3,3,3,3,2,3,3,3,2,3,4,2,3,1,1,3,2,2,1,3,3,1,3,2,3,3,3,3,2,1,3,2,3,3,3,1,3,3,4,4,3,2,2,3,1,1,3]).cuda()\n",
    "    criterion=nn.CrossEntropyLoss(weight=weight).cuda()\n",
    "#     lx, px = utils.predict(model,val_dataLoader)\n",
    "#     min_loss = criterion(Variable(px), Variable(lx)).item()\n",
    "    min_loss=4.1\n",
    "    print('min_loss is :%f'%(min_loss))\n",
    "    min_acc=0.80\n",
    "    patience=0\n",
    "    lr=0.0\n",
    "    momentum=0.9\n",
    "    for epoch in range(epochNum):\n",
    "        print('Epoch {}/{}'.format(epoch, epochNum - 1))\n",
    "        print('-' * 10)\n",
    "        if epoch==3 or epoch==4 or epoch==5:\n",
    "            lr=0.00006\n",
    "            momentum=0.95\n",
    "            print('set lr=:%f,momentum=%f'%(lr,momentum))\n",
    "        if epoch==6:\n",
    "            lr=1e-4\n",
    "            momentum=0.9\n",
    "            print('set lr=:%f,momentum=%f'%(lr,momentum))        \n",
    "        if patience==2:\n",
    "            patience=0\n",
    "            model.load_state_dict(torch.load('../model/InceptionV4/'+date+'_loss_best.pth')['state_dict'])\n",
    "            lr=lr/10\n",
    "            print('loss has increased lr divide 10 lr now is :%f'%(lr))\n",
    "        if epoch==0 or epoch==1 or epoch==2: #第一轮首先训练全连接层\n",
    "            lr=1e-3\n",
    "           # optimizer = torch.optim.Adam(model.fresh_params(),lr = lr,amsgrad=True,weight_decay=1e-4)\n",
    "#             optimizer=torch.optim.SGD(params=model.fresh_params(),lr=lr,momentum=0.9)\n",
    "            #optimizer=torch.optim.SGD(params=model.fresh_params(),lr=lr,momentum=0.9)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(model.parameters(),lr = lr,amsgrad=True,weight_decay=1e-4)\n",
    "             #optimizer=torch.optim.SGD(params=model.parameters(),lr=lr,momentum=momentum)\n",
    "#             optimizer=torch.optim.SGD(params=model.parameters(),lr=lr,momentum=momentum)\n",
    "        running_loss = RunningMean()\n",
    "        running_corrects = RunningMean()\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_dataLoader):\n",
    "            model.train(True)\n",
    "            n_batchsize=inputs.size(0)\n",
    "            inputs = Variable(inputs).cuda()\n",
    "            labels = Variable(labels).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            if isinstance(outputs,tuple):\n",
    "                loss=sum((criterion(o,labels)) for o in outputs)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "            running_loss.update(loss.item(),1)\n",
    "            running_corrects.update(torch.sum(preds == labels.data).data,n_batchsize)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx%30==29:\n",
    "                print('[epoch:%d,batch:%d]:acc: %f,loss:%f'%(epoch,batch_idx,running_corrects.value,running_loss.value))\n",
    "                if batch_idx%300==299: \n",
    "                    niter = epoch * len(train_dataset)/BATCH_SIZE + batch_idx\n",
    "                    writer.add_scalar('Train/Acc',running_corrects.value,niter)\n",
    "                    writer.add_scalar('Train/Loss',running_loss.value,niter)\n",
    "                    lx,px=utils.predict(model,val_dataLoader)\n",
    "                    log_loss = criterion(Variable(px), Variable(lx))\n",
    "                    log_loss = log_loss.item()\n",
    "                    _, preds = torch.max(px, dim=1)\n",
    "                    accuracy = torch.mean((preds == lx).float())\n",
    "                    writer.add_scalar('Val/Acc',accuracy,niter)\n",
    "                    writer.add_scalar('Val/Loss',log_loss,niter)\n",
    "                    print('[epoch:%d,batch:%d]: val_loss:%f,val_acc:%f,val_total:%d'%(epoch,batch_idx,log_loss,accuracy,len(val_dataset)))\n",
    "        print('[epoch:%d] :acc: %f,loss:%f,lr:%f,patience:%d'%(epoch,running_corrects.value,running_loss.value,lr,patience))       \n",
    "        lx,px=utils.predict(model,val_dataLoader)\n",
    "        log_loss = criterion(Variable(px), Variable(lx))\n",
    "        log_loss = log_loss.item()\n",
    "        _, preds = torch.max(px, dim=1)\n",
    "        accuracy = torch.mean((preds == lx).float())\n",
    "        writer.add_scalar('Val/Acc',accuracy,(epoch+1) * len(train_dataset)/BATCH_SIZE)\n",
    "        writer.add_scalar('Val/Loss',log_loss,(epoch+1) * len(train_dataset)/BATCH_SIZE)\n",
    "        print('[epoch:%d]: val_loss:%f,val_acc:%f,'%(epoch,log_loss,accuracy))\n",
    "        if  log_loss < min_loss:\n",
    "            utils.snapshot('../model/', 'InceptionV4', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy })          \n",
    "            patience = 0\n",
    "            min_loss=log_loss\n",
    "            print('save new model loss,now loss is ',min_loss)\n",
    "        else:\n",
    "            patience += 1\n",
    "        if accuracy>min_acc:\n",
    "            utils.snapshot('../model/', 'InceptionV4', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy },key='acc') \n",
    "            min_acc=accuracy\n",
    "            print('save new model acc,now acc is ',min_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] loading model... Done\n",
      "min_loss is :4.100000\n",
      "Epoch 0/59\n",
      "----------\n",
      "[epoch:0,batch:29]:acc: 0.101042,loss:3.930136\n",
      "[epoch:0,batch:59]:acc: 0.210417,loss:3.641856\n",
      "[epoch:0,batch:89]:acc: 0.283333,loss:3.404907\n",
      "[epoch:0,batch:119]:acc: 0.324219,loss:3.216955\n",
      "[epoch:0,batch:149]:acc: 0.354583,loss:3.060818\n",
      "[epoch:0,batch:179]:acc: 0.383333,loss:2.923591\n",
      "[epoch:0,batch:209]:acc: 0.407292,loss:2.810456\n",
      "[epoch:0,batch:239]:acc: 0.422135,loss:2.709611\n",
      "[epoch:0,batch:269]:acc: 0.437384,loss:2.612714\n",
      "[epoch:0,batch:299]:acc: 0.449271,loss:2.538002\n",
      "[epoch:0,batch:299]: val_loss:1.775880,val_acc:0.581406,val_total:4539\n",
      "[epoch:0,batch:329]:acc: 0.462595,loss:2.461736\n",
      "[epoch:0,batch:359]:acc: 0.472656,loss:2.395933\n",
      "[epoch:0,batch:389]:acc: 0.484215,loss:2.337884\n",
      "[epoch:0,batch:419]:acc: 0.492336,loss:2.285381\n",
      "[epoch:0,batch:449]:acc: 0.499583,loss:2.237738\n",
      "[epoch:0,batch:479]:acc: 0.505924,loss:2.191929\n",
      "[epoch:0,batch:509]:acc: 0.512929,loss:2.153266\n",
      "[epoch:0,batch:539]:acc: 0.519965,loss:2.111832\n",
      "[epoch:0,batch:569]:acc: 0.524671,loss:2.077591\n",
      "[epoch:0,batch:599]:acc: 0.529271,loss:2.050164\n",
      "[epoch:0,batch:599]: val_loss:1.504572,val_acc:0.628773,val_total:4539\n",
      "[epoch:0,batch:629]:acc: 0.532887,loss:2.022593\n",
      "[epoch:0,batch:659]:acc: 0.537879,loss:1.990220\n",
      "[epoch:0,batch:689]:acc: 0.541531,loss:1.965090\n",
      "[epoch:0,batch:719]:acc: 0.545660,loss:1.938989\n",
      "[epoch:0,batch:749]:acc: 0.551000,loss:1.912061\n",
      "[epoch:0,batch:779]:acc: 0.554567,loss:1.886801\n",
      "[epoch:0,batch:809]:acc: 0.557948,loss:1.864306\n",
      "[epoch:0,batch:839]:acc: 0.561533,loss:1.842141\n",
      "[epoch:0,batch:869]:acc: 0.564511,loss:1.822420\n",
      "[epoch:0,batch:899]:acc: 0.567257,loss:1.803547\n",
      "[epoch:0,batch:899]: val_loss:1.339573,val_acc:0.637806,val_total:4539\n",
      "[epoch:0,batch:929]:acc: 0.570363,loss:1.784207\n",
      "[epoch:0,batch:959]:acc: 0.573210,loss:1.766021\n",
      "[epoch:0,batch:989]:acc: 0.575726,loss:1.749588\n",
      "[epoch:0] :acc: 0.575780,loss:1.749865,lr:0.001000,patience:0\n",
      "[epoch:0]: val_loss:1.140749,val_acc:0.690901,\n",
      "save new model loss,now loss is  1.1407486200332642\n",
      "Epoch 1/59\n",
      "----------\n",
      "[epoch:1,batch:29]:acc: 0.664583,loss:1.307504\n",
      "[epoch:1,batch:59]:acc: 0.657292,loss:1.302175\n",
      "[epoch:1,batch:89]:acc: 0.666319,loss:1.246951\n",
      "[epoch:1,batch:119]:acc: 0.664062,loss:1.232231\n",
      "[epoch:1,batch:149]:acc: 0.657708,loss:1.223729\n",
      "[epoch:1,batch:179]:acc: 0.660590,loss:1.203808\n",
      "[epoch:1,batch:209]:acc: 0.661756,loss:1.193928\n",
      "[epoch:1,batch:239]:acc: 0.662500,loss:1.189768\n",
      "[epoch:1,batch:269]:acc: 0.662731,loss:1.184275\n",
      "[epoch:1,batch:299]:acc: 0.662188,loss:1.173606\n",
      "[epoch:1,batch:299]: val_loss:1.096239,val_acc:0.688478,val_total:4539\n",
      "[epoch:1,batch:329]:acc: 0.662311,loss:1.173819\n",
      "[epoch:1,batch:359]:acc: 0.663802,loss:1.169009\n",
      "[epoch:1,batch:389]:acc: 0.663622,loss:1.164404\n",
      "[epoch:1,batch:419]:acc: 0.663244,loss:1.158556\n",
      "[epoch:1,batch:449]:acc: 0.663889,loss:1.153540\n",
      "[epoch:1,batch:479]:acc: 0.664583,loss:1.145197\n",
      "[epoch:1,batch:509]:acc: 0.665012,loss:1.140343\n",
      "[epoch:1,batch:539]:acc: 0.665856,loss:1.136655\n",
      "[epoch:1,batch:569]:acc: 0.666283,loss:1.133689\n",
      "[epoch:1,batch:599]:acc: 0.667396,loss:1.125080\n",
      "[epoch:1,batch:599]: val_loss:1.084819,val_acc:0.686715,val_total:4539\n",
      "[epoch:1,batch:629]:acc: 0.667510,loss:1.123709\n",
      "[epoch:1,batch:659]:acc: 0.668324,loss:1.119781\n",
      "[epoch:1,batch:689]:acc: 0.668388,loss:1.116395\n",
      "[epoch:1,batch:719]:acc: 0.668533,loss:1.113739\n",
      "[epoch:1,batch:749]:acc: 0.669333,loss:1.109467\n",
      "[epoch:1,batch:779]:acc: 0.670793,loss:1.104647\n",
      "[epoch:1,batch:809]:acc: 0.671875,loss:1.101907\n",
      "[epoch:1,batch:839]:acc: 0.672768,loss:1.097015\n",
      "[epoch:1,batch:869]:acc: 0.674282,loss:1.093874\n",
      "[epoch:1,batch:899]:acc: 0.675139,loss:1.092344\n",
      "[epoch:1,batch:899]: val_loss:1.129485,val_acc:0.668429,val_total:4539\n",
      "[epoch:1,batch:929]:acc: 0.675437,loss:1.090790\n",
      "[epoch:1,batch:959]:acc: 0.675293,loss:1.088681\n",
      "[epoch:1,batch:989]:acc: 0.675410,loss:1.086435\n",
      "[epoch:1] :acc: 0.675442,loss:1.087348,lr:0.001000,patience:0\n",
      "[epoch:1]: val_loss:1.051156,val_acc:0.693545,\n",
      "save new model loss,now loss is  1.0511562824249268\n",
      "Epoch 2/59\n",
      "----------\n",
      "[epoch:2,batch:29]:acc: 0.701042,loss:1.099184\n",
      "[epoch:2,batch:59]:acc: 0.690625,loss:1.081524\n",
      "[epoch:2,batch:89]:acc: 0.696528,loss:1.030701\n",
      "[epoch:2,batch:119]:acc: 0.692448,loss:1.021353\n",
      "[epoch:2,batch:149]:acc: 0.693750,loss:1.006987\n",
      "[epoch:2,batch:179]:acc: 0.693750,loss:1.003896\n",
      "[epoch:2,batch:209]:acc: 0.696726,loss:0.994592\n",
      "[epoch:2,batch:239]:acc: 0.695703,loss:0.998447\n",
      "[epoch:2,batch:269]:acc: 0.697338,loss:0.991042\n",
      "[epoch:2,batch:299]:acc: 0.697187,loss:0.987068\n",
      "[epoch:2,batch:299]: val_loss:1.093206,val_acc:0.687156,val_total:4539\n",
      "[epoch:2,batch:329]:acc: 0.696686,loss:0.985101\n",
      "[epoch:2,batch:359]:acc: 0.696615,loss:0.977390\n",
      "[epoch:2,batch:389]:acc: 0.696474,loss:0.979805\n",
      "[epoch:2,batch:419]:acc: 0.696801,loss:0.978227\n",
      "[epoch:2,batch:449]:acc: 0.696597,loss:0.978656\n",
      "[epoch:2,batch:479]:acc: 0.696680,loss:0.976908\n",
      "[epoch:2,batch:509]:acc: 0.698039,loss:0.974598\n",
      "[epoch:2,batch:539]:acc: 0.699248,loss:0.971742\n",
      "[epoch:2,batch:569]:acc: 0.699507,loss:0.970281\n",
      "[epoch:2,batch:599]:acc: 0.698802,loss:0.971161\n",
      "[epoch:2,batch:599]: val_loss:1.097673,val_acc:0.675920,val_total:4539\n",
      "[epoch:2,batch:629]:acc: 0.699554,loss:0.966843\n",
      "[epoch:2,batch:659]:acc: 0.700047,loss:0.965424\n",
      "[epoch:2,batch:689]:acc: 0.699819,loss:0.964850\n",
      "[epoch:2,batch:719]:acc: 0.700174,loss:0.963831\n",
      "[epoch:2,batch:749]:acc: 0.699250,loss:0.962379\n",
      "[epoch:2,batch:779]:acc: 0.699359,loss:0.961789\n",
      "[epoch:2,batch:809]:acc: 0.699769,loss:0.961629\n",
      "[epoch:2,batch:839]:acc: 0.699479,loss:0.959676\n",
      "[epoch:2,batch:869]:acc: 0.698922,loss:0.958115\n",
      "[epoch:2,batch:899]:acc: 0.698646,loss:0.958923\n",
      "[epoch:2,batch:899]: val_loss:0.983817,val_acc:0.700595,val_total:4539\n",
      "[epoch:2,batch:929]:acc: 0.698824,loss:0.958939\n",
      "[epoch:2,batch:959]:acc: 0.699219,loss:0.956415\n",
      "[epoch:2,batch:989]:acc: 0.698737,loss:0.958043\n",
      "[epoch:2] :acc: 0.698553,loss:0.959988,lr:0.001000,patience:0\n",
      "[epoch:2]: val_loss:1.037966,val_acc:0.700815,\n",
      "save new model loss,now loss is  1.0379655361175537\n",
      "Epoch 3/59\n",
      "----------\n",
      "set lr=:0.000060,momentum=0.950000\n",
      "[epoch:3,batch:29]:acc: 0.717708,loss:0.895722\n",
      "[epoch:3,batch:59]:acc: 0.727604,loss:0.849220\n",
      "[epoch:3,batch:89]:acc: 0.726736,loss:0.848466\n",
      "[epoch:3,batch:119]:acc: 0.728125,loss:0.852466\n",
      "[epoch:3,batch:149]:acc: 0.729375,loss:0.848089\n",
      "[epoch:3,batch:179]:acc: 0.730382,loss:0.855786\n",
      "[epoch:3,batch:209]:acc: 0.726190,loss:0.854680\n",
      "[epoch:3,batch:239]:acc: 0.730208,loss:0.846824\n",
      "[epoch:3,batch:269]:acc: 0.728588,loss:0.843932\n",
      "[epoch:3,batch:299]:acc: 0.730729,loss:0.842077\n",
      "[epoch:3,batch:299]: val_loss:0.940957,val_acc:0.725490,val_total:4539\n",
      "[epoch:3,batch:329]:acc: 0.731061,loss:0.842022\n",
      "[epoch:3,batch:359]:acc: 0.732118,loss:0.837235\n",
      "[epoch:3,batch:389]:acc: 0.733093,loss:0.833589\n",
      "[epoch:3,batch:419]:acc: 0.732887,loss:0.832523\n",
      "[epoch:3,batch:449]:acc: 0.733472,loss:0.830151\n",
      "[epoch:3,batch:479]:acc: 0.734440,loss:0.826060\n",
      "[epoch:3,batch:509]:acc: 0.734926,loss:0.826110\n",
      "[epoch:3,batch:539]:acc: 0.734722,loss:0.827541\n",
      "[epoch:3,batch:569]:acc: 0.735088,loss:0.827216\n",
      "[epoch:3,batch:599]:acc: 0.734583,loss:0.826012\n",
      "[epoch:3,batch:599]: val_loss:0.990678,val_acc:0.719762,val_total:4539\n",
      "[epoch:3,batch:629]:acc: 0.735268,loss:0.825251\n",
      "[epoch:3,batch:659]:acc: 0.736411,loss:0.823119\n",
      "[epoch:3,batch:689]:acc: 0.736730,loss:0.823495\n",
      "[epoch:3,batch:719]:acc: 0.737153,loss:0.823689\n",
      "[epoch:3,batch:749]:acc: 0.737333,loss:0.823437\n",
      "[epoch:3,batch:779]:acc: 0.736979,loss:0.822753\n",
      "[epoch:3,batch:809]:acc: 0.737770,loss:0.820925\n",
      "[epoch:3,batch:839]:acc: 0.738542,loss:0.819328\n",
      "[epoch:3,batch:869]:acc: 0.739152,loss:0.817655\n",
      "[epoch:3,batch:899]:acc: 0.738715,loss:0.819999\n",
      "[epoch:3,batch:899]: val_loss:0.926699,val_acc:0.729015,val_total:4539\n",
      "[epoch:3,batch:929]:acc: 0.738441,loss:0.821512\n",
      "[epoch:3,batch:959]:acc: 0.737891,loss:0.822087\n",
      "[epoch:3,batch:989]:acc: 0.737595,loss:0.822950\n",
      "[epoch:3] :acc: 0.737554,loss:0.824668,lr:0.000060,patience:0\n",
      "[epoch:3]: val_loss:1.034935,val_acc:0.725931,\n",
      "save new model loss,now loss is  1.0349349975585938\n",
      "Epoch 4/59\n",
      "----------\n",
      "set lr=:0.000060,momentum=0.950000\n",
      "[epoch:4,batch:29]:acc: 0.752083,loss:0.804570\n",
      "[epoch:4,batch:59]:acc: 0.743229,loss:0.836560\n",
      "[epoch:4,batch:89]:acc: 0.743403,loss:0.837398\n",
      "[epoch:4,batch:119]:acc: 0.742448,loss:0.839119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:4,batch:149]:acc: 0.740833,loss:0.833128\n",
      "[epoch:4,batch:179]:acc: 0.740451,loss:0.830079\n",
      "[epoch:4,batch:209]:acc: 0.740774,loss:0.824087\n",
      "[epoch:4,batch:239]:acc: 0.736328,loss:0.835119\n",
      "[epoch:4,batch:269]:acc: 0.738426,loss:0.829073\n",
      "[epoch:4,batch:299]:acc: 0.739375,loss:0.826022\n",
      "[epoch:4,batch:299]: val_loss:0.878065,val_acc:0.735184,val_total:4539\n",
      "[epoch:4,batch:329]:acc: 0.737311,loss:0.826897\n",
      "[epoch:4,batch:359]:acc: 0.737413,loss:0.828679\n",
      "[epoch:4,batch:389]:acc: 0.735737,loss:0.827704\n",
      "[epoch:4,batch:419]:acc: 0.736384,loss:0.829635\n",
      "[epoch:4,batch:449]:acc: 0.739236,loss:0.823240\n",
      "[epoch:4,batch:479]:acc: 0.739453,loss:0.821778\n",
      "[epoch:4,batch:509]:acc: 0.739583,loss:0.819113\n",
      "[epoch:4,batch:539]:acc: 0.739988,loss:0.817136\n",
      "[epoch:4,batch:569]:acc: 0.740844,loss:0.816666\n",
      "[epoch:4,batch:599]:acc: 0.739896,loss:0.816102\n",
      "[epoch:4,batch:599]: val_loss:0.870882,val_acc:0.740251,val_total:4539\n",
      "[epoch:4,batch:629]:acc: 0.741171,loss:0.813789\n",
      "[epoch:4,batch:659]:acc: 0.740767,loss:0.813557\n",
      "[epoch:4,batch:689]:acc: 0.740127,loss:0.816449\n",
      "[epoch:4,batch:719]:acc: 0.739366,loss:0.817999\n",
      "[epoch:4,batch:749]:acc: 0.739792,loss:0.817928\n",
      "[epoch:4,batch:779]:acc: 0.740425,loss:0.815545\n",
      "[epoch:4,batch:809]:acc: 0.740741,loss:0.814582\n",
      "[epoch:4,batch:839]:acc: 0.740513,loss:0.815514\n",
      "[epoch:4,batch:869]:acc: 0.739691,loss:0.817841\n",
      "[epoch:4,batch:899]:acc: 0.739618,loss:0.817602\n",
      "[epoch:4,batch:899]: val_loss:0.890597,val_acc:0.737387,val_total:4539\n",
      "[epoch:4,batch:929]:acc: 0.738844,loss:0.816763\n",
      "[epoch:4,batch:959]:acc: 0.738867,loss:0.816478\n",
      "[epoch:4,batch:989]:acc: 0.738984,loss:0.816514\n",
      "[epoch:4] :acc: 0.739036,loss:0.817728,lr:0.000060,patience:0\n",
      "[epoch:4]: val_loss:0.942196,val_acc:0.722406,\n",
      "save new model loss,now loss is  0.9421961307525635\n",
      "Epoch 5/59\n",
      "----------\n",
      "set lr=:0.000060,momentum=0.950000\n",
      "[epoch:5,batch:29]:acc: 0.700000,loss:0.874827\n",
      "[epoch:5,batch:59]:acc: 0.734375,loss:0.828895\n",
      "[epoch:5,batch:89]:acc: 0.733681,loss:0.832370\n",
      "[epoch:5,batch:119]:acc: 0.734896,loss:0.840145\n",
      "[epoch:5,batch:149]:acc: 0.737292,loss:0.827892\n",
      "[epoch:5,batch:179]:acc: 0.741493,loss:0.821124\n",
      "[epoch:5,batch:209]:acc: 0.740625,loss:0.825799\n",
      "[epoch:5,batch:239]:acc: 0.742839,loss:0.821097\n",
      "[epoch:5,batch:269]:acc: 0.742593,loss:0.816221\n",
      "[epoch:5,batch:299]:acc: 0.742396,loss:0.809499\n",
      "[epoch:5,batch:299]: val_loss:0.854811,val_acc:0.742454,val_total:4539\n",
      "[epoch:5,batch:329]:acc: 0.741856,loss:0.812060\n",
      "[epoch:5,batch:359]:acc: 0.743056,loss:0.809560\n",
      "[epoch:5,batch:389]:acc: 0.743590,loss:0.806836\n",
      "[epoch:5,batch:419]:acc: 0.742783,loss:0.810419\n",
      "[epoch:5,batch:449]:acc: 0.742083,loss:0.811889\n",
      "[epoch:5,batch:479]:acc: 0.743685,loss:0.810657\n",
      "[epoch:5,batch:509]:acc: 0.744240,loss:0.807936\n",
      "[epoch:5,batch:539]:acc: 0.743981,loss:0.808283\n",
      "[epoch:5,batch:569]:acc: 0.743147,loss:0.812452\n",
      "[epoch:5,batch:599]:acc: 0.743073,loss:0.811626\n",
      "[epoch:5,batch:599]: val_loss:1.047827,val_acc:0.712932,val_total:4539\n",
      "[epoch:5,batch:629]:acc: 0.743502,loss:0.810059\n",
      "[epoch:5,batch:659]:acc: 0.743182,loss:0.809571\n",
      "[epoch:5,batch:689]:acc: 0.743659,loss:0.806798\n",
      "[epoch:5,batch:719]:acc: 0.744358,loss:0.805467\n",
      "[epoch:5,batch:749]:acc: 0.744375,loss:0.804905\n",
      "[epoch:5,batch:779]:acc: 0.744030,loss:0.804270\n",
      "[epoch:5,batch:809]:acc: 0.743711,loss:0.804367\n",
      "[epoch:5,batch:839]:acc: 0.743824,loss:0.803120\n",
      "[epoch:5,batch:869]:acc: 0.744109,loss:0.803136\n",
      "[epoch:5,batch:899]:acc: 0.743889,loss:0.804364\n",
      "[epoch:5,batch:899]: val_loss:0.882844,val_acc:0.738048,val_total:4539\n",
      "[epoch:5,batch:929]:acc: 0.744422,loss:0.803659\n",
      "[epoch:5,batch:959]:acc: 0.744173,loss:0.804231\n",
      "[epoch:5,batch:989]:acc: 0.744792,loss:0.803518\n",
      "[epoch:5] :acc: 0.744806,loss:0.803171,lr:0.000060,patience:0\n",
      "[epoch:5]: val_loss:0.963832,val_acc:0.728795,\n",
      "Epoch 6/59\n",
      "----------\n",
      "set lr=:0.000100,momentum=0.900000\n",
      "[epoch:6,batch:29]:acc: 0.771875,loss:0.719862\n",
      "[epoch:6,batch:59]:acc: 0.755208,loss:0.802008\n",
      "[epoch:6,batch:89]:acc: 0.751736,loss:0.797241\n",
      "[epoch:6,batch:119]:acc: 0.743750,loss:0.818286\n",
      "[epoch:6,batch:149]:acc: 0.749167,loss:0.810336\n",
      "[epoch:6,batch:179]:acc: 0.750521,loss:0.807292\n",
      "[epoch:6,batch:209]:acc: 0.749107,loss:0.807938\n",
      "[epoch:6,batch:239]:acc: 0.750781,loss:0.803282\n",
      "[epoch:6,batch:269]:acc: 0.749884,loss:0.804648\n",
      "[epoch:6,batch:299]:acc: 0.747604,loss:0.806887\n",
      "[epoch:6,batch:299]: val_loss:0.882674,val_acc:0.735404,val_total:4539\n",
      "[epoch:6,batch:329]:acc: 0.746780,loss:0.808220\n",
      "[epoch:6,batch:359]:acc: 0.743490,loss:0.813244\n",
      "[epoch:6,batch:389]:acc: 0.742708,loss:0.817882\n",
      "[epoch:6,batch:419]:acc: 0.741815,loss:0.819033\n",
      "[epoch:6,batch:449]:acc: 0.742500,loss:0.814594\n",
      "[epoch:6,batch:479]:acc: 0.743164,loss:0.814008\n",
      "[epoch:6,batch:509]:acc: 0.742831,loss:0.812076\n",
      "[epoch:6,batch:539]:acc: 0.742535,loss:0.813480\n",
      "[epoch:6,batch:569]:acc: 0.742434,loss:0.814783\n",
      "[epoch:6,batch:599]:acc: 0.743021,loss:0.812941\n",
      "[epoch:6,batch:599]: val_loss:0.895332,val_acc:0.732540,val_total:4539\n",
      "[epoch:6,batch:629]:acc: 0.742113,loss:0.816055\n",
      "[epoch:6,batch:659]:acc: 0.741619,loss:0.818851\n",
      "[epoch:6,batch:689]:acc: 0.740942,loss:0.820125\n",
      "[epoch:6,batch:719]:acc: 0.740191,loss:0.819661\n",
      "[epoch:6,batch:749]:acc: 0.740833,loss:0.816789\n",
      "[epoch:6,batch:779]:acc: 0.741987,loss:0.811931\n",
      "[epoch:6,batch:809]:acc: 0.742130,loss:0.812530\n",
      "[epoch:6,batch:839]:acc: 0.741815,loss:0.812709\n",
      "[epoch:6,batch:869]:acc: 0.741810,loss:0.813386\n",
      "[epoch:6,batch:899]:acc: 0.741042,loss:0.814584\n",
      "[epoch:6,batch:899]: val_loss:0.847594,val_acc:0.744878,val_total:4539\n",
      "[epoch:6,batch:929]:acc: 0.741331,loss:0.813748\n",
      "[epoch:6,batch:959]:acc: 0.741569,loss:0.812739\n",
      "[epoch:6,batch:989]:acc: 0.741572,loss:0.813311\n",
      "[epoch:6] :acc: 0.741495,loss:0.813916,lr:0.000100,patience:1\n",
      "[epoch:6]: val_loss:1.023278,val_acc:0.725050,\n",
      "Epoch 7/59\n",
      "----------\n",
      "loss has increased lr divide 10 lr now is :0.000010\n",
      "[epoch:7,batch:29]:acc: 0.736458,loss:0.801983\n",
      "[epoch:7,batch:59]:acc: 0.728646,loss:0.818015\n",
      "[epoch:7,batch:89]:acc: 0.733333,loss:0.813192\n",
      "[epoch:7,batch:119]:acc: 0.735156,loss:0.815330\n",
      "[epoch:7,batch:149]:acc: 0.740625,loss:0.807140\n",
      "[epoch:7,batch:179]:acc: 0.741667,loss:0.795582\n",
      "[epoch:7,batch:209]:acc: 0.741518,loss:0.795656\n",
      "[epoch:7,batch:239]:acc: 0.742318,loss:0.798668\n",
      "[epoch:7,batch:269]:acc: 0.741898,loss:0.805453\n",
      "[epoch:7,batch:299]:acc: 0.743854,loss:0.803160\n",
      "[epoch:7,batch:299]: val_loss:0.879089,val_acc:0.736726,val_total:4539\n",
      "[epoch:7,batch:329]:acc: 0.744602,loss:0.800997\n",
      "[epoch:7,batch:359]:acc: 0.743403,loss:0.802354\n",
      "[epoch:7,batch:389]:acc: 0.744471,loss:0.797539\n",
      "[epoch:7,batch:419]:acc: 0.744643,loss:0.798427\n",
      "[epoch:7,batch:449]:acc: 0.744861,loss:0.796472\n",
      "[epoch:7,batch:479]:acc: 0.746094,loss:0.794222\n",
      "[epoch:7,batch:509]:acc: 0.746752,loss:0.795593\n",
      "[epoch:7,batch:539]:acc: 0.746701,loss:0.796630\n",
      "[epoch:7,batch:569]:acc: 0.747149,loss:0.798160\n",
      "[epoch:7,batch:599]:acc: 0.746406,loss:0.799542\n",
      "[epoch:7,batch:599]: val_loss:0.908957,val_acc:0.736506,val_total:4539\n",
      "[epoch:7,batch:629]:acc: 0.746032,loss:0.798758\n",
      "[epoch:7,batch:659]:acc: 0.746023,loss:0.798847\n",
      "[epoch:7,batch:689]:acc: 0.744384,loss:0.804389\n",
      "[epoch:7,batch:719]:acc: 0.744227,loss:0.805337\n",
      "[epoch:7,batch:749]:acc: 0.744583,loss:0.804075\n",
      "[epoch:7,batch:779]:acc: 0.743429,loss:0.804537\n",
      "[epoch:7,batch:809]:acc: 0.743480,loss:0.803149\n",
      "[epoch:7,batch:839]:acc: 0.742857,loss:0.804443\n",
      "[epoch:7,batch:869]:acc: 0.742780,loss:0.804388\n",
      "[epoch:7,batch:899]:acc: 0.742743,loss:0.802672\n",
      "[epoch:7,batch:899]: val_loss:0.869486,val_acc:0.736946,val_total:4539\n",
      "[epoch:7,batch:929]:acc: 0.743784,loss:0.799283\n",
      "[epoch:7,batch:959]:acc: 0.743913,loss:0.800519\n",
      "[epoch:7,batch:989]:acc: 0.743434,loss:0.800140\n",
      "[epoch:7] :acc: 0.743292,loss:0.800206,lr:0.000010,patience:0\n",
      "[epoch:7]: val_loss:0.941531,val_acc:0.726812,\n",
      "save new model loss,now loss is  0.9415309429168701\n",
      "Epoch 8/59\n",
      "----------\n",
      "[epoch:8,batch:29]:acc: 0.768750,loss:0.737950\n",
      "[epoch:8,batch:59]:acc: 0.753646,loss:0.780415\n",
      "[epoch:8,batch:89]:acc: 0.748958,loss:0.782215\n",
      "[epoch:8,batch:119]:acc: 0.747396,loss:0.779584\n",
      "[epoch:8,batch:149]:acc: 0.744583,loss:0.785811\n",
      "[epoch:8,batch:179]:acc: 0.748785,loss:0.780276\n",
      "[epoch:8,batch:209]:acc: 0.745685,loss:0.784119\n",
      "[epoch:8,batch:239]:acc: 0.746354,loss:0.787170\n",
      "[epoch:8,batch:269]:acc: 0.746875,loss:0.786506\n",
      "[epoch:8,batch:299]:acc: 0.749583,loss:0.782464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:8,batch:299]: val_loss:0.954132,val_acc:0.723287,val_total:4539\n",
      "[epoch:8,batch:329]:acc: 0.746307,loss:0.788703\n",
      "[epoch:8,batch:359]:acc: 0.746267,loss:0.791374\n",
      "[epoch:8,batch:389]:acc: 0.746955,loss:0.793379\n",
      "[epoch:8,batch:419]:acc: 0.744866,loss:0.798215\n",
      "[epoch:8,batch:449]:acc: 0.743681,loss:0.800307\n",
      "[epoch:8,batch:479]:acc: 0.743424,loss:0.798570\n",
      "[epoch:8,batch:509]:acc: 0.743627,loss:0.798411\n",
      "[epoch:8,batch:539]:acc: 0.742477,loss:0.800734\n",
      "[epoch:8,batch:569]:acc: 0.742379,loss:0.801829\n",
      "[epoch:8,batch:599]:acc: 0.742031,loss:0.803142\n",
      "[epoch:8,batch:599]: val_loss:0.903363,val_acc:0.738268,val_total:4539\n",
      "[epoch:8,batch:629]:acc: 0.741766,loss:0.806969\n",
      "[epoch:8,batch:659]:acc: 0.741714,loss:0.807631\n",
      "[epoch:8,batch:689]:acc: 0.741078,loss:0.809138\n",
      "[epoch:8,batch:719]:acc: 0.741276,loss:0.806826\n",
      "[epoch:8,batch:749]:acc: 0.742083,loss:0.805437\n",
      "[epoch:8,batch:779]:acc: 0.742548,loss:0.805047\n",
      "[epoch:8,batch:809]:acc: 0.743210,loss:0.803344\n",
      "[epoch:8,batch:839]:acc: 0.743564,loss:0.802841\n",
      "[epoch:8,batch:869]:acc: 0.743642,loss:0.802385\n",
      "[epoch:8,batch:899]:acc: 0.743437,loss:0.804136\n",
      "[epoch:8,batch:899]: val_loss:0.876597,val_acc:0.739590,val_total:4539\n",
      "[epoch:8,batch:929]:acc: 0.743784,loss:0.804149\n",
      "[epoch:8,batch:959]:acc: 0.743750,loss:0.804728\n",
      "[epoch:8,batch:989]:acc: 0.743434,loss:0.805677\n",
      "[epoch:8] :acc: 0.743544,loss:0.807467,lr:0.000010,patience:0\n",
      "[epoch:8]: val_loss:0.971678,val_acc:0.716898,\n",
      "Epoch 9/59\n",
      "----------\n",
      "[epoch:9,batch:29]:acc: 0.744792,loss:0.776830\n",
      "[epoch:9,batch:59]:acc: 0.756250,loss:0.785203\n",
      "[epoch:9,batch:89]:acc: 0.748611,loss:0.787941\n",
      "[epoch:9,batch:119]:acc: 0.747135,loss:0.788437\n",
      "[epoch:9,batch:149]:acc: 0.748333,loss:0.796352\n",
      "[epoch:9,batch:179]:acc: 0.747743,loss:0.795118\n",
      "[epoch:9,batch:209]:acc: 0.745833,loss:0.796756\n",
      "[epoch:9,batch:239]:acc: 0.743620,loss:0.798557\n",
      "[epoch:9,batch:269]:acc: 0.742824,loss:0.803114\n",
      "[epoch:9,batch:299]:acc: 0.745417,loss:0.803525\n",
      "[epoch:9,batch:299]: val_loss:0.945756,val_acc:0.723948,val_total:4539\n",
      "[epoch:9,batch:329]:acc: 0.744318,loss:0.798754\n",
      "[epoch:9,batch:359]:acc: 0.744010,loss:0.799933\n",
      "[epoch:9,batch:389]:acc: 0.742628,loss:0.802774\n",
      "[epoch:9,batch:419]:acc: 0.741369,loss:0.806186\n",
      "[epoch:9,batch:449]:acc: 0.740833,loss:0.808519\n",
      "[epoch:9,batch:479]:acc: 0.741797,loss:0.806428\n",
      "[epoch:9,batch:509]:acc: 0.741605,loss:0.808934\n",
      "[epoch:9,batch:539]:acc: 0.742361,loss:0.804428\n",
      "[epoch:9,batch:569]:acc: 0.742434,loss:0.802318\n",
      "[epoch:9,batch:599]:acc: 0.742656,loss:0.802912\n",
      "[epoch:9,batch:599]: val_loss:0.870897,val_acc:0.743336,val_total:4539\n",
      "[epoch:9,batch:629]:acc: 0.742907,loss:0.803276\n",
      "[epoch:9,batch:659]:acc: 0.742330,loss:0.803931\n",
      "[epoch:9,batch:689]:acc: 0.742527,loss:0.801600\n",
      "[epoch:9,batch:719]:acc: 0.741797,loss:0.803323\n",
      "[epoch:9,batch:749]:acc: 0.742250,loss:0.803182\n",
      "[epoch:9,batch:779]:acc: 0.741546,loss:0.805779\n",
      "[epoch:9,batch:809]:acc: 0.741628,loss:0.805758\n",
      "[epoch:9,batch:839]:acc: 0.741927,loss:0.806057\n",
      "[epoch:9,batch:869]:acc: 0.741990,loss:0.806254\n",
      "[epoch:9,batch:899]:acc: 0.742535,loss:0.805979\n",
      "[epoch:9,batch:899]: val_loss:0.904131,val_acc:0.732320,val_total:4539\n",
      "[epoch:9,batch:929]:acc: 0.741969,loss:0.806740\n",
      "[epoch:9,batch:959]:acc: 0.742318,loss:0.807429\n",
      "[epoch:9,batch:989]:acc: 0.743718,loss:0.805383\n",
      "[epoch:9] :acc: 0.743671,loss:0.805589,lr:0.000010,patience:1\n",
      "[epoch:9]: val_loss:0.949502,val_acc:0.729896,\n",
      "Epoch 10/59\n",
      "----------\n",
      "loss has increased lr divide 10 lr now is :0.000001\n",
      "[epoch:10,batch:29]:acc: 0.750000,loss:0.803489\n",
      "[epoch:10,batch:59]:acc: 0.747917,loss:0.782880\n",
      "[epoch:10,batch:89]:acc: 0.750000,loss:0.784131\n",
      "[epoch:10,batch:119]:acc: 0.748177,loss:0.786984\n",
      "[epoch:10,batch:149]:acc: 0.751667,loss:0.791214\n",
      "[epoch:10,batch:179]:acc: 0.748958,loss:0.797224\n",
      "[epoch:10,batch:209]:acc: 0.749851,loss:0.798098\n",
      "[epoch:10,batch:239]:acc: 0.750911,loss:0.789894\n",
      "[epoch:10,batch:269]:acc: 0.751042,loss:0.790957\n",
      "[epoch:10,batch:299]:acc: 0.750417,loss:0.793477\n",
      "[epoch:10,batch:299]: val_loss:0.902980,val_acc:0.735625,val_total:4539\n",
      "[epoch:10,batch:329]:acc: 0.750473,loss:0.795644\n",
      "[epoch:10,batch:359]:acc: 0.749740,loss:0.799418\n",
      "[epoch:10,batch:389]:acc: 0.749760,loss:0.797833\n",
      "[epoch:10,batch:419]:acc: 0.749479,loss:0.799243\n",
      "[epoch:10,batch:449]:acc: 0.748472,loss:0.800663\n",
      "[epoch:10,batch:479]:acc: 0.747396,loss:0.802605\n",
      "[epoch:10,batch:509]:acc: 0.748100,loss:0.799802\n",
      "[epoch:10,batch:539]:acc: 0.748553,loss:0.800505\n",
      "[epoch:10,batch:569]:acc: 0.748081,loss:0.801114\n",
      "[epoch:10,batch:599]:acc: 0.747552,loss:0.801943\n",
      "[epoch:10,batch:599]: val_loss:0.924167,val_acc:0.734082,val_total:4539\n",
      "[epoch:10,batch:629]:acc: 0.746677,loss:0.805977\n",
      "[epoch:10,batch:659]:acc: 0.746970,loss:0.804051\n",
      "[epoch:10,batch:689]:acc: 0.747509,loss:0.803672\n",
      "[epoch:10,batch:719]:acc: 0.747135,loss:0.802218\n",
      "[epoch:10,batch:749]:acc: 0.747000,loss:0.803255\n",
      "[epoch:10,batch:779]:acc: 0.745954,loss:0.804178\n",
      "[epoch:10,batch:809]:acc: 0.745139,loss:0.804789\n",
      "[epoch:10,batch:839]:acc: 0.744196,loss:0.804647\n",
      "[epoch:10,batch:869]:acc: 0.744576,loss:0.803767\n",
      "[epoch:10,batch:899]:acc: 0.745208,loss:0.802297\n",
      "[epoch:10,batch:899]: val_loss:0.900262,val_acc:0.732540,val_total:4539\n",
      "[epoch:10,batch:929]:acc: 0.745128,loss:0.803822\n",
      "[epoch:10,batch:959]:acc: 0.745280,loss:0.803403\n",
      "[epoch:10,batch:989]:acc: 0.745423,loss:0.803285\n",
      "[epoch:10] :acc: 0.745405,loss:0.803377,lr:0.000001,patience:0\n",
      "[epoch:10]: val_loss:1.018666,val_acc:0.721525,\n",
      "Epoch 11/59\n",
      "----------\n",
      "[epoch:11,batch:29]:acc: 0.745833,loss:0.810339\n",
      "[epoch:11,batch:59]:acc: 0.743750,loss:0.794598\n",
      "[epoch:11,batch:89]:acc: 0.739583,loss:0.813713\n",
      "[epoch:11,batch:119]:acc: 0.739323,loss:0.808574\n",
      "[epoch:11,batch:149]:acc: 0.746875,loss:0.803205\n",
      "[epoch:11,batch:179]:acc: 0.744444,loss:0.802466\n",
      "[epoch:11,batch:209]:acc: 0.745685,loss:0.808560\n",
      "[epoch:11,batch:239]:acc: 0.747526,loss:0.804995\n",
      "[epoch:11,batch:269]:acc: 0.747454,loss:0.804942\n",
      "[epoch:11,batch:299]:acc: 0.746771,loss:0.810896\n",
      "[epoch:11,batch:299]: val_loss:0.928940,val_acc:0.727473,val_total:4539\n",
      "[epoch:11,batch:329]:acc: 0.745265,loss:0.812906\n",
      "[epoch:11,batch:359]:acc: 0.744965,loss:0.812667\n",
      "[epoch:11,batch:389]:acc: 0.745433,loss:0.810017\n",
      "[epoch:11,batch:419]:acc: 0.745908,loss:0.805178\n",
      "[epoch:11,batch:449]:acc: 0.745278,loss:0.806344\n",
      "[epoch:11,batch:479]:acc: 0.745508,loss:0.806322\n",
      "[epoch:11,batch:509]:acc: 0.744179,loss:0.807308\n",
      "[epoch:11,batch:539]:acc: 0.743866,loss:0.807095\n",
      "[epoch:11,batch:569]:acc: 0.744408,loss:0.807175\n",
      "[epoch:11,batch:599]:acc: 0.744635,loss:0.806213\n",
      "[epoch:11,batch:599]: val_loss:0.888056,val_acc:0.736946,val_total:4539\n",
      "[epoch:11,batch:629]:acc: 0.744544,loss:0.807235\n",
      "[epoch:11,batch:659]:acc: 0.744271,loss:0.807488\n",
      "[epoch:11,batch:689]:acc: 0.744973,loss:0.805262\n",
      "[epoch:11,batch:719]:acc: 0.744184,loss:0.805615\n",
      "[epoch:11,batch:749]:acc: 0.744417,loss:0.804601\n",
      "[epoch:11,batch:779]:acc: 0.743830,loss:0.806316\n",
      "[epoch:11,batch:809]:acc: 0.744599,loss:0.803031\n",
      "[epoch:11,batch:839]:acc: 0.744903,loss:0.801258\n",
      "[epoch:11,batch:869]:acc: 0.745402,loss:0.800263\n",
      "[epoch:11,batch:899]:acc: 0.745104,loss:0.803165\n",
      "[epoch:11,batch:899]: val_loss:0.883280,val_acc:0.737828,val_total:4539\n",
      "[epoch:11,batch:929]:acc: 0.745060,loss:0.802879\n",
      "[epoch:11,batch:959]:acc: 0.745182,loss:0.802197\n",
      "[epoch:11,batch:989]:acc: 0.745360,loss:0.801448\n",
      "[epoch:11] :acc: 0.745436,loss:0.801708,lr:0.000001,patience:1\n",
      "[epoch:11]: val_loss:0.960735,val_acc:0.726812,\n",
      "Epoch 12/59\n",
      "----------\n",
      "loss has increased lr divide 10 lr now is :0.000000\n",
      "[epoch:12,batch:29]:acc: 0.746875,loss:0.790541\n",
      "[epoch:12,batch:59]:acc: 0.750000,loss:0.792577\n",
      "[epoch:12,batch:89]:acc: 0.750000,loss:0.782881\n",
      "[epoch:12,batch:119]:acc: 0.745313,loss:0.786834\n",
      "[epoch:12,batch:149]:acc: 0.744583,loss:0.795852\n",
      "[epoch:12,batch:179]:acc: 0.743229,loss:0.799049\n",
      "[epoch:12,batch:209]:acc: 0.742560,loss:0.799319\n",
      "[epoch:12,batch:239]:acc: 0.745573,loss:0.791967\n",
      "[epoch:12,batch:269]:acc: 0.745949,loss:0.789771\n",
      "[epoch:12,batch:299]:acc: 0.746875,loss:0.788818\n",
      "[epoch:12,batch:299]: val_loss:0.899702,val_acc:0.730337,val_total:4539\n",
      "[epoch:12,batch:329]:acc: 0.745739,loss:0.790603\n",
      "[epoch:12,batch:359]:acc: 0.747830,loss:0.785221\n",
      "[epoch:12,batch:389]:acc: 0.746154,loss:0.788716\n",
      "[epoch:12,batch:419]:acc: 0.747247,loss:0.788676\n",
      "[epoch:12,batch:449]:acc: 0.746736,loss:0.790927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:12,batch:479]:acc: 0.747461,loss:0.792407\n",
      "[epoch:12,batch:509]:acc: 0.748284,loss:0.790578\n",
      "[epoch:12,batch:539]:acc: 0.749479,loss:0.790896\n",
      "[epoch:12,batch:569]:acc: 0.750713,loss:0.787650\n",
      "[epoch:12,batch:599]:acc: 0.751250,loss:0.787465\n",
      "[epoch:12,batch:599]: val_loss:0.878805,val_acc:0.735845,val_total:4539\n",
      "[epoch:12,batch:629]:acc: 0.749454,loss:0.791399\n",
      "[epoch:12,batch:659]:acc: 0.749858,loss:0.789533\n",
      "[epoch:12,batch:689]:acc: 0.749864,loss:0.789864\n",
      "[epoch:12,batch:719]:acc: 0.748785,loss:0.791562\n",
      "[epoch:12,batch:749]:acc: 0.749500,loss:0.791106\n",
      "[epoch:12,batch:779]:acc: 0.749199,loss:0.790620\n",
      "[epoch:12,batch:809]:acc: 0.749151,loss:0.790228\n",
      "[epoch:12,batch:839]:acc: 0.748958,loss:0.790900\n",
      "[epoch:12,batch:869]:acc: 0.749389,loss:0.790240\n",
      "[epoch:12,batch:899]:acc: 0.749514,loss:0.790130\n",
      "[epoch:12,batch:899]: val_loss:0.876832,val_acc:0.737828,val_total:4539\n",
      "[epoch:12,batch:929]:acc: 0.749933,loss:0.789716\n",
      "[epoch:12,batch:959]:acc: 0.750033,loss:0.789248\n",
      "[epoch:12,batch:989]:acc: 0.749621,loss:0.790779\n",
      "[epoch:12] :acc: 0.749566,loss:0.790701,lr:0.000000,patience:0\n",
      "[epoch:12]: val_loss:0.891814,val_acc:0.737828,\n",
      "save new model loss,now loss is  0.8918142318725586\n",
      "Epoch 13/59\n",
      "----------\n",
      "[epoch:13,batch:29]:acc: 0.775000,loss:0.754047\n",
      "[epoch:13,batch:59]:acc: 0.763021,loss:0.760983\n",
      "[epoch:13,batch:89]:acc: 0.755208,loss:0.781616\n",
      "[epoch:13,batch:119]:acc: 0.758854,loss:0.774773\n",
      "[epoch:13,batch:149]:acc: 0.755625,loss:0.779824\n",
      "[epoch:13,batch:179]:acc: 0.757292,loss:0.770516\n",
      "[epoch:13,batch:209]:acc: 0.754315,loss:0.776233\n",
      "[epoch:13,batch:239]:acc: 0.750781,loss:0.776517\n",
      "[epoch:13,batch:269]:acc: 0.748958,loss:0.783160\n",
      "[epoch:13,batch:299]:acc: 0.748542,loss:0.783182\n",
      "[epoch:13,batch:299]: val_loss:0.841186,val_acc:0.744657,val_total:4539\n",
      "[epoch:13,batch:329]:acc: 0.748958,loss:0.781230\n",
      "[epoch:13,batch:359]:acc: 0.749132,loss:0.785045\n",
      "[epoch:13,batch:389]:acc: 0.748478,loss:0.790045\n",
      "[epoch:13,batch:419]:acc: 0.747693,loss:0.788599\n",
      "[epoch:13,batch:449]:acc: 0.747917,loss:0.791776\n",
      "[epoch:13,batch:479]:acc: 0.748633,loss:0.792314\n",
      "[epoch:13,batch:509]:acc: 0.748039,loss:0.796020\n",
      "[epoch:13,batch:539]:acc: 0.747917,loss:0.795864\n",
      "[epoch:13,batch:569]:acc: 0.747807,loss:0.796998\n",
      "[epoch:13,batch:599]:acc: 0.748542,loss:0.796580\n",
      "[epoch:13,batch:599]: val_loss:0.905476,val_acc:0.733421,val_total:4539\n",
      "[epoch:13,batch:629]:acc: 0.748264,loss:0.796844\n",
      "[epoch:13,batch:659]:acc: 0.748059,loss:0.795962\n",
      "[epoch:13,batch:689]:acc: 0.749004,loss:0.795898\n",
      "[epoch:13,batch:719]:acc: 0.747873,loss:0.797940\n",
      "[epoch:13,batch:749]:acc: 0.747667,loss:0.797502\n",
      "[epoch:13,batch:779]:acc: 0.747556,loss:0.799191\n",
      "[epoch:13,batch:809]:acc: 0.747569,loss:0.799190\n",
      "[epoch:13,batch:839]:acc: 0.747247,loss:0.799664\n",
      "[epoch:13,batch:869]:acc: 0.747378,loss:0.799020\n",
      "[epoch:13,batch:899]:acc: 0.747222,loss:0.798470\n",
      "[epoch:13,batch:899]: val_loss:0.865188,val_acc:0.739150,val_total:4539\n",
      "[epoch:13,batch:929]:acc: 0.747245,loss:0.798367\n",
      "[epoch:13,batch:959]:acc: 0.747559,loss:0.797309\n",
      "[epoch:13,batch:989]:acc: 0.747917,loss:0.795898\n",
      "[epoch:13] :acc: 0.748022,loss:0.795723,lr:0.000000,patience:0\n",
      "[epoch:13]: val_loss:1.026396,val_acc:0.720643,\n",
      "Epoch 14/59\n",
      "----------\n",
      "[epoch:14,batch:29]:acc: 0.744792,loss:0.782388\n",
      "[epoch:14,batch:59]:acc: 0.745833,loss:0.791811\n",
      "[epoch:14,batch:89]:acc: 0.747569,loss:0.791125\n",
      "[epoch:14,batch:119]:acc: 0.747656,loss:0.791336\n",
      "[epoch:14,batch:149]:acc: 0.743750,loss:0.802620\n",
      "[epoch:14,batch:179]:acc: 0.743229,loss:0.798868\n",
      "[epoch:14,batch:209]:acc: 0.742857,loss:0.797583\n",
      "[epoch:14,batch:239]:acc: 0.746224,loss:0.787433\n",
      "[epoch:14,batch:269]:acc: 0.745833,loss:0.786746\n",
      "[epoch:14,batch:299]:acc: 0.745104,loss:0.786111\n",
      "[epoch:14,batch:299]: val_loss:0.868626,val_acc:0.739811,val_total:4539\n",
      "[epoch:14,batch:329]:acc: 0.746023,loss:0.784533\n",
      "[epoch:14,batch:359]:acc: 0.748264,loss:0.781430\n",
      "[epoch:14,batch:389]:acc: 0.747276,loss:0.786230\n",
      "[epoch:14,batch:419]:acc: 0.748065,loss:0.787415\n",
      "[epoch:14,batch:449]:acc: 0.747708,loss:0.786722\n",
      "[epoch:14,batch:479]:acc: 0.747331,loss:0.786773\n",
      "[epoch:14,batch:509]:acc: 0.747304,loss:0.787581\n",
      "[epoch:14,batch:539]:acc: 0.747627,loss:0.786398\n",
      "[epoch:14,batch:569]:acc: 0.747368,loss:0.787073\n",
      "[epoch:14,batch:599]:acc: 0.747396,loss:0.788698\n",
      "[epoch:14,batch:599]: val_loss:0.897524,val_acc:0.734303,val_total:4539\n",
      "[epoch:14,batch:629]:acc: 0.748512,loss:0.787716\n",
      "[epoch:14,batch:659]:acc: 0.748201,loss:0.787868\n",
      "[epoch:14,batch:689]:acc: 0.746196,loss:0.792364\n",
      "[epoch:14,batch:719]:acc: 0.747266,loss:0.788233\n",
      "[epoch:14,batch:749]:acc: 0.747000,loss:0.787188\n",
      "[epoch:14,batch:779]:acc: 0.747035,loss:0.788448\n",
      "[epoch:14,batch:809]:acc: 0.747184,loss:0.788191\n",
      "[epoch:14,batch:839]:acc: 0.747173,loss:0.788124\n",
      "[epoch:14,batch:869]:acc: 0.746947,loss:0.788976\n",
      "[epoch:14,batch:899]:acc: 0.747153,loss:0.788919\n",
      "[epoch:14,batch:899]: val_loss:0.897385,val_acc:0.741132,val_total:4539\n",
      "[epoch:14,batch:929]:acc: 0.747413,loss:0.789321\n",
      "[epoch:14,batch:959]:acc: 0.746777,loss:0.793164\n",
      "[epoch:14,batch:989]:acc: 0.746875,loss:0.793963\n",
      "[epoch:14] :acc: 0.746792,loss:0.793878,lr:0.000000,patience:1\n",
      "[epoch:14]: val_loss:1.010025,val_acc:0.717118,\n",
      "Epoch 15/59\n",
      "----------\n",
      "loss has increased lr divide 10 lr now is :0.000000\n",
      "[epoch:15,batch:29]:acc: 0.760417,loss:0.738604\n",
      "[epoch:15,batch:59]:acc: 0.755208,loss:0.759817\n",
      "[epoch:15,batch:89]:acc: 0.748264,loss:0.783902\n",
      "[epoch:15,batch:119]:acc: 0.745052,loss:0.791949\n",
      "[epoch:15,batch:149]:acc: 0.750000,loss:0.792712\n",
      "[epoch:15,batch:179]:acc: 0.743229,loss:0.803018\n",
      "[epoch:15,batch:209]:acc: 0.746726,loss:0.799835\n",
      "[epoch:15,batch:239]:acc: 0.746094,loss:0.804371\n",
      "[epoch:15,batch:269]:acc: 0.745255,loss:0.803766\n",
      "[epoch:15,batch:299]:acc: 0.747292,loss:0.798574\n",
      "[epoch:15,batch:299]: val_loss:0.905887,val_acc:0.735184,val_total:4539\n",
      "[epoch:15,batch:329]:acc: 0.747538,loss:0.796976\n",
      "[epoch:15,batch:359]:acc: 0.747656,loss:0.795094\n",
      "[epoch:15,batch:389]:acc: 0.747676,loss:0.793946\n",
      "[epoch:15,batch:419]:acc: 0.747619,loss:0.793448\n",
      "[epoch:15,batch:449]:acc: 0.746944,loss:0.793288\n",
      "[epoch:15,batch:479]:acc: 0.745052,loss:0.796678\n",
      "[epoch:15,batch:509]:acc: 0.744975,loss:0.796681\n",
      "[epoch:15,batch:539]:acc: 0.745370,loss:0.798734\n",
      "[epoch:15,batch:569]:acc: 0.745669,loss:0.796473\n",
      "[epoch:15,batch:599]:acc: 0.744792,loss:0.799065\n",
      "[epoch:15,batch:599]: val_loss:0.896477,val_acc:0.734303,val_total:4539\n",
      "[epoch:15,batch:629]:acc: 0.745585,loss:0.797448\n",
      "[epoch:15,batch:659]:acc: 0.747254,loss:0.794099\n",
      "[epoch:15,batch:689]:acc: 0.748234,loss:0.792914\n",
      "[epoch:15,batch:719]:acc: 0.748611,loss:0.791904\n",
      "[epoch:15,batch:749]:acc: 0.747208,loss:0.797599\n",
      "[epoch:15,batch:779]:acc: 0.747636,loss:0.795868\n",
      "[epoch:15,batch:809]:acc: 0.747762,loss:0.794962\n",
      "[epoch:15,batch:839]:acc: 0.746912,loss:0.796666\n",
      "[epoch:15,batch:869]:acc: 0.746480,loss:0.799249\n",
      "[epoch:15,batch:899]:acc: 0.745556,loss:0.800010\n",
      "[epoch:15,batch:899]: val_loss:0.973705,val_acc:0.726812,val_total:4539\n",
      "[epoch:15,batch:929]:acc: 0.745632,loss:0.799144\n",
      "[epoch:15,batch:959]:acc: 0.745964,loss:0.798850\n",
      "[epoch:15,batch:989]:acc: 0.745960,loss:0.798653\n",
      "[epoch:15] :acc: 0.746004,loss:0.799365,lr:0.000000,patience:0\n",
      "[epoch:15]: val_loss:0.985078,val_acc:0.730998,\n",
      "Epoch 16/59\n",
      "----------\n",
      "[epoch:16,batch:29]:acc: 0.727083,loss:0.848963\n",
      "[epoch:16,batch:59]:acc: 0.728125,loss:0.848055\n",
      "[epoch:16,batch:89]:acc: 0.734028,loss:0.842923\n",
      "[epoch:16,batch:119]:acc: 0.736458,loss:0.825426\n",
      "[epoch:16,batch:149]:acc: 0.742083,loss:0.813726\n",
      "[epoch:16,batch:179]:acc: 0.744618,loss:0.814781\n",
      "[epoch:16,batch:209]:acc: 0.746875,loss:0.807653\n",
      "[epoch:16,batch:239]:acc: 0.747266,loss:0.805114\n",
      "[epoch:16,batch:269]:acc: 0.745949,loss:0.805157\n",
      "[epoch:16,batch:299]:acc: 0.745833,loss:0.803410\n",
      "[epoch:16,batch:299]: val_loss:0.874883,val_acc:0.740912,val_total:4539\n",
      "[epoch:16,batch:329]:acc: 0.746307,loss:0.802978\n",
      "[epoch:16,batch:359]:acc: 0.745660,loss:0.803249\n",
      "[epoch:16,batch:389]:acc: 0.744952,loss:0.806900\n",
      "[epoch:16,batch:419]:acc: 0.743973,loss:0.809578\n",
      "[epoch:16,batch:449]:acc: 0.743819,loss:0.809141\n",
      "[epoch:16,batch:479]:acc: 0.744271,loss:0.806324\n",
      "[epoch:16,batch:509]:acc: 0.743382,loss:0.808095\n",
      "[epoch:16,batch:539]:acc: 0.743171,loss:0.808023\n",
      "[epoch:16,batch:569]:acc: 0.744189,loss:0.806445\n",
      "[epoch:16,batch:599]:acc: 0.744167,loss:0.806132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:16,batch:599]: val_loss:0.955097,val_acc:0.724829,val_total:4539\n",
      "[epoch:16,batch:629]:acc: 0.745337,loss:0.803995\n",
      "[epoch:16,batch:659]:acc: 0.745881,loss:0.803477\n",
      "[epoch:16,batch:689]:acc: 0.745924,loss:0.803867\n",
      "[epoch:16,batch:719]:acc: 0.746745,loss:0.802505\n",
      "[epoch:16,batch:749]:acc: 0.746542,loss:0.800804\n",
      "[epoch:16,batch:779]:acc: 0.747035,loss:0.800515\n",
      "[epoch:16,batch:809]:acc: 0.746528,loss:0.801535\n",
      "[epoch:16,batch:839]:acc: 0.746726,loss:0.800819\n",
      "[epoch:16,batch:869]:acc: 0.746013,loss:0.802461\n",
      "[epoch:16,batch:899]:acc: 0.745694,loss:0.802263\n",
      "[epoch:16,batch:899]: val_loss:0.865509,val_acc:0.736286,val_total:4539\n",
      "[epoch:16,batch:929]:acc: 0.745363,loss:0.802766\n",
      "[epoch:16,batch:959]:acc: 0.745703,loss:0.802053\n",
      "[epoch:16,batch:989]:acc: 0.745139,loss:0.802791\n",
      "[epoch:16] :acc: 0.745184,loss:0.803176,lr:0.000000,patience:1\n",
      "[epoch:16]: val_loss:0.864583,val_acc:0.742675,\n",
      "save new model loss,now loss is  0.8645831942558289\n",
      "Epoch 17/59\n",
      "----------\n",
      "[epoch:17,batch:29]:acc: 0.756250,loss:0.779626\n",
      "[epoch:17,batch:59]:acc: 0.739583,loss:0.815877\n",
      "[epoch:17,batch:89]:acc: 0.737153,loss:0.821435\n",
      "[epoch:17,batch:119]:acc: 0.735417,loss:0.824476\n",
      "[epoch:17,batch:149]:acc: 0.739792,loss:0.810795\n",
      "[epoch:17,batch:179]:acc: 0.742882,loss:0.801117\n",
      "[epoch:17,batch:209]:acc: 0.740923,loss:0.799133\n",
      "[epoch:17,batch:239]:acc: 0.742578,loss:0.803473\n",
      "[epoch:17,batch:269]:acc: 0.741435,loss:0.802049\n",
      "[epoch:17,batch:299]:acc: 0.741354,loss:0.800179\n",
      "[epoch:17,batch:299]: val_loss:0.904046,val_acc:0.735184,val_total:4539\n",
      "[epoch:17,batch:329]:acc: 0.742519,loss:0.799369\n",
      "[epoch:17,batch:359]:acc: 0.743229,loss:0.799349\n",
      "[epoch:17,batch:389]:acc: 0.743269,loss:0.799213\n",
      "[epoch:17,batch:419]:acc: 0.744345,loss:0.796721\n",
      "[epoch:17,batch:449]:acc: 0.742639,loss:0.799470\n",
      "[epoch:17,batch:479]:acc: 0.742578,loss:0.798333\n",
      "[epoch:17,batch:509]:acc: 0.742402,loss:0.799455\n",
      "[epoch:17,batch:539]:acc: 0.743519,loss:0.797477\n",
      "[epoch:17,batch:569]:acc: 0.744298,loss:0.797752\n",
      "[epoch:17,batch:599]:acc: 0.743802,loss:0.800912\n",
      "[epoch:17,batch:599]: val_loss:0.956056,val_acc:0.727253,val_total:4539\n",
      "[epoch:17,batch:629]:acc: 0.743552,loss:0.801940\n",
      "[epoch:17,batch:659]:acc: 0.744318,loss:0.798464\n",
      "[epoch:17,batch:689]:acc: 0.743614,loss:0.799820\n",
      "[epoch:17,batch:719]:acc: 0.743403,loss:0.800757\n",
      "[epoch:17,batch:749]:acc: 0.743625,loss:0.800894\n",
      "[epoch:17,batch:779]:acc: 0.743309,loss:0.801899\n",
      "[epoch:17,batch:809]:acc: 0.743711,loss:0.803251\n",
      "[epoch:17,batch:839]:acc: 0.743155,loss:0.805328\n",
      "[epoch:17,batch:869]:acc: 0.743032,loss:0.805689\n",
      "[epoch:17,batch:899]:acc: 0.743993,loss:0.803536\n",
      "[epoch:17,batch:899]: val_loss:0.892878,val_acc:0.735184,val_total:4539\n",
      "[epoch:17,batch:929]:acc: 0.743952,loss:0.802951\n",
      "[epoch:17,batch:959]:acc: 0.744010,loss:0.801976\n",
      "[epoch:17,batch:989]:acc: 0.744160,loss:0.802033\n",
      "[epoch:17] :acc: 0.744112,loss:0.802454,lr:0.000000,patience:0\n",
      "[epoch:17]: val_loss:0.905175,val_acc:0.731659,\n",
      "Epoch 18/59\n",
      "----------\n",
      "[epoch:18,batch:29]:acc: 0.739583,loss:0.814117\n",
      "[epoch:18,batch:59]:acc: 0.746875,loss:0.816468\n",
      "[epoch:18,batch:89]:acc: 0.750347,loss:0.798822\n",
      "[epoch:18,batch:119]:acc: 0.749479,loss:0.795809\n",
      "[epoch:18,batch:149]:acc: 0.742500,loss:0.815325\n",
      "[epoch:18,batch:179]:acc: 0.740278,loss:0.818625\n",
      "[epoch:18,batch:209]:acc: 0.740774,loss:0.819866\n",
      "[epoch:18,batch:239]:acc: 0.740625,loss:0.811502\n",
      "[epoch:18,batch:269]:acc: 0.741782,loss:0.810485\n",
      "[epoch:18,batch:299]:acc: 0.740625,loss:0.809605\n",
      "[epoch:18,batch:299]: val_loss:0.858951,val_acc:0.742014,val_total:4539\n",
      "[epoch:18,batch:329]:acc: 0.741004,loss:0.810032\n",
      "[epoch:18,batch:359]:acc: 0.741146,loss:0.812094\n",
      "[epoch:18,batch:389]:acc: 0.743510,loss:0.805264\n",
      "[epoch:18,batch:419]:acc: 0.744196,loss:0.804919\n",
      "[epoch:18,batch:449]:acc: 0.744861,loss:0.803653\n",
      "[epoch:18,batch:479]:acc: 0.744076,loss:0.806710\n",
      "[epoch:18,batch:509]:acc: 0.744730,loss:0.804091\n",
      "[epoch:18,batch:539]:acc: 0.744792,loss:0.805338\n",
      "[epoch:18,batch:569]:acc: 0.745230,loss:0.804161\n",
      "[epoch:18,batch:599]:acc: 0.744427,loss:0.809103\n",
      "[epoch:18,batch:599]: val_loss:0.852877,val_acc:0.742895,val_total:4539\n",
      "[epoch:18,batch:629]:acc: 0.744048,loss:0.808926\n",
      "[epoch:18,batch:659]:acc: 0.743134,loss:0.811196\n",
      "[epoch:18,batch:689]:acc: 0.743433,loss:0.810078\n",
      "[epoch:18,batch:719]:acc: 0.743576,loss:0.809263\n",
      "[epoch:18,batch:749]:acc: 0.742625,loss:0.808873\n",
      "[epoch:18,batch:779]:acc: 0.742388,loss:0.808730\n",
      "[epoch:18,batch:809]:acc: 0.741705,loss:0.811535\n",
      "[epoch:18,batch:839]:acc: 0.742299,loss:0.811005\n",
      "[epoch:18,batch:869]:acc: 0.742636,loss:0.812153\n",
      "[epoch:18,batch:899]:acc: 0.743229,loss:0.811999\n",
      "[epoch:18,batch:899]: val_loss:0.926703,val_acc:0.729235,val_total:4539\n",
      "[epoch:18,batch:929]:acc: 0.743280,loss:0.811767\n",
      "[epoch:18,batch:959]:acc: 0.743620,loss:0.808734\n",
      "[epoch:18,batch:989]:acc: 0.744192,loss:0.808341\n",
      "[epoch:18] :acc: 0.744144,loss:0.808882,lr:0.000000,patience:1\n",
      "[epoch:18]: val_loss:0.849772,val_acc:0.743556,\n",
      "save new model loss,now loss is  0.8497717380523682\n",
      "Epoch 19/59\n",
      "----------\n",
      "[epoch:19,batch:29]:acc: 0.748958,loss:0.793370\n",
      "[epoch:19,batch:59]:acc: 0.748958,loss:0.805096\n",
      "[epoch:19,batch:89]:acc: 0.751042,loss:0.814628\n",
      "[epoch:19,batch:119]:acc: 0.745052,loss:0.815201\n",
      "[epoch:19,batch:149]:acc: 0.744375,loss:0.831692\n",
      "[epoch:19,batch:179]:acc: 0.746181,loss:0.825528\n",
      "[epoch:19,batch:209]:acc: 0.747917,loss:0.822828\n",
      "[epoch:19,batch:239]:acc: 0.747786,loss:0.820727\n",
      "[epoch:19,batch:269]:acc: 0.745718,loss:0.822231\n",
      "[epoch:19,batch:299]:acc: 0.745417,loss:0.818337\n",
      "[epoch:19,batch:299]: val_loss:0.929100,val_acc:0.730778,val_total:4539\n",
      "[epoch:19,batch:329]:acc: 0.746212,loss:0.814663\n",
      "[epoch:19,batch:359]:acc: 0.746528,loss:0.809029\n",
      "[epoch:19,batch:389]:acc: 0.747837,loss:0.804722\n",
      "[epoch:19,batch:419]:acc: 0.748586,loss:0.803244\n",
      "[epoch:19,batch:449]:acc: 0.746528,loss:0.808745\n",
      "[epoch:19,batch:479]:acc: 0.745443,loss:0.811315\n",
      "[epoch:19,batch:509]:acc: 0.744608,loss:0.810487\n",
      "[epoch:19,batch:539]:acc: 0.744850,loss:0.807376\n",
      "[epoch:19,batch:569]:acc: 0.746601,loss:0.804779\n",
      "[epoch:19,batch:599]:acc: 0.746250,loss:0.805586\n",
      "[epoch:19,batch:599]: val_loss:0.919486,val_acc:0.731439,val_total:4539\n",
      "[epoch:19,batch:629]:acc: 0.745635,loss:0.806093\n",
      "[epoch:19,batch:659]:acc: 0.746117,loss:0.804660\n",
      "[epoch:19,batch:689]:acc: 0.745697,loss:0.806534\n",
      "[epoch:19,batch:719]:acc: 0.745703,loss:0.805179\n",
      "[epoch:19,batch:749]:acc: 0.746125,loss:0.803569\n",
      "[epoch:19,batch:779]:acc: 0.746474,loss:0.802641\n",
      "[epoch:19,batch:809]:acc: 0.746566,loss:0.803621\n",
      "[epoch:19,batch:839]:acc: 0.745164,loss:0.806898\n",
      "[epoch:19,batch:869]:acc: 0.744612,loss:0.807314\n",
      "[epoch:19,batch:899]:acc: 0.744375,loss:0.806495\n",
      "[epoch:19,batch:899]: val_loss:0.935413,val_acc:0.728575,val_total:4539\n",
      "[epoch:19,batch:929]:acc: 0.745329,loss:0.804031\n",
      "[epoch:19,batch:959]:acc: 0.745996,loss:0.801501\n",
      "[epoch:19,batch:989]:acc: 0.746275,loss:0.800912\n",
      "[epoch:19] :acc: 0.745815,loss:0.803090,lr:0.000000,patience:0\n",
      "[epoch:19]: val_loss:0.981393,val_acc:0.724609,\n",
      "Epoch 20/59\n",
      "----------\n",
      "[epoch:20,batch:29]:acc: 0.757292,loss:0.825763\n",
      "[epoch:20,batch:59]:acc: 0.745313,loss:0.829219\n",
      "[epoch:20,batch:89]:acc: 0.741319,loss:0.819108\n",
      "[epoch:20,batch:119]:acc: 0.744010,loss:0.809944\n",
      "[epoch:20,batch:149]:acc: 0.743125,loss:0.803336\n",
      "[epoch:20,batch:179]:acc: 0.742361,loss:0.810907\n",
      "[epoch:20,batch:209]:acc: 0.744048,loss:0.806328\n",
      "[epoch:20,batch:239]:acc: 0.745313,loss:0.800505\n",
      "[epoch:20,batch:269]:acc: 0.745255,loss:0.799672\n",
      "[epoch:20,batch:299]:acc: 0.746979,loss:0.797242\n",
      "[epoch:20,batch:299]: val_loss:0.881265,val_acc:0.735845,val_total:4539\n",
      "[epoch:20,batch:329]:acc: 0.747633,loss:0.792818\n",
      "[epoch:20,batch:359]:acc: 0.747135,loss:0.797642\n",
      "[epoch:20,batch:389]:acc: 0.746875,loss:0.798240\n",
      "[epoch:20,batch:419]:acc: 0.747321,loss:0.794795\n",
      "[epoch:20,batch:449]:acc: 0.747014,loss:0.797006\n",
      "[epoch:20,batch:479]:acc: 0.748112,loss:0.794444\n",
      "[epoch:20,batch:509]:acc: 0.747488,loss:0.794324\n",
      "[epoch:20,batch:539]:acc: 0.746991,loss:0.795005\n",
      "[epoch:20,batch:569]:acc: 0.748246,loss:0.791938\n",
      "[epoch:20,batch:599]:acc: 0.748125,loss:0.792397\n",
      "[epoch:20,batch:599]: val_loss:0.905120,val_acc:0.732760,val_total:4539\n",
      "[epoch:20,batch:629]:acc: 0.749058,loss:0.791868\n",
      "[epoch:20,batch:659]:acc: 0.749290,loss:0.791659\n",
      "[epoch:20,batch:689]:acc: 0.748551,loss:0.792734\n",
      "[epoch:20,batch:719]:acc: 0.748307,loss:0.794380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:20,batch:749]:acc: 0.747667,loss:0.795049\n",
      "[epoch:20,batch:779]:acc: 0.748638,loss:0.794365\n",
      "[epoch:20,batch:809]:acc: 0.748187,loss:0.794510\n",
      "[epoch:20,batch:839]:acc: 0.747991,loss:0.795719\n",
      "[epoch:20,batch:869]:acc: 0.747414,loss:0.795251\n",
      "[epoch:20,batch:899]:acc: 0.747465,loss:0.795376\n",
      "[epoch:20,batch:899]: val_loss:0.883469,val_acc:0.737828,val_total:4539\n",
      "[epoch:20,batch:929]:acc: 0.747144,loss:0.795715\n",
      "[epoch:20,batch:959]:acc: 0.747461,loss:0.796715\n",
      "[epoch:20,batch:989]:acc: 0.747348,loss:0.797186\n",
      "[epoch:20] :acc: 0.747391,loss:0.797488,lr:0.000000,patience:1\n",
      "[epoch:20]: val_loss:0.903589,val_acc:0.739811,\n",
      "Epoch 21/59\n",
      "----------\n",
      "loss has increased lr divide 10 lr now is :0.000000\n",
      "[epoch:21,batch:29]:acc: 0.754167,loss:0.756956\n",
      "[epoch:21,batch:59]:acc: 0.752604,loss:0.786661\n",
      "[epoch:21,batch:89]:acc: 0.749306,loss:0.799789\n",
      "[epoch:21,batch:119]:acc: 0.744010,loss:0.807095\n",
      "[epoch:21,batch:149]:acc: 0.743542,loss:0.804863\n",
      "[epoch:21,batch:179]:acc: 0.742535,loss:0.797395\n",
      "[epoch:21,batch:209]:acc: 0.745089,loss:0.798931\n",
      "[epoch:21,batch:239]:acc: 0.745182,loss:0.796512\n",
      "[epoch:21,batch:269]:acc: 0.746181,loss:0.794976\n",
      "[epoch:21,batch:299]:acc: 0.748750,loss:0.789528\n",
      "[epoch:21,batch:299]: val_loss:0.924736,val_acc:0.730117,val_total:4539\n",
      "[epoch:21,batch:329]:acc: 0.747727,loss:0.793646\n",
      "[epoch:21,batch:359]:acc: 0.747743,loss:0.795353\n",
      "[epoch:21,batch:389]:acc: 0.749760,loss:0.791825\n",
      "[epoch:21,batch:419]:acc: 0.747470,loss:0.801364\n",
      "[epoch:21,batch:449]:acc: 0.746042,loss:0.800784\n",
      "[epoch:21,batch:479]:acc: 0.744857,loss:0.801399\n",
      "[epoch:21,batch:509]:acc: 0.744730,loss:0.801111\n",
      "[epoch:21,batch:539]:acc: 0.745197,loss:0.799543\n",
      "[epoch:21,batch:569]:acc: 0.745011,loss:0.800269\n",
      "[epoch:21,batch:599]:acc: 0.745938,loss:0.799427\n",
      "[epoch:21,batch:599]: val_loss:0.938887,val_acc:0.728134,val_total:4539\n",
      "[epoch:21,batch:629]:acc: 0.746379,loss:0.799755\n",
      "[epoch:21,batch:659]:acc: 0.746449,loss:0.797118\n",
      "[epoch:21,batch:689]:acc: 0.746105,loss:0.797927\n",
      "[epoch:21,batch:719]:acc: 0.745920,loss:0.797776\n",
      "[epoch:21,batch:749]:acc: 0.746208,loss:0.797222\n",
      "[epoch:21,batch:779]:acc: 0.746474,loss:0.798206\n",
      "[epoch:21,batch:809]:acc: 0.744946,loss:0.801000\n",
      "[epoch:21,batch:839]:acc: 0.745350,loss:0.800238\n",
      "[epoch:21,batch:869]:acc: 0.744684,loss:0.801272\n",
      "[epoch:21,batch:899]:acc: 0.745035,loss:0.799592\n",
      "[epoch:21,batch:899]: val_loss:0.886894,val_acc:0.736286,val_total:4539\n",
      "[epoch:21,batch:929]:acc: 0.744691,loss:0.800205\n",
      "[epoch:21,batch:959]:acc: 0.745605,loss:0.798264\n",
      "[epoch:21,batch:989]:acc: 0.745991,loss:0.798194\n",
      "[epoch:21] :acc: 0.745846,loss:0.798855,lr:0.000000,patience:0\n",
      "[epoch:21]: val_loss:0.933625,val_acc:0.730117,\n",
      "Epoch 22/59\n",
      "----------\n",
      "[epoch:22,batch:29]:acc: 0.734375,loss:0.860560\n",
      "[epoch:22,batch:59]:acc: 0.741667,loss:0.815631\n",
      "[epoch:22,batch:89]:acc: 0.745139,loss:0.810478\n",
      "[epoch:22,batch:119]:acc: 0.746354,loss:0.805442\n",
      "[epoch:22,batch:149]:acc: 0.746042,loss:0.795752\n",
      "[epoch:22,batch:179]:acc: 0.743229,loss:0.797075\n",
      "[epoch:22,batch:209]:acc: 0.743601,loss:0.799325\n",
      "[epoch:22,batch:239]:acc: 0.744922,loss:0.799117\n",
      "[epoch:22,batch:269]:acc: 0.744097,loss:0.800546\n",
      "[epoch:22,batch:299]:acc: 0.745729,loss:0.799292\n",
      "[epoch:22,batch:299]: val_loss:0.883493,val_acc:0.739150,val_total:4539\n",
      "[epoch:22,batch:329]:acc: 0.747443,loss:0.793569\n",
      "[epoch:22,batch:359]:acc: 0.748351,loss:0.791754\n",
      "[epoch:22,batch:389]:acc: 0.748077,loss:0.791226\n",
      "[epoch:22,batch:419]:acc: 0.749628,loss:0.786999\n",
      "[epoch:22,batch:449]:acc: 0.750069,loss:0.788267\n",
      "[epoch:22,batch:479]:acc: 0.750716,loss:0.787765\n",
      "[epoch:22,batch:509]:acc: 0.750000,loss:0.789470\n",
      "[epoch:22,batch:539]:acc: 0.748669,loss:0.788555\n",
      "[epoch:22,batch:569]:acc: 0.748794,loss:0.791082\n",
      "[epoch:22,batch:599]:acc: 0.748490,loss:0.790653\n",
      "[epoch:22,batch:599]: val_loss:0.964311,val_acc:0.729456,val_total:4539\n",
      "[epoch:22,batch:629]:acc: 0.747619,loss:0.792519\n",
      "[epoch:22,batch:659]:acc: 0.748295,loss:0.789791\n",
      "[epoch:22,batch:689]:acc: 0.748687,loss:0.790984\n",
      "[epoch:22,batch:719]:acc: 0.748003,loss:0.790840\n",
      "[epoch:22,batch:749]:acc: 0.747625,loss:0.791331\n",
      "[epoch:22,batch:779]:acc: 0.748197,loss:0.790581\n",
      "[epoch:22,batch:809]:acc: 0.747531,loss:0.790981\n",
      "[epoch:22,batch:839]:acc: 0.746912,loss:0.792694\n",
      "[epoch:22,batch:869]:acc: 0.746300,loss:0.793736\n",
      "[epoch:22,batch:899]:acc: 0.745278,loss:0.794879\n",
      "[epoch:22,batch:899]: val_loss:0.867474,val_acc:0.740471,val_total:4539\n",
      "[epoch:22,batch:929]:acc: 0.745329,loss:0.794548\n",
      "[epoch:22,batch:959]:acc: 0.744954,loss:0.795400\n",
      "[epoch:22,batch:989]:acc: 0.744318,loss:0.797766\n",
      "[epoch:22] :acc: 0.744270,loss:0.798086,lr:0.000000,patience:1\n",
      "[epoch:22]: val_loss:0.966982,val_acc:0.722406,\n",
      "Epoch 23/59\n",
      "----------\n",
      "loss has increased lr divide 10 lr now is :0.000000\n",
      "[epoch:23,batch:29]:acc: 0.759375,loss:0.765595\n",
      "[epoch:23,batch:59]:acc: 0.738021,loss:0.798711\n",
      "[epoch:23,batch:89]:acc: 0.739931,loss:0.804778\n",
      "[epoch:23,batch:119]:acc: 0.740625,loss:0.792757\n",
      "[epoch:23,batch:149]:acc: 0.738125,loss:0.802135\n",
      "[epoch:23,batch:179]:acc: 0.739062,loss:0.804342\n",
      "[epoch:23,batch:209]:acc: 0.738542,loss:0.803643\n",
      "[epoch:23,batch:239]:acc: 0.738802,loss:0.797802\n",
      "[epoch:23,batch:269]:acc: 0.739583,loss:0.798971\n",
      "[epoch:23,batch:299]:acc: 0.741042,loss:0.796687\n",
      "[epoch:23,batch:299]: val_loss:0.914828,val_acc:0.730998,val_total:4539\n",
      "[epoch:23,batch:329]:acc: 0.739394,loss:0.806419\n",
      "[epoch:23,batch:359]:acc: 0.739757,loss:0.805065\n",
      "[epoch:23,batch:389]:acc: 0.740785,loss:0.802694\n",
      "[epoch:23,batch:419]:acc: 0.741741,loss:0.799564\n",
      "[epoch:23,batch:449]:acc: 0.740347,loss:0.802498\n",
      "[epoch:23,batch:479]:acc: 0.739714,loss:0.806518\n",
      "[epoch:23,batch:509]:acc: 0.740012,loss:0.805477\n",
      "[epoch:23,batch:539]:acc: 0.739873,loss:0.805989\n",
      "[epoch:23,batch:569]:acc: 0.739583,loss:0.806554\n",
      "[epoch:23,batch:599]:acc: 0.739583,loss:0.805039\n",
      "[epoch:23,batch:599]: val_loss:0.881711,val_acc:0.737387,val_total:4539\n",
      "[epoch:23,batch:629]:acc: 0.740228,loss:0.805638\n",
      "[epoch:23,batch:659]:acc: 0.740199,loss:0.804453\n",
      "[epoch:23,batch:689]:acc: 0.740399,loss:0.805770\n",
      "[epoch:23,batch:719]:acc: 0.740278,loss:0.806382\n",
      "[epoch:23,batch:749]:acc: 0.740542,loss:0.806487\n",
      "[epoch:23,batch:779]:acc: 0.741186,loss:0.804600\n",
      "[epoch:23,batch:809]:acc: 0.741821,loss:0.804588\n",
      "[epoch:23,batch:839]:acc: 0.742336,loss:0.803426\n",
      "[epoch:23,batch:869]:acc: 0.742385,loss:0.801825\n",
      "[epoch:23,batch:899]:acc: 0.741701,loss:0.803649\n",
      "[epoch:23,batch:899]: val_loss:0.853459,val_acc:0.741573,val_total:4539\n",
      "[epoch:23,batch:929]:acc: 0.741969,loss:0.801997\n",
      "[epoch:23,batch:959]:acc: 0.741797,loss:0.803026\n",
      "[epoch:23,batch:989]:acc: 0.741477,loss:0.804665\n",
      "[epoch:23] :acc: 0.741337,loss:0.805429,lr:0.000000,patience:0\n",
      "[epoch:23]: val_loss:0.978240,val_acc:0.729676,\n",
      "Epoch 24/59\n",
      "----------\n",
      "[epoch:24,batch:29]:acc: 0.743750,loss:0.789929\n",
      "[epoch:24,batch:59]:acc: 0.745833,loss:0.775838\n",
      "[epoch:24,batch:89]:acc: 0.745486,loss:0.782486\n",
      "[epoch:24,batch:119]:acc: 0.744271,loss:0.800655\n",
      "[epoch:24,batch:149]:acc: 0.744792,loss:0.806641\n",
      "[epoch:24,batch:179]:acc: 0.742361,loss:0.808573\n",
      "[epoch:24,batch:209]:acc: 0.740625,loss:0.812251\n",
      "[epoch:24,batch:239]:acc: 0.743880,loss:0.801960\n",
      "[epoch:24,batch:269]:acc: 0.742014,loss:0.805624\n",
      "[epoch:24,batch:299]:acc: 0.741875,loss:0.803633\n",
      "[epoch:24,batch:299]: val_loss:0.880416,val_acc:0.744217,val_total:4539\n",
      "[epoch:24,batch:329]:acc: 0.742898,loss:0.802249\n",
      "[epoch:24,batch:359]:acc: 0.743229,loss:0.801999\n",
      "[epoch:24,batch:389]:acc: 0.744071,loss:0.804748\n",
      "[epoch:24,batch:419]:acc: 0.745089,loss:0.805555\n",
      "[epoch:24,batch:449]:acc: 0.745347,loss:0.804899\n",
      "[epoch:24,batch:479]:acc: 0.744466,loss:0.805409\n",
      "[epoch:24,batch:509]:acc: 0.743321,loss:0.807171\n",
      "[epoch:24,batch:539]:acc: 0.743576,loss:0.806372\n",
      "[epoch:24,batch:569]:acc: 0.742325,loss:0.808122\n",
      "[epoch:24,batch:599]:acc: 0.743281,loss:0.804829\n",
      "[epoch:24,batch:599]: val_loss:0.880038,val_acc:0.743336,val_total:4539\n",
      "[epoch:24,batch:629]:acc: 0.743651,loss:0.804690\n",
      "[epoch:24,batch:659]:acc: 0.743987,loss:0.804483\n",
      "[epoch:24,batch:689]:acc: 0.743931,loss:0.805947\n",
      "[epoch:24,batch:719]:acc: 0.744488,loss:0.808980\n",
      "[epoch:24,batch:749]:acc: 0.744458,loss:0.809313\n",
      "[epoch:24,batch:779]:acc: 0.743710,loss:0.811569\n",
      "[epoch:24,batch:809]:acc: 0.744290,loss:0.809660\n",
      "[epoch:24,batch:839]:acc: 0.744680,loss:0.809858\n",
      "[epoch:24,batch:869]:acc: 0.743894,loss:0.811608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:24,batch:899]:acc: 0.743854,loss:0.811573\n",
      "[epoch:24,batch:899]: val_loss:0.979808,val_acc:0.722406,val_total:4539\n",
      "[epoch:24,batch:929]:acc: 0.744825,loss:0.809391\n",
      "[epoch:24,batch:959]:acc: 0.745508,loss:0.807292\n",
      "[epoch:24,batch:989]:acc: 0.745770,loss:0.805501\n",
      "[epoch:24] :acc: 0.745720,loss:0.805511,lr:0.000000,patience:1\n",
      "[epoch:24]: val_loss:0.911978,val_acc:0.732320,\n",
      "Epoch 25/59\n",
      "----------\n",
      "loss has increased lr divide 10 lr now is :0.000000\n",
      "[epoch:25,batch:29]:acc: 0.763542,loss:0.818334\n",
      "[epoch:25,batch:59]:acc: 0.756250,loss:0.797482\n",
      "[epoch:25,batch:89]:acc: 0.757986,loss:0.788346\n",
      "[epoch:25,batch:119]:acc: 0.751042,loss:0.794832\n",
      "[epoch:25,batch:149]:acc: 0.752083,loss:0.794296\n",
      "[epoch:25,batch:179]:acc: 0.753993,loss:0.790211\n",
      "[epoch:25,batch:209]:acc: 0.750149,loss:0.796113\n",
      "[epoch:25,batch:239]:acc: 0.752734,loss:0.791044\n",
      "[epoch:25,batch:269]:acc: 0.755903,loss:0.785867\n",
      "[epoch:25,batch:299]:acc: 0.756042,loss:0.781655\n",
      "[epoch:25,batch:299]: val_loss:0.873247,val_acc:0.738268,val_total:4539\n",
      "[epoch:25,batch:329]:acc: 0.756439,loss:0.785741\n",
      "[epoch:25,batch:359]:acc: 0.754601,loss:0.785973\n",
      "[epoch:25,batch:389]:acc: 0.753045,loss:0.789392\n",
      "[epoch:25,batch:419]:acc: 0.750670,loss:0.794419\n",
      "[epoch:25,batch:449]:acc: 0.750278,loss:0.793779\n",
      "[epoch:25,batch:479]:acc: 0.749870,loss:0.794003\n",
      "[epoch:25,batch:509]:acc: 0.748897,loss:0.795997\n",
      "[epoch:25,batch:539]:acc: 0.747801,loss:0.797353\n",
      "[epoch:25,batch:569]:acc: 0.747533,loss:0.799851\n",
      "[epoch:25,batch:599]:acc: 0.747656,loss:0.799872\n",
      "[epoch:25,batch:599]: val_loss:0.907310,val_acc:0.735625,val_total:4539\n",
      "[epoch:25,batch:629]:acc: 0.748115,loss:0.800634\n",
      "[epoch:25,batch:659]:acc: 0.747633,loss:0.802034\n",
      "[epoch:25,batch:689]:acc: 0.747011,loss:0.803532\n",
      "[epoch:25,batch:719]:acc: 0.747222,loss:0.802430\n",
      "[epoch:25,batch:749]:acc: 0.746667,loss:0.805418\n",
      "[epoch:25,batch:779]:acc: 0.746635,loss:0.804969\n",
      "[epoch:25,batch:809]:acc: 0.746219,loss:0.804682\n",
      "[epoch:25,batch:839]:acc: 0.746838,loss:0.803451\n",
      "[epoch:25,batch:869]:acc: 0.746516,loss:0.803620\n",
      "[epoch:25,batch:899]:acc: 0.747049,loss:0.801778\n",
      "[epoch:25,batch:899]: val_loss:0.913315,val_acc:0.730557,val_total:4539\n",
      "[epoch:25,batch:929]:acc: 0.747110,loss:0.800826\n",
      "[epoch:25,batch:959]:acc: 0.747591,loss:0.800019\n",
      "[epoch:25,batch:989]:acc: 0.747569,loss:0.799834\n",
      "[epoch:25] :acc: 0.747486,loss:0.799967,lr:0.000000,patience:0\n",
      "[epoch:25]: val_loss:0.918546,val_acc:0.732100,\n",
      "Epoch 26/59\n",
      "----------\n",
      "[epoch:26,batch:29]:acc: 0.713542,loss:0.857023\n",
      "[epoch:26,batch:59]:acc: 0.727083,loss:0.831490\n",
      "[epoch:26,batch:89]:acc: 0.734028,loss:0.818934\n",
      "[epoch:26,batch:119]:acc: 0.740625,loss:0.811200\n",
      "[epoch:26,batch:149]:acc: 0.745208,loss:0.801421\n",
      "[epoch:26,batch:179]:acc: 0.748437,loss:0.802055\n",
      "[epoch:26,batch:209]:acc: 0.744345,loss:0.802513\n",
      "[epoch:26,batch:239]:acc: 0.747005,loss:0.800926\n",
      "[epoch:26,batch:269]:acc: 0.749074,loss:0.800039\n",
      "[epoch:26,batch:299]:acc: 0.750208,loss:0.801586\n",
      "[epoch:26,batch:299]: val_loss:0.887669,val_acc:0.736726,val_total:4539\n",
      "[epoch:26,batch:329]:acc: 0.747727,loss:0.806139\n",
      "[epoch:26,batch:359]:acc: 0.746875,loss:0.806925\n",
      "[epoch:26,batch:389]:acc: 0.747356,loss:0.805108\n",
      "[epoch:26,batch:419]:acc: 0.746726,loss:0.806180\n",
      "[epoch:26,batch:449]:acc: 0.747847,loss:0.802728\n",
      "[epoch:26,batch:479]:acc: 0.748112,loss:0.800294\n",
      "[epoch:26,batch:509]:acc: 0.747855,loss:0.800555\n",
      "[epoch:26,batch:539]:acc: 0.747627,loss:0.803160\n",
      "[epoch:26,batch:569]:acc: 0.747643,loss:0.803302\n",
      "[epoch:26,batch:599]:acc: 0.747917,loss:0.802158\n",
      "[epoch:26,batch:599]: val_loss:0.881765,val_acc:0.741353,val_total:4539\n",
      "[epoch:26,batch:629]:acc: 0.748313,loss:0.799920\n",
      "[epoch:26,batch:659]:acc: 0.747917,loss:0.801834\n",
      "[epoch:26,batch:689]:acc: 0.747554,loss:0.802282\n",
      "[epoch:26,batch:719]:acc: 0.747830,loss:0.801326\n",
      "[epoch:26,batch:749]:acc: 0.748708,loss:0.801683\n",
      "[epoch:26,batch:779]:acc: 0.749079,loss:0.798869\n",
      "[epoch:26,batch:809]:acc: 0.749306,loss:0.799389\n",
      "[epoch:26,batch:839]:acc: 0.748065,loss:0.799873\n",
      "[epoch:26,batch:869]:acc: 0.748096,loss:0.799741\n",
      "[epoch:26,batch:899]:acc: 0.748333,loss:0.798673\n",
      "[epoch:26,batch:899]: val_loss:0.885392,val_acc:0.735845,val_total:4539\n",
      "[epoch:26,batch:929]:acc: 0.747782,loss:0.798362\n",
      "[epoch:26,batch:959]:acc: 0.747461,loss:0.798643\n",
      "[epoch:26,batch:989]:acc: 0.747128,loss:0.798918\n",
      "[epoch:26] :acc: 0.747202,loss:0.798661,lr:0.000000,patience:1\n",
      "[epoch:26]: val_loss:0.855112,val_acc:0.742234,\n",
      "Epoch 27/59\n",
      "----------\n",
      "loss has increased lr divide 10 lr now is :0.000000\n",
      "[epoch:27,batch:29]:acc: 0.756250,loss:0.791640\n",
      "[epoch:27,batch:59]:acc: 0.744271,loss:0.822050\n",
      "[epoch:27,batch:89]:acc: 0.741319,loss:0.825690\n",
      "[epoch:27,batch:119]:acc: 0.747656,loss:0.807920\n",
      "[epoch:27,batch:149]:acc: 0.749583,loss:0.802761\n",
      "[epoch:27,batch:179]:acc: 0.748611,loss:0.810384\n",
      "[epoch:27,batch:209]:acc: 0.744643,loss:0.809596\n",
      "[epoch:27,batch:239]:acc: 0.743359,loss:0.811235\n",
      "[epoch:27,batch:269]:acc: 0.744676,loss:0.805871\n",
      "[epoch:27,batch:299]:acc: 0.743229,loss:0.803625\n",
      "[epoch:27,batch:299]: val_loss:0.881205,val_acc:0.738709,val_total:4539\n",
      "[epoch:27,batch:329]:acc: 0.745265,loss:0.798506\n",
      "[epoch:27,batch:359]:acc: 0.748264,loss:0.795533\n",
      "[epoch:27,batch:389]:acc: 0.748157,loss:0.792688\n",
      "[epoch:27,batch:419]:acc: 0.748363,loss:0.794825\n",
      "[epoch:27,batch:449]:acc: 0.748819,loss:0.793704\n",
      "[epoch:27,batch:479]:acc: 0.748372,loss:0.793990\n",
      "[epoch:27,batch:509]:acc: 0.748039,loss:0.795053\n",
      "[epoch:27,batch:539]:acc: 0.747512,loss:0.797679\n",
      "[epoch:27,batch:569]:acc: 0.747697,loss:0.796957\n",
      "[epoch:27,batch:599]:acc: 0.747135,loss:0.798556\n",
      "[epoch:27,batch:599]: val_loss:0.899127,val_acc:0.738709,val_total:4539\n",
      "[epoch:27,batch:629]:acc: 0.747421,loss:0.797381\n",
      "[epoch:27,batch:659]:acc: 0.747017,loss:0.797412\n",
      "[epoch:27,batch:689]:acc: 0.746377,loss:0.798386\n",
      "[epoch:27,batch:719]:acc: 0.746571,loss:0.798931\n",
      "[epoch:27,batch:749]:acc: 0.746125,loss:0.799913\n",
      "[epoch:27,batch:779]:acc: 0.746835,loss:0.799055\n",
      "[epoch:27,batch:809]:acc: 0.746991,loss:0.798906\n",
      "[epoch:27,batch:839]:acc: 0.746912,loss:0.799468\n",
      "[epoch:27,batch:869]:acc: 0.747522,loss:0.798638\n",
      "[epoch:27,batch:899]:acc: 0.746910,loss:0.799603\n",
      "[epoch:27,batch:899]: val_loss:0.861049,val_acc:0.742454,val_total:4539\n",
      "[epoch:27,batch:929]:acc: 0.747312,loss:0.799950\n",
      "[epoch:27,batch:959]:acc: 0.746191,loss:0.801455\n",
      "[epoch:27,batch:989]:acc: 0.746023,loss:0.800801\n",
      "[epoch:27] :acc: 0.746130,loss:0.801135,lr:0.000000,patience:0\n",
      "[epoch:27]: val_loss:1.007779,val_acc:0.724389,\n",
      "Epoch 28/59\n",
      "----------\n",
      "[epoch:28,batch:29]:acc: 0.738542,loss:0.819175\n",
      "[epoch:28,batch:59]:acc: 0.740625,loss:0.837137\n",
      "[epoch:28,batch:89]:acc: 0.743403,loss:0.816879\n",
      "[epoch:28,batch:119]:acc: 0.740885,loss:0.818118\n",
      "[epoch:28,batch:149]:acc: 0.744375,loss:0.812022\n",
      "[epoch:28,batch:179]:acc: 0.745139,loss:0.809391\n",
      "[epoch:28,batch:209]:acc: 0.743601,loss:0.807341\n",
      "[epoch:28,batch:239]:acc: 0.741146,loss:0.814787\n",
      "[epoch:28,batch:269]:acc: 0.741319,loss:0.812476\n",
      "[epoch:28,batch:299]:acc: 0.740833,loss:0.810739\n",
      "[epoch:28,batch:299]: val_loss:0.875598,val_acc:0.737828,val_total:4539\n",
      "[epoch:28,batch:329]:acc: 0.741098,loss:0.809751\n",
      "[epoch:28,batch:359]:acc: 0.742535,loss:0.806182\n",
      "[epoch:28,batch:389]:acc: 0.742708,loss:0.807217\n",
      "[epoch:28,batch:419]:acc: 0.743080,loss:0.805710\n",
      "[epoch:28,batch:449]:acc: 0.743542,loss:0.803890\n",
      "[epoch:28,batch:479]:acc: 0.743034,loss:0.806047\n",
      "[epoch:28,batch:509]:acc: 0.742953,loss:0.804237\n",
      "[epoch:28,batch:539]:acc: 0.743345,loss:0.801015\n",
      "[epoch:28,batch:569]:acc: 0.742982,loss:0.800658\n",
      "[epoch:28,batch:599]:acc: 0.743698,loss:0.797910\n",
      "[epoch:28,batch:599]: val_loss:0.892173,val_acc:0.737607,val_total:4539\n",
      "[epoch:28,batch:629]:acc: 0.744395,loss:0.796030\n",
      "[epoch:28,batch:659]:acc: 0.744413,loss:0.795983\n",
      "[epoch:28,batch:689]:acc: 0.745154,loss:0.794706\n",
      "[epoch:28,batch:719]:acc: 0.744271,loss:0.797182\n",
      "[epoch:28,batch:749]:acc: 0.744750,loss:0.797024\n",
      "[epoch:28,batch:779]:acc: 0.744671,loss:0.798221\n",
      "[epoch:28,batch:809]:acc: 0.745293,loss:0.798212\n",
      "[epoch:28,batch:839]:acc: 0.745313,loss:0.797728\n",
      "[epoch:28,batch:869]:acc: 0.744181,loss:0.800102\n",
      "[epoch:28,batch:899]:acc: 0.744132,loss:0.799933\n",
      "[epoch:28,batch:899]: val_loss:0.929143,val_acc:0.730778,val_total:4539\n",
      "[epoch:28,batch:929]:acc: 0.744926,loss:0.797943\n",
      "[epoch:28,batch:959]:acc: 0.745182,loss:0.797627\n",
      "[epoch:28,batch:989]:acc: 0.745170,loss:0.797705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:28] :acc: 0.745121,loss:0.798027,lr:0.000000,patience:1\n",
      "[epoch:28]: val_loss:0.945407,val_acc:0.731439,\n",
      "Epoch 29/59\n",
      "----------\n",
      "loss has increased lr divide 10 lr now is :0.000000\n",
      "[epoch:29,batch:29]:acc: 0.741667,loss:0.795646\n",
      "[epoch:29,batch:59]:acc: 0.755729,loss:0.784835\n",
      "[epoch:29,batch:89]:acc: 0.750694,loss:0.800290\n",
      "[epoch:29,batch:119]:acc: 0.745313,loss:0.803258\n",
      "[epoch:29,batch:149]:acc: 0.744583,loss:0.807013\n",
      "[epoch:29,batch:179]:acc: 0.746007,loss:0.801671\n",
      "[epoch:29,batch:209]:acc: 0.744048,loss:0.803684\n",
      "[epoch:29,batch:239]:acc: 0.742318,loss:0.801273\n",
      "[epoch:29,batch:269]:acc: 0.743866,loss:0.802330\n",
      "[epoch:29,batch:299]:acc: 0.743646,loss:0.802985\n",
      "[epoch:29,batch:299]: val_loss:0.849386,val_acc:0.745759,val_total:4539\n",
      "[epoch:29,batch:329]:acc: 0.745739,loss:0.797906\n",
      "[epoch:29,batch:359]:acc: 0.743490,loss:0.800988\n",
      "[epoch:29,batch:389]:acc: 0.743990,loss:0.802920\n",
      "[epoch:29,batch:419]:acc: 0.743973,loss:0.798545\n",
      "[epoch:29,batch:449]:acc: 0.743403,loss:0.798681\n",
      "[epoch:29,batch:479]:acc: 0.742904,loss:0.799483\n",
      "[epoch:29,batch:509]:acc: 0.743811,loss:0.798102\n",
      "[epoch:29,batch:539]:acc: 0.743866,loss:0.800632\n",
      "[epoch:29,batch:569]:acc: 0.743257,loss:0.803978\n",
      "[epoch:29,batch:599]:acc: 0.744219,loss:0.803651\n",
      "[epoch:29,batch:599]: val_loss:0.914412,val_acc:0.733201,val_total:4539\n",
      "[epoch:29,batch:629]:acc: 0.743800,loss:0.804372\n",
      "[epoch:29,batch:659]:acc: 0.743987,loss:0.804178\n",
      "[epoch:29,batch:689]:acc: 0.744067,loss:0.803928\n",
      "[epoch:29,batch:719]:acc: 0.743967,loss:0.804427\n",
      "[epoch:29,batch:749]:acc: 0.742750,loss:0.806333\n",
      "[epoch:29,batch:779]:acc: 0.743910,loss:0.805748\n",
      "[epoch:29,batch:809]:acc: 0.744213,loss:0.804342\n",
      "[epoch:29,batch:839]:acc: 0.744420,loss:0.803336\n",
      "[epoch:29,batch:869]:acc: 0.743930,loss:0.803906\n",
      "[epoch:29,batch:899]:acc: 0.743854,loss:0.803928\n",
      "[epoch:29,batch:899]: val_loss:0.927018,val_acc:0.727914,val_total:4539\n",
      "[epoch:29,batch:929]:acc: 0.743851,loss:0.804258\n",
      "[epoch:29,batch:959]:acc: 0.743197,loss:0.805804\n",
      "[epoch:29,batch:989]:acc: 0.743340,loss:0.805762\n",
      "[epoch:29] :acc: 0.743261,loss:0.806368,lr:0.000000,patience:0\n",
      "[epoch:29]: val_loss:0.923867,val_acc:0.732320,\n",
      "Epoch 30/59\n",
      "----------\n",
      "[epoch:30,batch:29]:acc: 0.746875,loss:0.827198\n",
      "[epoch:30,batch:59]:acc: 0.734896,loss:0.825299\n",
      "[epoch:30,batch:89]:acc: 0.740972,loss:0.813344\n",
      "[epoch:30,batch:119]:acc: 0.744010,loss:0.807433\n",
      "[epoch:30,batch:149]:acc: 0.745000,loss:0.808587\n",
      "[epoch:30,batch:179]:acc: 0.749306,loss:0.798458\n",
      "[epoch:30,batch:209]:acc: 0.747917,loss:0.809340\n",
      "[epoch:30,batch:239]:acc: 0.749349,loss:0.806888\n",
      "[epoch:30,batch:269]:acc: 0.749537,loss:0.807270\n",
      "[epoch:30,batch:299]:acc: 0.746667,loss:0.809427\n",
      "[epoch:30,batch:299]: val_loss:0.858356,val_acc:0.744878,val_total:4539\n",
      "[epoch:30,batch:329]:acc: 0.748390,loss:0.802978\n",
      "[epoch:30,batch:359]:acc: 0.747656,loss:0.803848\n",
      "[epoch:30,batch:389]:acc: 0.747356,loss:0.803569\n",
      "[epoch:30,batch:419]:acc: 0.746726,loss:0.803435\n",
      "[epoch:30,batch:449]:acc: 0.747083,loss:0.802599\n",
      "[epoch:30,batch:479]:acc: 0.746419,loss:0.803863\n",
      "[epoch:30,batch:509]:acc: 0.746752,loss:0.806524\n",
      "[epoch:30,batch:539]:acc: 0.746470,loss:0.808523\n",
      "[epoch:30,batch:569]:acc: 0.747039,loss:0.808210\n",
      "[epoch:30,batch:599]:acc: 0.746563,loss:0.808046\n",
      "[epoch:30,batch:599]: val_loss:0.925296,val_acc:0.732320,val_total:4539\n",
      "[epoch:30,batch:629]:acc: 0.746329,loss:0.808559\n",
      "[epoch:30,batch:659]:acc: 0.746354,loss:0.807665\n",
      "[epoch:30,batch:689]:acc: 0.745380,loss:0.808493\n",
      "[epoch:30,batch:719]:acc: 0.744575,loss:0.808488\n",
      "[epoch:30,batch:749]:acc: 0.745417,loss:0.806561\n",
      "[epoch:30,batch:779]:acc: 0.744631,loss:0.807190\n",
      "[epoch:30,batch:809]:acc: 0.744907,loss:0.807098\n",
      "[epoch:30,batch:839]:acc: 0.744754,loss:0.809049\n",
      "[epoch:30,batch:869]:acc: 0.744289,loss:0.809274\n",
      "[epoch:30,batch:899]:acc: 0.744410,loss:0.807967\n",
      "[epoch:30,batch:899]: val_loss:0.890389,val_acc:0.734964,val_total:4539\n",
      "[epoch:30,batch:929]:acc: 0.745262,loss:0.805944\n",
      "[epoch:30,batch:959]:acc: 0.744694,loss:0.806264\n",
      "[epoch:30,batch:989]:acc: 0.744634,loss:0.804411\n",
      "[epoch:30] :acc: 0.744680,loss:0.804388,lr:0.000000,patience:1\n",
      "[epoch:30]: val_loss:0.934671,val_acc:0.737167,\n",
      "Epoch 31/59\n",
      "----------\n",
      "loss has increased lr divide 10 lr now is :0.000000\n",
      "[epoch:31,batch:29]:acc: 0.738542,loss:0.781527\n",
      "[epoch:31,batch:59]:acc: 0.732812,loss:0.786395\n",
      "[epoch:31,batch:89]:acc: 0.739236,loss:0.795997\n",
      "[epoch:31,batch:119]:acc: 0.741146,loss:0.809875\n",
      "[epoch:31,batch:149]:acc: 0.746875,loss:0.796189\n",
      "[epoch:31,batch:179]:acc: 0.747222,loss:0.786222\n",
      "[epoch:31,batch:209]:acc: 0.744643,loss:0.791801\n",
      "[epoch:31,batch:239]:acc: 0.744271,loss:0.796056\n",
      "[epoch:31,batch:269]:acc: 0.745023,loss:0.793513\n",
      "[epoch:31,batch:299]:acc: 0.742917,loss:0.801241\n",
      "[epoch:31,batch:299]: val_loss:0.958997,val_acc:0.724829,val_total:4539\n",
      "[epoch:31,batch:329]:acc: 0.741383,loss:0.803292\n",
      "[epoch:31,batch:359]:acc: 0.741667,loss:0.805413\n",
      "[epoch:31,batch:389]:acc: 0.741026,loss:0.806647\n",
      "[epoch:31,batch:419]:acc: 0.739137,loss:0.809494\n",
      "[epoch:31,batch:449]:acc: 0.740000,loss:0.808937\n",
      "[epoch:31,batch:479]:acc: 0.738997,loss:0.812040\n",
      "[epoch:31,batch:509]:acc: 0.741667,loss:0.808072\n",
      "[epoch:31,batch:539]:acc: 0.741493,loss:0.809486\n",
      "[epoch:31,batch:569]:acc: 0.742160,loss:0.806540\n",
      "[epoch:31,batch:599]:acc: 0.741667,loss:0.806764\n",
      "[epoch:31,batch:599]: val_loss:0.870045,val_acc:0.739590,val_total:4539\n",
      "[epoch:31,batch:629]:acc: 0.740675,loss:0.808176\n",
      "[epoch:31,batch:659]:acc: 0.742235,loss:0.805373\n",
      "[epoch:31,batch:689]:acc: 0.742391,loss:0.803756\n",
      "[epoch:31,batch:719]:acc: 0.742231,loss:0.804177\n",
      "[epoch:31,batch:749]:acc: 0.742208,loss:0.804025\n",
      "[epoch:31,batch:779]:acc: 0.742788,loss:0.801339\n",
      "[epoch:31,batch:809]:acc: 0.743094,loss:0.799871\n",
      "[epoch:31,batch:839]:acc: 0.743787,loss:0.797853\n",
      "[epoch:31,batch:869]:acc: 0.744361,loss:0.796733\n",
      "[epoch:31,batch:899]:acc: 0.744653,loss:0.796271\n",
      "[epoch:31,batch:899]: val_loss:0.917536,val_acc:0.734964,val_total:4539\n",
      "[epoch:31,batch:929]:acc: 0.744724,loss:0.796758\n",
      "[epoch:31,batch:959]:acc: 0.745378,loss:0.795400\n",
      "[epoch:31,batch:989]:acc: 0.744918,loss:0.795879\n",
      "[epoch:31] :acc: 0.744774,loss:0.796574,lr:0.000000,patience:0\n",
      "[epoch:31]: val_loss:0.901516,val_acc:0.736065,\n",
      "Epoch 32/59\n",
      "----------\n",
      "[epoch:32,batch:29]:acc: 0.759375,loss:0.741001\n",
      "[epoch:32,batch:59]:acc: 0.764062,loss:0.740588\n",
      "[epoch:32,batch:89]:acc: 0.755208,loss:0.754582\n",
      "[epoch:32,batch:119]:acc: 0.750000,loss:0.770029\n",
      "[epoch:32,batch:149]:acc: 0.750417,loss:0.761063\n",
      "[epoch:32,batch:179]:acc: 0.748264,loss:0.773310\n",
      "[epoch:32,batch:209]:acc: 0.746577,loss:0.773187\n",
      "[epoch:32,batch:239]:acc: 0.748828,loss:0.772585\n",
      "[epoch:32,batch:269]:acc: 0.747685,loss:0.780260\n",
      "[epoch:32,batch:299]:acc: 0.745625,loss:0.785641\n",
      "[epoch:32,batch:299]: val_loss:0.851313,val_acc:0.741132,val_total:4539\n",
      "[epoch:32,batch:329]:acc: 0.743845,loss:0.793507\n",
      "[epoch:32,batch:359]:acc: 0.743576,loss:0.791928\n",
      "[epoch:32,batch:389]:acc: 0.744311,loss:0.791128\n",
      "[epoch:32,batch:419]:acc: 0.745461,loss:0.787401\n",
      "[epoch:32,batch:449]:acc: 0.745208,loss:0.787731\n",
      "[epoch:32,batch:479]:acc: 0.745638,loss:0.787539\n",
      "[epoch:32,batch:509]:acc: 0.745527,loss:0.789676\n",
      "[epoch:32,batch:539]:acc: 0.746238,loss:0.787138\n",
      "[epoch:32,batch:569]:acc: 0.745779,loss:0.788406\n",
      "[epoch:32,batch:599]:acc: 0.746458,loss:0.788480\n",
      "[epoch:32,batch:599]: val_loss:1.024643,val_acc:0.723728,val_total:4539\n",
      "[epoch:32,batch:629]:acc: 0.747966,loss:0.787489\n",
      "[epoch:32,batch:659]:acc: 0.748580,loss:0.784302\n",
      "[epoch:32,batch:689]:acc: 0.748913,loss:0.784428\n",
      "[epoch:32,batch:719]:acc: 0.749479,loss:0.786006\n",
      "[epoch:32,batch:749]:acc: 0.750000,loss:0.786671\n"
     ]
    }
   ],
   "source": [
    "train(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reuseTrain(path,epochNum):\n",
    "    writer=SummaryWriter('log/'+date+'/InceptionV4/') # 创建 /log/日期/InceptionResnet的组织形式\n",
    "    train_dataset=MyDataSet(json_Description=ANNOTATION_TRAIN,transform=preprocess_with_augmentation(normalize_05,IMAGE_SIZE),path_pre=IMAGE_TRAIN_PRE)\n",
    "    val_dataset=MyDataSet(json_Description=ANNOTATION_VAL,transform=preprocess(normalize_05,IMAGE_SIZE),path_pre=IMAGE_VAL_PRE)\n",
    "    train_dataLoader=DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE,num_workers=16,shuffle=True)\n",
    "    val_dataLoader=DataLoader(dataset=val_dataset,batch_size=BATCH_SIZE,num_workers=1,shuffle=False)\n",
    "    model=getmodel()\n",
    "    criterion=nn.CrossEntropyLoss().cuda()\n",
    "    modelParams=torch.load(path)\n",
    "    model.load_state_dict(modelParams['state_dict'])\n",
    "    min_loss=modelParams['val_loss']\n",
    "    print('min_loss is :%f'%(min_loss))   \n",
    "    min_acc=max(modelParams['val_correct'],0.81)\n",
    "    print('val_correct is %f'%(min_acc))\n",
    "    patience=0\n",
    "    lr=1e-4\n",
    "    momentum=0.9\n",
    "    beginepoch=modelParams['epoch']\n",
    "    for epoch in range(beginepoch,epochNum):\n",
    "        print('Epoch {}/{}'.format(epoch, epochNum - 1))\n",
    "        print('-' * 10)\n",
    "        if patience==2:\n",
    "            patience=0\n",
    "            model.load_state_dict(torch.load('../model/InceptionV4/'+date+'_loss_best.pth')['state_dict'])\n",
    "            lr=lr/10\n",
    "        optimizer=torch.optim.SGD(params=model.parameters(),lr=lr,momentum=momentum,weight_decay=1e-4)\n",
    "        print('lr now is %f'%(lr))\n",
    "        print('now patience is %d '%(patience))\n",
    "        running_loss = RunningMean()\n",
    "        running_corrects = RunningMean()\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_dataLoader):\n",
    "            model.train(True)\n",
    "            n_batchsize=inputs.size(0)\n",
    "            inputs = Variable(inputs).cuda()\n",
    "            labels = Variable(labels).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            if isinstance(outputs,tuple):\n",
    "                loss=sum((criterion(o,labels)) for o in outputs)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "            running_loss.update(loss.item(),1)\n",
    "            running_corrects.update(torch.sum(preds == labels.data).data,n_batchsize)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx%30==29:\n",
    "                print('[epoch:%d,batch:%d]:acc: %f,loss:%f'%(epoch,batch_idx,running_corrects.value,running_loss.value))\n",
    "                if batch_idx%300==299: \n",
    "                    niter = epoch * len(train_dataset)/BATCH_SIZE + batch_idx\n",
    "                    writer.add_scalar('Train/Acc',running_corrects.value,niter)\n",
    "                    writer.add_scalar('Train/Loss',running_loss.value,niter)\n",
    "                    lx,px=utils.predict(model,val_dataLoader)\n",
    "                    log_loss = criterion(Variable(px), Variable(lx))\n",
    "                    log_loss = log_loss.item()\n",
    "                    _, preds = torch.max(px, dim=1)\n",
    "                    accuracy = torch.mean((preds == lx).float())\n",
    "                    writer.add_scalar('Val/Acc',accuracy,niter)\n",
    "                    writer.add_scalar('Val/Loss',log_loss,niter)\n",
    "                    print('[epoch:%d,batch:%d]: val_loss:%f,val_acc:%f,val_total:%d'%(epoch,batch_idx,log_loss,accuracy,len(val_dataset)))\n",
    "        print('[epoch:%d] :acc: %f,loss:%f,lr:%f,patience:%d'%(epoch,running_corrects.value,running_loss.value,lr,patience))      \n",
    "        lx,px=utils.predict(model,val_dataLoader)\n",
    "        log_loss = criterion(Variable(px), Variable(lx))\n",
    "        log_loss = log_loss.item()\n",
    "        _, preds = torch.max(px, dim=1)\n",
    "        accuracy = torch.mean((preds == lx).float())\n",
    "        writer.add_scalar('Val/Acc',accuracy,(epoch+1) * len(train_dataset)/BATCH_SIZE)\n",
    "        writer.add_scalar('Val/Loss',log_loss,(epoch+1) * len(train_dataset)/BATCH_SIZE)\n",
    "        print('[epoch:%d]: val_loss:%f,val_acc:%f,'%(epoch,log_loss,accuracy))\n",
    "        if  log_loss < min_loss:\n",
    "            utils.snapshot('../model/', 'InceptionV4', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy })          \n",
    "            patience = 0\n",
    "            min_loss=log_loss\n",
    "            print('save new model loss,now loss is ',min_loss)\n",
    "        else:\n",
    "            patience += 1\n",
    "        if accuracy>min_acc:\n",
    "            utils.snapshot('../model/', 'InceptionV4', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy },key='acc') \n",
    "            min_acc=accuracy\n",
    "            print('save new model acc,now acc is ',min_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] loading model... Done\n",
      "min_loss is :0.541180\n",
      "val_correct is 0.814937\n",
      "Epoch 23/59\n",
      "----------\n",
      "lr now is 0.000100\n",
      "now patience is 0 \n",
      "[epoch:23,batch:29]:acc: 0.927083,loss:0.220210\n",
      "[epoch:23,batch:59]:acc: 0.921875,loss:0.205425\n",
      "[epoch:23,batch:89]:acc: 0.920833,loss:0.210347\n",
      "[epoch:23,batch:119]:acc: 0.921615,loss:0.212755\n",
      "[epoch:23,batch:149]:acc: 0.922500,loss:0.209653\n",
      "[epoch:23,batch:179]:acc: 0.922743,loss:0.209175\n",
      "[epoch:23,batch:209]:acc: 0.923512,loss:0.206459\n",
      "[epoch:23,batch:239]:acc: 0.924479,loss:0.204577\n",
      "[epoch:23,batch:269]:acc: 0.925347,loss:0.202524\n",
      "[epoch:23,batch:299]:acc: 0.927396,loss:0.200184\n",
      "[epoch:23,batch:299]: val_loss:0.466941,val_acc:0.826173,val_total:4539\n",
      "[epoch:23,batch:329]:acc: 0.929072,loss:0.197999\n",
      "[epoch:23,batch:359]:acc: 0.929167,loss:0.197435\n",
      "[epoch:23,batch:389]:acc: 0.930128,loss:0.195921\n",
      "[epoch:23,batch:419]:acc: 0.930655,loss:0.195363\n",
      "[epoch:23,batch:449]:acc: 0.932361,loss:0.192986\n",
      "[epoch:23,batch:479]:acc: 0.932943,loss:0.191181\n"
     ]
    }
   ],
   "source": [
    "reuseTrain('../model/InceptionV4/2018-11-01_acc_best.pth',60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainWithRawData(path,epochNum):\n",
    "    train_dataset=MyDataSet(json_Description=ANNOTATION_TRAIN,transform=preprocess(normalize_05,IMAGE_SIZE),path_pre=IMAGE_TRAIN_PRE)\n",
    "    val_dataset=MyDataSet(json_Description=ANNOTATION_VAL,transform=preprocess(normalize_05,IMAGE_SIZE),path_pre=IMAGE_VAL_PRE)\n",
    "    train_dataLoader=DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE,num_workers=16,shuffle=True)\n",
    "    val_dataLoader=DataLoader(dataset=val_dataset,batch_size=BATCH_SIZE,num_workers=1,shuffle=False)\n",
    "    model=getmodel()\n",
    "    criterion=nn.CrossEntropyLoss().cuda()\n",
    "    modelParams=torch.load(path)\n",
    "    model.load_state_dict(modelParams['state_dict'])\n",
    "    min_loss=modelParams['val_loss']\n",
    "    print('min_loss is :%f'%(min_loss))\n",
    "    print('val_correct is %f'%(modelParams['val_correct']))\n",
    "    min_acc=max(modelParams['val_correct'],0.81)\n",
    "    optinizerSave=modelParams['optimizer']\n",
    "    patience=0\n",
    "    lr=1e-4\n",
    "    momentum=0.9\n",
    "    beginepoch=modelParams['epoch']\n",
    "    for epoch in range(beginepoch,epochNum):\n",
    "        print('Epoch {}/{}'.format(epoch, epochNum - 1))\n",
    "        print('-' * 10)\n",
    "        if patience==3:\n",
    "            patience=0\n",
    "            model.load_state_dict(torch.load('../model/InceptionV4/'+date+'_loss_best.pth')['state_dict'])\n",
    "            lr=lr/5\n",
    "            print('lr desencd')\n",
    "        if epoch==beginepoch:\n",
    "            optimizer=torch.optim.SGD(params=model.parameters(),lr=lr,momentum=momentum)\n",
    "#             optimizer.load_state_dict(optinizerSave)\n",
    "#             lr=optimizer['lr']\n",
    "#             momentum=optimizer['momentum']\n",
    "            print('begin lr is ',lr)\n",
    "            \n",
    "        else:\n",
    "            optimizer=torch.optim.SGD(params=model.parameters(),lr=lr,momentum=momentum)\n",
    "                   \n",
    "        running_loss = RunningMean()\n",
    "        running_corrects = RunningMean()\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_dataLoader):\n",
    "            model.train(True)\n",
    "            n_batchsize=inputs.size(0)\n",
    "            inputs = Variable(inputs).cuda()\n",
    "            labels = Variable(labels).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            if isinstance(outputs,tuple):\n",
    "                loss=sum((criterion(o,labels)) for o in outputs)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "            running_loss.update(loss.item(),1)\n",
    "            running_corrects.update(torch.sum(preds == labels.data).data,n_batchsize)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx%30==29:\n",
    "                print('[epoch:%d,batch:%d]:acc: %f,loss:%f'%(epoch,batch_idx,running_corrects.value,running_loss.value))\n",
    "                if batch_idx%300==299: \n",
    "                    niter = epoch * len(train_dataset)/BATCH_SIZE + batch_idx\n",
    "                    lx,px=utils.predict(model,val_dataLoader)\n",
    "                    log_loss = criterion(Variable(px), Variable(lx))\n",
    "                    log_loss = log_loss.item()\n",
    "                    _, preds = torch.max(px, dim=1)\n",
    "                    accuracy = torch.mean((preds == lx).float())\n",
    "                    print('[epoch:%d,batch:%d]: val_loss:%f,val_acc:%f,val_total:%d'%(epoch,batch_idx,log_loss,accuracy,len(val_dataset)))\n",
    "                    if  log_loss < min_loss:\n",
    "                        utils.snapshot('../model/', 'InceptionV4', {\n",
    "                               'epoch': epoch + 1,\n",
    "                               'state_dict': model.state_dict(),\n",
    "                               'optimizer': optimizer.state_dict(),\n",
    "                               'val_loss': log_loss,\n",
    "                               'val_correct':accuracy })          \n",
    "\n",
    "                        min_loss=log_loss\n",
    "                        patience=0\n",
    "                        print('save new model loss,now loss is ',min_loss)\n",
    "\n",
    "                    if accuracy>min_acc:\n",
    "                        utils.snapshot('../model/', 'InceptionV4', {\n",
    "                               'epoch': epoch + 1,\n",
    "                               'state_dict': model.state_dict(),\n",
    "                               'optimizer': optimizer.state_dict(),\n",
    "                               'val_loss': log_loss,\n",
    "                               'val_correct':accuracy },key='acc') \n",
    "                        min_acc=accuracy\n",
    "                        print('save new model acc,now acc is ',min_acc)\n",
    "        print('[epoch:%d] :acc: %f,loss:%f,lr:%f,patience:%d'%(epoch,running_corrects.value,running_loss.value,lr,patience))         \n",
    "        lx,px=utils.predict(model,val_dataLoader)\n",
    "        log_loss = criterion(Variable(px), Variable(lx))\n",
    "        log_loss = log_loss.item()\n",
    "        _, preds = torch.max(px, dim=1)\n",
    "        accuracy = torch.mean((preds == lx).float())\n",
    "        print('[epoch:%d]: val_loss:%f,val_acc:%f,'%(epoch,log_loss,accuracy))\n",
    "        if  log_loss < min_loss:\n",
    "            utils.snapshot('../model/', 'InceptionV4', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy })          \n",
    "            patience = 0\n",
    "            min_loss=log_loss\n",
    "            print('save new model loss,now loss is ',min_loss)\n",
    "        else:\n",
    "            patience += 1\n",
    "        if accuracy>min_acc:\n",
    "            utils.snapshot('../model/', 'InceptionV4', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy },key='acc') \n",
    "            min_acc=accuracy\n",
    "            print('save new model acc,now acc is ',min_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] loading model... Done\n",
      "min_loss is :0.541180\n",
      "val_correct is 0.814937\n",
      "Epoch 23/59\n",
      "----------\n",
      "begin lr is  0.0001\n",
      "[epoch:23,batch:29]:acc: 0.866667,loss:0.343528\n",
      "[epoch:23,batch:59]:acc: 0.875000,loss:0.314108\n",
      "[epoch:23,batch:89]:acc: 0.873264,loss:0.315965\n",
      "[epoch:23,batch:119]:acc: 0.874219,loss:0.314024\n",
      "[epoch:23,batch:149]:acc: 0.874583,loss:0.311133\n",
      "[epoch:23,batch:179]:acc: 0.873785,loss:0.310046\n",
      "[epoch:23,batch:209]:acc: 0.874405,loss:0.306651\n",
      "[epoch:23,batch:239]:acc: 0.875391,loss:0.302885\n",
      "[epoch:23,batch:269]:acc: 0.875463,loss:0.303127\n",
      "[epoch:23,batch:299]:acc: 0.877083,loss:0.299272\n",
      "[epoch:23,batch:299]: val_loss:0.398825,val_acc:0.844900,val_total:4539\n",
      "save new model loss,now loss is  0.3988248109817505\n",
      "save new model acc,now acc is  tensor(0.8449, device='cuda:0')\n",
      "[epoch:23,batch:329]:acc: 0.876420,loss:0.297243\n",
      "[epoch:23,batch:359]:acc: 0.877691,loss:0.296511\n",
      "[epoch:23,batch:389]:acc: 0.879567,loss:0.293852\n",
      "[epoch:23,batch:419]:acc: 0.880580,loss:0.292931\n",
      "[epoch:23,batch:449]:acc: 0.882014,loss:0.289945\n",
      "[epoch:23,batch:479]:acc: 0.883073,loss:0.287419\n",
      "[epoch:23,batch:509]:acc: 0.883027,loss:0.287336\n",
      "[epoch:23,batch:539]:acc: 0.883565,loss:0.285712\n",
      "[epoch:23,batch:569]:acc: 0.883827,loss:0.285255\n",
      "[epoch:23,batch:599]:acc: 0.884323,loss:0.283937\n",
      "[epoch:23,batch:599]: val_loss:0.395252,val_acc:0.844459,val_total:4539\n",
      "save new model loss,now loss is  0.3952517807483673\n",
      "[epoch:23,batch:629]:acc: 0.884524,loss:0.283721\n",
      "[epoch:23,batch:659]:acc: 0.885275,loss:0.281586\n",
      "[epoch:23,batch:689]:acc: 0.885190,loss:0.281300\n",
      "[epoch:23,batch:719]:acc: 0.885113,loss:0.280919\n",
      "[epoch:23,batch:749]:acc: 0.885708,loss:0.279568\n",
      "[epoch:23,batch:779]:acc: 0.886018,loss:0.278415\n",
      "[epoch:23,batch:809]:acc: 0.886304,loss:0.277391\n",
      "[epoch:23,batch:839]:acc: 0.886347,loss:0.277278\n",
      "[epoch:23,batch:869]:acc: 0.886566,loss:0.276643\n",
      "[epoch:23,batch:899]:acc: 0.886493,loss:0.276841\n",
      "[epoch:23,batch:899]: val_loss:0.391386,val_acc:0.850628,val_total:4539\n",
      "save new model loss,now loss is  0.3913855254650116\n",
      "save new model acc,now acc is  tensor(0.8506, device='cuda:0')\n",
      "[epoch:23,batch:929]:acc: 0.886996,loss:0.275729\n",
      "[epoch:23,batch:959]:acc: 0.886882,loss:0.275711\n",
      "[epoch:23,batch:989]:acc: 0.886995,loss:0.275449\n",
      "[epoch:23] :acc: 0.886843,loss:0.276061,lr:0.000100,patience:0\n",
      "[epoch:23]: val_loss:0.402770,val_acc:0.845340,\n",
      "Epoch 24/59\n",
      "----------\n",
      "[epoch:24,batch:29]:acc: 0.897917,loss:0.250476\n",
      "[epoch:24,batch:59]:acc: 0.902604,loss:0.244032\n",
      "[epoch:24,batch:89]:acc: 0.898958,loss:0.247414\n",
      "[epoch:24,batch:119]:acc: 0.895312,loss:0.255211\n",
      "[epoch:24,batch:149]:acc: 0.895417,loss:0.255062\n",
      "[epoch:24,batch:179]:acc: 0.896701,loss:0.251218\n",
      "[epoch:24,batch:209]:acc: 0.897470,loss:0.251052\n",
      "[epoch:24,batch:239]:acc: 0.898047,loss:0.250714\n",
      "[epoch:24,batch:269]:acc: 0.899769,loss:0.248876\n",
      "[epoch:24,batch:299]:acc: 0.899479,loss:0.248180\n",
      "[epoch:24,batch:299]: val_loss:0.385962,val_acc:0.851509,val_total:4539\n",
      "save new model loss,now loss is  0.38596194982528687\n",
      "save new model acc,now acc is  tensor(0.8515, device='cuda:0')\n",
      "[epoch:24,batch:329]:acc: 0.899148,loss:0.249325\n",
      "[epoch:24,batch:359]:acc: 0.899479,loss:0.247810\n",
      "[epoch:24,batch:389]:acc: 0.898878,loss:0.248199\n",
      "[epoch:24,batch:419]:acc: 0.897991,loss:0.249851\n",
      "[epoch:24,batch:449]:acc: 0.899722,loss:0.246953\n",
      "[epoch:24,batch:479]:acc: 0.900130,loss:0.246111\n",
      "[epoch:24,batch:509]:acc: 0.899326,loss:0.246586\n",
      "[epoch:24,batch:539]:acc: 0.899363,loss:0.246386\n",
      "[epoch:24,batch:569]:acc: 0.899287,loss:0.246554\n",
      "[epoch:24,batch:599]:acc: 0.899687,loss:0.245917\n",
      "[epoch:24,batch:599]: val_loss:0.386649,val_acc:0.851729,val_total:4539\n",
      "save new model acc,now acc is  tensor(0.8517, device='cuda:0')\n",
      "[epoch:24,batch:629]:acc: 0.899752,loss:0.246138\n",
      "[epoch:24,batch:659]:acc: 0.900426,loss:0.245162\n",
      "[epoch:24,batch:689]:acc: 0.900679,loss:0.244318\n",
      "[epoch:24,batch:719]:acc: 0.901172,loss:0.243491\n",
      "[epoch:24,batch:749]:acc: 0.901208,loss:0.242916\n",
      "[epoch:24,batch:779]:acc: 0.901482,loss:0.242493\n",
      "[epoch:24,batch:809]:acc: 0.901080,loss:0.242959\n",
      "[epoch:24,batch:839]:acc: 0.901451,loss:0.242039\n",
      "[epoch:24,batch:869]:acc: 0.902119,loss:0.241490\n",
      "[epoch:24,batch:899]:acc: 0.902639,loss:0.241147\n",
      "[epoch:24,batch:899]: val_loss:0.392931,val_acc:0.851729,val_total:4539\n",
      "[epoch:24,batch:929]:acc: 0.902722,loss:0.240414\n",
      "[epoch:24,batch:959]:acc: 0.902279,loss:0.240818\n",
      "[epoch:24,batch:989]:acc: 0.902146,loss:0.241341\n",
      "[epoch:24] :acc: 0.902166,loss:0.241738,lr:0.000100,patience:0\n",
      "[epoch:24]: val_loss:0.414278,val_acc:0.843358,\n",
      "Epoch 25/59\n",
      "----------\n",
      "[epoch:25,batch:29]:acc: 0.900000,loss:0.237217\n",
      "[epoch:25,batch:59]:acc: 0.903646,loss:0.238090\n",
      "[epoch:25,batch:89]:acc: 0.905556,loss:0.234645\n",
      "[epoch:25,batch:119]:acc: 0.905729,loss:0.234867\n",
      "[epoch:25,batch:149]:acc: 0.905833,loss:0.236922\n",
      "[epoch:25,batch:179]:acc: 0.905382,loss:0.236686\n",
      "[epoch:25,batch:209]:acc: 0.904167,loss:0.237482\n",
      "[epoch:25,batch:239]:acc: 0.903776,loss:0.239027\n",
      "[epoch:25,batch:269]:acc: 0.904051,loss:0.238834\n",
      "[epoch:25,batch:299]:acc: 0.903958,loss:0.237495\n",
      "[epoch:25,batch:299]: val_loss:0.388526,val_acc:0.850408,val_total:4539\n",
      "[epoch:25,batch:329]:acc: 0.905587,loss:0.235929\n",
      "[epoch:25,batch:359]:acc: 0.905903,loss:0.234735\n",
      "[epoch:25,batch:389]:acc: 0.904968,loss:0.237186\n",
      "[epoch:25,batch:419]:acc: 0.904315,loss:0.236399\n",
      "[epoch:25,batch:449]:acc: 0.904097,loss:0.236503\n",
      "[epoch:25,batch:479]:acc: 0.904492,loss:0.235575\n",
      "[epoch:25,batch:509]:acc: 0.905515,loss:0.235008\n",
      "[epoch:25,batch:539]:acc: 0.905324,loss:0.235235\n",
      "[epoch:25,batch:569]:acc: 0.905373,loss:0.235592\n",
      "[epoch:25,batch:599]:acc: 0.905312,loss:0.235798\n",
      "[epoch:25,batch:599]: val_loss:0.385285,val_acc:0.849526,val_total:4539\n",
      "save new model loss,now loss is  0.38528531789779663\n",
      "[epoch:25,batch:629]:acc: 0.905308,loss:0.235144\n",
      "[epoch:25,batch:659]:acc: 0.904782,loss:0.235280\n",
      "[epoch:25,batch:689]:acc: 0.904982,loss:0.234996\n",
      "[epoch:25,batch:719]:acc: 0.905295,loss:0.235046\n",
      "[epoch:25,batch:749]:acc: 0.905708,loss:0.234875\n",
      "[epoch:25,batch:779]:acc: 0.905329,loss:0.235463\n",
      "[epoch:25,batch:809]:acc: 0.905093,loss:0.235646\n",
      "[epoch:25,batch:839]:acc: 0.905618,loss:0.235384\n",
      "[epoch:25,batch:869]:acc: 0.905388,loss:0.235594\n",
      "[epoch:25,batch:899]:acc: 0.905278,loss:0.236064\n",
      "[epoch:25,batch:899]: val_loss:0.385502,val_acc:0.855915,val_total:4539\n",
      "save new model acc,now acc is  tensor(0.8559, device='cuda:0')\n",
      "[epoch:25,batch:929]:acc: 0.905208,loss:0.236186\n",
      "[epoch:25,batch:959]:acc: 0.905078,loss:0.236307\n",
      "[epoch:25,batch:989]:acc: 0.904766,loss:0.236818\n",
      "[epoch:25] :acc: 0.904657,loss:0.237685,lr:0.000100,patience:0\n",
      "[epoch:25]: val_loss:0.439460,val_acc:0.841595,\n",
      "Epoch 26/59\n",
      "----------\n",
      "[epoch:26,batch:29]:acc: 0.907292,loss:0.214901\n",
      "[epoch:26,batch:59]:acc: 0.919792,loss:0.200106\n",
      "[epoch:26,batch:89]:acc: 0.915625,loss:0.211018\n",
      "[epoch:26,batch:119]:acc: 0.914062,loss:0.217680\n",
      "[epoch:26,batch:149]:acc: 0.914167,loss:0.217594\n",
      "[epoch:26,batch:179]:acc: 0.913021,loss:0.221458\n",
      "[epoch:26,batch:209]:acc: 0.912649,loss:0.221915\n",
      "[epoch:26,batch:239]:acc: 0.913932,loss:0.221192\n",
      "[epoch:26,batch:269]:acc: 0.913773,loss:0.220568\n",
      "[epoch:26,batch:299]:acc: 0.913542,loss:0.219685\n",
      "[epoch:26,batch:299]: val_loss:0.382598,val_acc:0.855034,val_total:4539\n",
      "save new model loss,now loss is  0.38259798288345337\n",
      "[epoch:26,batch:329]:acc: 0.911364,loss:0.222951\n",
      "[epoch:26,batch:359]:acc: 0.912413,loss:0.221858\n",
      "[epoch:26,batch:389]:acc: 0.912340,loss:0.220679\n",
      "[epoch:26,batch:419]:acc: 0.912574,loss:0.220216\n",
      "[epoch:26,batch:449]:acc: 0.911667,loss:0.221258\n",
      "[epoch:26,batch:479]:acc: 0.911328,loss:0.221793\n",
      "[epoch:26,batch:509]:acc: 0.910907,loss:0.222636\n",
      "[epoch:26,batch:539]:acc: 0.910185,loss:0.223953\n",
      "[epoch:26,batch:569]:acc: 0.910417,loss:0.223377\n",
      "[epoch:26,batch:599]:acc: 0.910156,loss:0.223410\n",
      "[epoch:26,batch:599]: val_loss:0.390356,val_acc:0.852390,val_total:4539\n",
      "[epoch:26,batch:629]:acc: 0.910169,loss:0.223757\n",
      "[epoch:26,batch:659]:acc: 0.910511,loss:0.223204\n",
      "[epoch:26,batch:689]:acc: 0.910190,loss:0.223802\n",
      "[epoch:26,batch:719]:acc: 0.910417,loss:0.223264\n",
      "[epoch:26,batch:749]:acc: 0.910625,loss:0.222803\n",
      "[epoch:26,batch:779]:acc: 0.910897,loss:0.222314\n",
      "[epoch:26,batch:809]:acc: 0.911150,loss:0.222332\n",
      "[epoch:26,batch:839]:acc: 0.910975,loss:0.222018\n",
      "[epoch:26,batch:869]:acc: 0.911063,loss:0.222028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:26,batch:899]:acc: 0.911111,loss:0.222622\n",
      "[epoch:26,batch:899]: val_loss:0.387109,val_acc:0.852170,val_total:4539\n",
      "[epoch:26,batch:929]:acc: 0.911156,loss:0.222515\n",
      "[epoch:26,batch:959]:acc: 0.911068,loss:0.222613\n",
      "[epoch:26,batch:989]:acc: 0.911332,loss:0.222384\n",
      "[epoch:26] :acc: 0.911278,loss:0.224945,lr:0.000100,patience:0\n",
      "[epoch:26]: val_loss:0.423382,val_acc:0.842697,\n",
      "Epoch 27/59\n",
      "----------\n",
      "[epoch:27,batch:29]:acc: 0.936458,loss:0.176843\n",
      "[epoch:27,batch:59]:acc: 0.925521,loss:0.193973\n",
      "[epoch:27,batch:89]:acc: 0.919444,loss:0.200614\n",
      "[epoch:27,batch:119]:acc: 0.920052,loss:0.201676\n",
      "[epoch:27,batch:149]:acc: 0.916875,loss:0.207845\n",
      "[epoch:27,batch:179]:acc: 0.917535,loss:0.207062\n",
      "[epoch:27,batch:209]:acc: 0.918452,loss:0.205606\n",
      "[epoch:27,batch:239]:acc: 0.917057,loss:0.207392\n",
      "[epoch:27,batch:269]:acc: 0.917014,loss:0.207489\n",
      "[epoch:27,batch:299]:acc: 0.917292,loss:0.207714\n",
      "[epoch:27,batch:299]: val_loss:0.385085,val_acc:0.855475,val_total:4539\n",
      "[epoch:27,batch:329]:acc: 0.917898,loss:0.208150\n",
      "[epoch:27,batch:359]:acc: 0.918490,loss:0.208203\n",
      "[epoch:27,batch:389]:acc: 0.918269,loss:0.207583\n",
      "[epoch:27,batch:419]:acc: 0.918006,loss:0.208933\n",
      "[epoch:27,batch:449]:acc: 0.918611,loss:0.207316\n",
      "[epoch:27,batch:479]:acc: 0.919466,loss:0.206463\n",
      "[epoch:27,batch:509]:acc: 0.919118,loss:0.206304\n",
      "[epoch:27,batch:539]:acc: 0.918981,loss:0.205877\n",
      "[epoch:27,batch:569]:acc: 0.919134,loss:0.206492\n",
      "[epoch:27,batch:599]:acc: 0.918854,loss:0.207475\n",
      "[epoch:27,batch:599]: val_loss:0.384517,val_acc:0.855034,val_total:4539\n",
      "[epoch:27,batch:629]:acc: 0.918800,loss:0.206883\n",
      "[epoch:27,batch:659]:acc: 0.918845,loss:0.206442\n",
      "[epoch:27,batch:689]:acc: 0.918841,loss:0.206848\n",
      "[epoch:27,batch:719]:acc: 0.918446,loss:0.207613\n",
      "[epoch:27,batch:749]:acc: 0.918667,loss:0.206799\n",
      "[epoch:27,batch:779]:acc: 0.918870,loss:0.206490\n",
      "[epoch:27,batch:809]:acc: 0.918904,loss:0.206735\n",
      "[epoch:27,batch:839]:acc: 0.918899,loss:0.207357\n",
      "[epoch:27,batch:869]:acc: 0.918822,loss:0.207250\n",
      "[epoch:27,batch:899]:acc: 0.918194,loss:0.208182\n",
      "[epoch:27,batch:899]: val_loss:0.394418,val_acc:0.855254,val_total:4539\n",
      "[epoch:27,batch:929]:acc: 0.917910,loss:0.208042\n",
      "[epoch:27,batch:959]:acc: 0.917643,loss:0.208143\n",
      "[epoch:27,batch:989]:acc: 0.917361,loss:0.208653\n",
      "[epoch:27] :acc: 0.917300,loss:0.210006,lr:0.000100,patience:1\n",
      "[epoch:27]: val_loss:0.388472,val_acc:0.849306,\n",
      "Epoch 28/59\n",
      "----------\n",
      "[epoch:28,batch:29]:acc: 0.896875,loss:0.231600\n",
      "[epoch:28,batch:59]:acc: 0.913542,loss:0.204096\n",
      "[epoch:28,batch:89]:acc: 0.913194,loss:0.207561\n",
      "[epoch:28,batch:119]:acc: 0.917448,loss:0.202967\n",
      "[epoch:28,batch:149]:acc: 0.918125,loss:0.200134\n",
      "[epoch:28,batch:179]:acc: 0.919097,loss:0.199382\n",
      "[epoch:28,batch:209]:acc: 0.919792,loss:0.198101\n",
      "[epoch:28,batch:239]:acc: 0.919271,loss:0.200348\n",
      "[epoch:28,batch:269]:acc: 0.920023,loss:0.199486\n",
      "[epoch:28,batch:299]:acc: 0.919896,loss:0.200078\n",
      "[epoch:28,batch:299]: val_loss:0.382600,val_acc:0.857678,val_total:4539\n",
      "save new model acc,now acc is  tensor(0.8577, device='cuda:0')\n",
      "[epoch:28,batch:329]:acc: 0.920076,loss:0.200597\n",
      "[epoch:28,batch:359]:acc: 0.920226,loss:0.201445\n",
      "[epoch:28,batch:389]:acc: 0.920353,loss:0.200765\n",
      "[epoch:28,batch:419]:acc: 0.920685,loss:0.200491\n",
      "[epoch:28,batch:449]:acc: 0.921250,loss:0.200423\n",
      "[epoch:28,batch:479]:acc: 0.921419,loss:0.199906\n",
      "[epoch:28,batch:509]:acc: 0.921998,loss:0.198456\n",
      "[epoch:28,batch:539]:acc: 0.921701,loss:0.198691\n",
      "[epoch:28,batch:569]:acc: 0.921162,loss:0.199403\n",
      "[epoch:28,batch:599]:acc: 0.921667,loss:0.198727\n",
      "[epoch:28,batch:599]: val_loss:0.385118,val_acc:0.854373,val_total:4539\n",
      "[epoch:28,batch:629]:acc: 0.922371,loss:0.198313\n",
      "[epoch:28,batch:659]:acc: 0.923011,loss:0.198320\n",
      "[epoch:28,batch:689]:acc: 0.922509,loss:0.198498\n",
      "[epoch:28,batch:719]:acc: 0.923177,loss:0.198267\n",
      "[epoch:28,batch:749]:acc: 0.923500,loss:0.197925\n",
      "[epoch:28,batch:779]:acc: 0.923518,loss:0.198062\n",
      "[epoch:28,batch:809]:acc: 0.923534,loss:0.198038\n",
      "[epoch:28,batch:839]:acc: 0.923512,loss:0.198461\n",
      "[epoch:28,batch:869]:acc: 0.923455,loss:0.198899\n",
      "[epoch:28,batch:899]:acc: 0.923160,loss:0.199128\n",
      "[epoch:28,batch:899]: val_loss:0.387030,val_acc:0.852831,val_total:4539\n",
      "[epoch:28,batch:929]:acc: 0.922917,loss:0.199124\n",
      "[epoch:28,batch:959]:acc: 0.922982,loss:0.199121\n",
      "[epoch:28,batch:989]:acc: 0.923674,loss:0.198012\n",
      "[epoch:28] :acc: 0.923732,loss:0.197894,lr:0.000100,patience:2\n",
      "[epoch:28]: val_loss:0.392427,val_acc:0.851068,\n",
      "Epoch 29/59\n",
      "----------\n",
      "lr desencd\n",
      "[epoch:29,batch:29]:acc: 0.916667,loss:0.219910\n",
      "[epoch:29,batch:59]:acc: 0.910937,loss:0.223954\n",
      "[epoch:29,batch:89]:acc: 0.911111,loss:0.217931\n",
      "[epoch:29,batch:119]:acc: 0.911458,loss:0.218808\n",
      "[epoch:29,batch:149]:acc: 0.912083,loss:0.218934\n",
      "[epoch:29,batch:179]:acc: 0.914410,loss:0.215660\n",
      "[epoch:29,batch:209]:acc: 0.913839,loss:0.214907\n",
      "[epoch:29,batch:239]:acc: 0.915365,loss:0.213785\n",
      "[epoch:29,batch:269]:acc: 0.914236,loss:0.214496\n",
      "[epoch:29,batch:299]:acc: 0.913021,loss:0.217369\n",
      "[epoch:29,batch:299]: val_loss:0.395413,val_acc:0.851289,val_total:4539\n",
      "[epoch:29,batch:329]:acc: 0.912405,loss:0.219348\n",
      "[epoch:29,batch:359]:acc: 0.912153,loss:0.220488\n",
      "[epoch:29,batch:389]:acc: 0.913381,loss:0.218533\n",
      "[epoch:29,batch:419]:acc: 0.913393,loss:0.218804\n",
      "[epoch:29,batch:449]:acc: 0.913681,loss:0.218464\n",
      "[epoch:29,batch:479]:acc: 0.913086,loss:0.219261\n",
      "[epoch:29,batch:509]:acc: 0.913235,loss:0.218641\n",
      "[epoch:29,batch:539]:acc: 0.912963,loss:0.218850\n",
      "[epoch:29,batch:569]:acc: 0.913980,loss:0.218297\n",
      "[epoch:29,batch:599]:acc: 0.914323,loss:0.217890\n",
      "[epoch:29,batch:599]: val_loss:0.383838,val_acc:0.857458,val_total:4539\n",
      "[epoch:29,batch:629]:acc: 0.914187,loss:0.218100\n",
      "[epoch:29,batch:659]:acc: 0.914347,loss:0.217791\n",
      "[epoch:29,batch:689]:acc: 0.913723,loss:0.218523\n",
      "[epoch:29,batch:719]:acc: 0.913585,loss:0.218964\n",
      "[epoch:29,batch:749]:acc: 0.914125,loss:0.218126\n",
      "[epoch:29,batch:779]:acc: 0.914944,loss:0.217025\n",
      "[epoch:29,batch:809]:acc: 0.915046,loss:0.216672\n",
      "[epoch:29,batch:839]:acc: 0.915067,loss:0.216484\n",
      "[epoch:29,batch:869]:acc: 0.914978,loss:0.216259\n",
      "[epoch:29,batch:899]:acc: 0.914826,loss:0.217129\n",
      "[epoch:29,batch:899]: val_loss:0.387159,val_acc:0.855034,val_total:4539\n",
      "[epoch:29,batch:929]:acc: 0.914617,loss:0.217380\n",
      "[epoch:29,batch:959]:acc: 0.914388,loss:0.217835\n",
      "[epoch:29,batch:989]:acc: 0.914362,loss:0.217589\n",
      "[epoch:29] :acc: 0.914305,loss:0.218062,lr:0.000020,patience:0\n",
      "[epoch:29]: val_loss:0.405121,val_acc:0.846662,\n",
      "Epoch 30/59\n",
      "----------\n",
      "[epoch:30,batch:29]:acc: 0.916667,loss:0.219164\n",
      "[epoch:30,batch:59]:acc: 0.911979,loss:0.219844\n",
      "[epoch:30,batch:89]:acc: 0.910069,loss:0.225893\n",
      "[epoch:30,batch:119]:acc: 0.910937,loss:0.227153\n",
      "[epoch:30,batch:149]:acc: 0.910417,loss:0.224773\n",
      "[epoch:30,batch:179]:acc: 0.910590,loss:0.225152\n",
      "[epoch:30,batch:209]:acc: 0.912500,loss:0.222674\n",
      "[epoch:30,batch:239]:acc: 0.913021,loss:0.220654\n",
      "[epoch:30,batch:269]:acc: 0.912500,loss:0.221514\n",
      "[epoch:30,batch:299]:acc: 0.912917,loss:0.220710\n",
      "[epoch:30,batch:299]: val_loss:0.385974,val_acc:0.854814,val_total:4539\n",
      "[epoch:30,batch:329]:acc: 0.912500,loss:0.219615\n",
      "[epoch:30,batch:359]:acc: 0.913021,loss:0.220030\n",
      "[epoch:30,batch:389]:acc: 0.913782,loss:0.219732\n",
      "[epoch:30,batch:419]:acc: 0.914062,loss:0.218497\n",
      "[epoch:30,batch:449]:acc: 0.914306,loss:0.218810\n",
      "[epoch:30,batch:479]:acc: 0.915039,loss:0.217542\n",
      "[epoch:30,batch:509]:acc: 0.914767,loss:0.217333\n",
      "[epoch:30,batch:539]:acc: 0.915278,loss:0.216605\n",
      "[epoch:30,batch:569]:acc: 0.915132,loss:0.216002\n",
      "[epoch:30,batch:599]:acc: 0.915156,loss:0.215725\n",
      "[epoch:30,batch:599]: val_loss:0.388498,val_acc:0.850848,val_total:4539\n",
      "[epoch:30,batch:629]:acc: 0.914782,loss:0.216563\n",
      "[epoch:30,batch:659]:acc: 0.915009,loss:0.215980\n",
      "[epoch:30,batch:689]:acc: 0.914538,loss:0.216802\n",
      "[epoch:30,batch:719]:acc: 0.914497,loss:0.217381\n",
      "[epoch:30,batch:749]:acc: 0.914958,loss:0.216598\n",
      "[epoch:30,batch:779]:acc: 0.914463,loss:0.216929\n",
      "[epoch:30,batch:809]:acc: 0.914236,loss:0.216748\n",
      "[epoch:30,batch:839]:acc: 0.913653,loss:0.217797\n",
      "[epoch:30,batch:869]:acc: 0.913290,loss:0.218200\n",
      "[epoch:30,batch:899]:acc: 0.913715,loss:0.217449\n",
      "[epoch:30,batch:899]: val_loss:0.387055,val_acc:0.854373,val_total:4539\n",
      "[epoch:30,batch:929]:acc: 0.914113,loss:0.216424\n",
      "[epoch:30,batch:959]:acc: 0.914193,loss:0.216467\n",
      "[epoch:30,batch:989]:acc: 0.913794,loss:0.216656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:30] :acc: 0.913800,loss:0.216876,lr:0.000020,patience:1\n",
      "[epoch:30]: val_loss:0.463799,val_acc:0.837189,\n",
      "Epoch 31/59\n",
      "----------\n",
      "[epoch:31,batch:29]:acc: 0.919792,loss:0.209406\n",
      "[epoch:31,batch:59]:acc: 0.918229,loss:0.208172\n",
      "[epoch:31,batch:89]:acc: 0.920486,loss:0.203706\n",
      "[epoch:31,batch:119]:acc: 0.921875,loss:0.199520\n",
      "[epoch:31,batch:149]:acc: 0.917708,loss:0.205363\n",
      "[epoch:31,batch:179]:acc: 0.918056,loss:0.205786\n",
      "[epoch:31,batch:209]:acc: 0.915625,loss:0.208784\n",
      "[epoch:31,batch:239]:acc: 0.916797,loss:0.207726\n",
      "[epoch:31,batch:269]:acc: 0.919560,loss:0.203543\n",
      "[epoch:31,batch:299]:acc: 0.919896,loss:0.203214\n",
      "[epoch:31,batch:299]: val_loss:0.391023,val_acc:0.852170,val_total:4539\n",
      "[epoch:31,batch:329]:acc: 0.918750,loss:0.206644\n",
      "[epoch:31,batch:359]:acc: 0.918316,loss:0.207482\n",
      "[epoch:31,batch:389]:acc: 0.918429,loss:0.208695\n",
      "[epoch:31,batch:419]:acc: 0.917857,loss:0.209926\n",
      "[epoch:31,batch:449]:acc: 0.917639,loss:0.210807\n",
      "[epoch:31,batch:479]:acc: 0.918099,loss:0.210252\n",
      "[epoch:31,batch:509]:acc: 0.918260,loss:0.210600\n",
      "[epoch:31,batch:539]:acc: 0.917593,loss:0.210959\n",
      "[epoch:31,batch:569]:acc: 0.916831,loss:0.212176\n",
      "[epoch:31,batch:599]:acc: 0.916250,loss:0.213032\n",
      "[epoch:31,batch:599]: val_loss:0.385186,val_acc:0.855034,val_total:4539\n",
      "[epoch:31,batch:629]:acc: 0.916220,loss:0.213831\n",
      "[epoch:31,batch:659]:acc: 0.916383,loss:0.213420\n",
      "[epoch:31,batch:689]:acc: 0.916123,loss:0.214050\n",
      "[epoch:31,batch:719]:acc: 0.915972,loss:0.214340\n",
      "[epoch:31,batch:749]:acc: 0.916125,loss:0.213848\n",
      "[epoch:31,batch:779]:acc: 0.916747,loss:0.212899\n",
      "[epoch:31,batch:809]:acc: 0.916782,loss:0.212832\n",
      "[epoch:31,batch:839]:acc: 0.916927,loss:0.212876\n",
      "[epoch:31,batch:869]:acc: 0.917349,loss:0.212379\n",
      "[epoch:31,batch:899]:acc: 0.916979,loss:0.212853\n",
      "[epoch:31,batch:899]: val_loss:0.384374,val_acc:0.854594,val_total:4539\n",
      "[epoch:31,batch:929]:acc: 0.917137,loss:0.212960\n",
      "[epoch:31,batch:959]:acc: 0.917253,loss:0.212951\n",
      "[epoch:31,batch:989]:acc: 0.917045,loss:0.213304\n",
      "[epoch:31] :acc: 0.916922,loss:0.214599,lr:0.000020,patience:2\n",
      "[epoch:31]: val_loss:0.472676,val_acc:0.838070,\n",
      "Epoch 32/59\n",
      "----------\n",
      "lr desencd\n",
      "[epoch:32,batch:29]:acc: 0.926042,loss:0.198349\n",
      "[epoch:32,batch:59]:acc: 0.924479,loss:0.204261\n",
      "[epoch:32,batch:89]:acc: 0.922222,loss:0.206632\n",
      "[epoch:32,batch:119]:acc: 0.921354,loss:0.206568\n",
      "[epoch:32,batch:149]:acc: 0.918125,loss:0.212047\n",
      "[epoch:32,batch:179]:acc: 0.914757,loss:0.215811\n",
      "[epoch:32,batch:209]:acc: 0.915179,loss:0.216408\n",
      "[epoch:32,batch:239]:acc: 0.914714,loss:0.215579\n",
      "[epoch:32,batch:269]:acc: 0.913194,loss:0.217122\n",
      "[epoch:32,batch:299]:acc: 0.913229,loss:0.215985\n",
      "[epoch:32,batch:299]: val_loss:0.387969,val_acc:0.853712,val_total:4539\n",
      "[epoch:32,batch:329]:acc: 0.913163,loss:0.215350\n",
      "[epoch:32,batch:359]:acc: 0.913715,loss:0.214430\n",
      "[epoch:32,batch:389]:acc: 0.912500,loss:0.216022\n",
      "[epoch:32,batch:419]:acc: 0.913095,loss:0.215854\n",
      "[epoch:32,batch:449]:acc: 0.913056,loss:0.216736\n",
      "[epoch:32,batch:479]:acc: 0.913411,loss:0.215172\n",
      "[epoch:32,batch:509]:acc: 0.913358,loss:0.215514\n",
      "[epoch:32,batch:539]:acc: 0.913542,loss:0.215180\n",
      "[epoch:32,batch:569]:acc: 0.912884,loss:0.215361\n",
      "[epoch:32,batch:599]:acc: 0.913125,loss:0.215204\n",
      "[epoch:32,batch:599]: val_loss:0.385972,val_acc:0.855475,val_total:4539\n",
      "[epoch:32,batch:629]:acc: 0.913442,loss:0.214516\n",
      "[epoch:32,batch:659]:acc: 0.913778,loss:0.214475\n",
      "[epoch:32,batch:689]:acc: 0.913995,loss:0.214355\n",
      "[epoch:32,batch:719]:acc: 0.914366,loss:0.214260\n",
      "[epoch:32,batch:749]:acc: 0.914458,loss:0.214393\n",
      "[epoch:32,batch:779]:acc: 0.914263,loss:0.215271\n",
      "[epoch:32,batch:809]:acc: 0.913889,loss:0.215914\n",
      "[epoch:32,batch:839]:acc: 0.913728,loss:0.216197\n",
      "[epoch:32,batch:869]:acc: 0.913147,loss:0.216757\n",
      "[epoch:32,batch:899]:acc: 0.913090,loss:0.217107\n",
      "[epoch:32,batch:899]: val_loss:0.386216,val_acc:0.856576,val_total:4539\n",
      "[epoch:32,batch:929]:acc: 0.913138,loss:0.217357\n",
      "[epoch:32,batch:959]:acc: 0.913118,loss:0.217690\n",
      "[epoch:32,batch:989]:acc: 0.913131,loss:0.217421\n",
      "[epoch:32] :acc: 0.913075,loss:0.217870,lr:0.000004,patience:0\n",
      "[epoch:32]: val_loss:0.499777,val_acc:0.834325,\n",
      "Epoch 33/59\n",
      "----------\n",
      "[epoch:33,batch:29]:acc: 0.923958,loss:0.202147\n",
      "[epoch:33,batch:59]:acc: 0.912500,loss:0.212523\n",
      "[epoch:33,batch:89]:acc: 0.915972,loss:0.206572\n",
      "[epoch:33,batch:119]:acc: 0.917708,loss:0.204711\n",
      "[epoch:33,batch:149]:acc: 0.916250,loss:0.210162\n",
      "[epoch:33,batch:179]:acc: 0.914583,loss:0.213630\n",
      "[epoch:33,batch:209]:acc: 0.912649,loss:0.215717\n",
      "[epoch:33,batch:239]:acc: 0.913021,loss:0.215655\n",
      "[epoch:33,batch:269]:acc: 0.914120,loss:0.214373\n",
      "[epoch:33,batch:299]:acc: 0.914062,loss:0.215814\n",
      "[epoch:33,batch:299]: val_loss:0.386102,val_acc:0.852170,val_total:4539\n",
      "[epoch:33,batch:329]:acc: 0.914299,loss:0.215400\n",
      "[epoch:33,batch:359]:acc: 0.913715,loss:0.216532\n",
      "[epoch:33,batch:389]:acc: 0.915385,loss:0.213904\n",
      "[epoch:33,batch:419]:acc: 0.914435,loss:0.214773\n",
      "[epoch:33,batch:449]:acc: 0.914722,loss:0.215301\n",
      "[epoch:33,batch:479]:acc: 0.914648,loss:0.215576\n",
      "[epoch:33,batch:509]:acc: 0.913909,loss:0.217076\n",
      "[epoch:33,batch:539]:acc: 0.914352,loss:0.216855\n",
      "[epoch:33,batch:569]:acc: 0.914035,loss:0.217497\n",
      "[epoch:33,batch:599]:acc: 0.913802,loss:0.218047\n",
      "[epoch:33,batch:599]: val_loss:0.386527,val_acc:0.854594,val_total:4539\n",
      "[epoch:33,batch:629]:acc: 0.913145,loss:0.219491\n",
      "[epoch:33,batch:659]:acc: 0.912642,loss:0.219358\n",
      "[epoch:33,batch:689]:acc: 0.913089,loss:0.218665\n",
      "[epoch:33,batch:719]:acc: 0.913672,loss:0.218339\n",
      "[epoch:33,batch:749]:acc: 0.913083,loss:0.219435\n",
      "[epoch:33,batch:779]:acc: 0.912861,loss:0.219494\n",
      "[epoch:33,batch:809]:acc: 0.912886,loss:0.219447\n",
      "[epoch:33,batch:839]:acc: 0.913170,loss:0.219382\n",
      "[epoch:33,batch:869]:acc: 0.913182,loss:0.219599\n",
      "[epoch:33,batch:899]:acc: 0.913681,loss:0.218671\n",
      "[epoch:33,batch:899]: val_loss:0.392887,val_acc:0.852611,val_total:4539\n",
      "[epoch:33,batch:929]:acc: 0.914113,loss:0.218470\n",
      "[epoch:33,batch:959]:acc: 0.914128,loss:0.218186\n",
      "[epoch:33,batch:989]:acc: 0.914457,loss:0.217758\n",
      "[epoch:33] :acc: 0.914462,loss:0.217855,lr:0.000004,patience:1\n",
      "[epoch:33]: val_loss:0.394953,val_acc:0.854373,\n",
      "Epoch 34/59\n",
      "----------\n",
      "[epoch:34,batch:29]:acc: 0.919792,loss:0.216171\n",
      "[epoch:34,batch:59]:acc: 0.918229,loss:0.220775\n",
      "[epoch:34,batch:89]:acc: 0.916667,loss:0.225237\n",
      "[epoch:34,batch:119]:acc: 0.918229,loss:0.220649\n",
      "[epoch:34,batch:149]:acc: 0.916667,loss:0.217248\n",
      "[epoch:34,batch:179]:acc: 0.913889,loss:0.219489\n",
      "[epoch:34,batch:209]:acc: 0.914137,loss:0.220842\n",
      "[epoch:34,batch:239]:acc: 0.914714,loss:0.219270\n",
      "[epoch:34,batch:269]:acc: 0.912731,loss:0.220381\n",
      "[epoch:34,batch:299]:acc: 0.913438,loss:0.220327\n",
      "[epoch:34,batch:299]: val_loss:0.386622,val_acc:0.855695,val_total:4539\n",
      "[epoch:34,batch:329]:acc: 0.914110,loss:0.218571\n",
      "[epoch:34,batch:359]:acc: 0.914410,loss:0.217459\n",
      "[epoch:34,batch:389]:acc: 0.914423,loss:0.217112\n",
      "[epoch:34,batch:419]:acc: 0.914955,loss:0.215357\n",
      "[epoch:34,batch:449]:acc: 0.915208,loss:0.215136\n",
      "[epoch:34,batch:479]:acc: 0.915951,loss:0.214910\n",
      "[epoch:34,batch:509]:acc: 0.916115,loss:0.214817\n",
      "[epoch:34,batch:539]:acc: 0.916377,loss:0.214134\n",
      "[epoch:34,batch:569]:acc: 0.916173,loss:0.214106\n",
      "[epoch:34,batch:599]:acc: 0.915521,loss:0.214527\n",
      "[epoch:34,batch:599]: val_loss:0.384227,val_acc:0.851950,val_total:4539\n",
      "[epoch:34,batch:629]:acc: 0.914435,loss:0.215589\n",
      "[epoch:34,batch:659]:acc: 0.914489,loss:0.215983\n",
      "[epoch:34,batch:689]:acc: 0.915127,loss:0.215451\n",
      "[epoch:34,batch:719]:acc: 0.914887,loss:0.216270\n",
      "[epoch:34,batch:749]:acc: 0.915458,loss:0.215580\n",
      "[epoch:34,batch:779]:acc: 0.915064,loss:0.215875\n",
      "[epoch:34,batch:809]:acc: 0.915586,loss:0.215412\n",
      "[epoch:34,batch:839]:acc: 0.915402,loss:0.215416\n",
      "[epoch:34,batch:869]:acc: 0.915338,loss:0.215851\n",
      "[epoch:34,batch:899]:acc: 0.915139,loss:0.216557\n",
      "[epoch:34,batch:899]: val_loss:0.382637,val_acc:0.854814,val_total:4539\n",
      "[epoch:34,batch:929]:acc: 0.915591,loss:0.216139\n",
      "[epoch:34,batch:959]:acc: 0.915592,loss:0.216263\n",
      "[epoch:34,batch:989]:acc: 0.915751,loss:0.215855\n",
      "[epoch:34] :acc: 0.915723,loss:0.216119,lr:0.000004,patience:2\n",
      "[epoch:34]: val_loss:0.612039,val_acc:0.826614,\n",
      "Epoch 35/59\n",
      "----------\n",
      "lr desencd\n",
      "[epoch:35,batch:29]:acc: 0.915625,loss:0.216597\n",
      "[epoch:35,batch:59]:acc: 0.920833,loss:0.212883\n",
      "[epoch:35,batch:89]:acc: 0.916667,loss:0.215753\n",
      "[epoch:35,batch:119]:acc: 0.911719,loss:0.224986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:35,batch:149]:acc: 0.910625,loss:0.226631\n",
      "[epoch:35,batch:179]:acc: 0.908507,loss:0.228585\n",
      "[epoch:35,batch:209]:acc: 0.909524,loss:0.225421\n",
      "[epoch:35,batch:239]:acc: 0.911068,loss:0.223978\n",
      "[epoch:35,batch:269]:acc: 0.911343,loss:0.221882\n",
      "[epoch:35,batch:299]:acc: 0.913958,loss:0.220089\n",
      "[epoch:35,batch:299]: val_loss:0.385963,val_acc:0.853933,val_total:4539\n",
      "[epoch:35,batch:329]:acc: 0.912500,loss:0.222044\n",
      "[epoch:35,batch:359]:acc: 0.913542,loss:0.219778\n",
      "[epoch:35,batch:389]:acc: 0.912420,loss:0.221292\n",
      "[epoch:35,batch:419]:acc: 0.911235,loss:0.222126\n",
      "[epoch:35,batch:449]:acc: 0.911458,loss:0.221843\n",
      "[epoch:35,batch:479]:acc: 0.910937,loss:0.222381\n",
      "[epoch:35,batch:509]:acc: 0.910784,loss:0.222030\n",
      "[epoch:35,batch:539]:acc: 0.911632,loss:0.221052\n",
      "[epoch:35,batch:569]:acc: 0.911842,loss:0.219759\n",
      "[epoch:35,batch:599]:acc: 0.912292,loss:0.219169\n",
      "[epoch:35,batch:599]: val_loss:0.389872,val_acc:0.852170,val_total:4539\n",
      "[epoch:35,batch:629]:acc: 0.911458,loss:0.220911\n",
      "[epoch:35,batch:659]:acc: 0.911458,loss:0.221170\n",
      "[epoch:35,batch:689]:acc: 0.911504,loss:0.220350\n",
      "[epoch:35,batch:719]:acc: 0.911719,loss:0.220413\n",
      "[epoch:35,batch:749]:acc: 0.912208,loss:0.219980\n",
      "[epoch:35,batch:779]:acc: 0.912580,loss:0.219904\n",
      "[epoch:35,batch:809]:acc: 0.912461,loss:0.220193\n",
      "[epoch:35,batch:839]:acc: 0.913207,loss:0.219337\n",
      "[epoch:35,batch:869]:acc: 0.913039,loss:0.219348\n",
      "[epoch:35,batch:899]:acc: 0.913333,loss:0.218761\n",
      "[epoch:35,batch:899]: val_loss:0.382770,val_acc:0.856576,val_total:4539\n",
      "[epoch:35,batch:929]:acc: 0.913071,loss:0.218845\n",
      "[epoch:35,batch:959]:acc: 0.912988,loss:0.218872\n",
      "[epoch:35,batch:989]:acc: 0.913131,loss:0.219226\n",
      "[epoch:35] :acc: 0.913012,loss:0.219515,lr:0.000001,patience:0\n",
      "[epoch:35]: val_loss:0.398673,val_acc:0.853712,\n",
      "Epoch 36/59\n",
      "----------\n",
      "[epoch:36,batch:29]:acc: 0.925000,loss:0.194162\n",
      "[epoch:36,batch:59]:acc: 0.925000,loss:0.202242\n",
      "[epoch:36,batch:89]:acc: 0.918403,loss:0.210030\n",
      "[epoch:36,batch:119]:acc: 0.917448,loss:0.207906\n",
      "[epoch:36,batch:149]:acc: 0.917708,loss:0.208414\n",
      "[epoch:36,batch:179]:acc: 0.916667,loss:0.209690\n",
      "[epoch:36,batch:209]:acc: 0.917411,loss:0.210545\n",
      "[epoch:36,batch:239]:acc: 0.916016,loss:0.213190\n",
      "[epoch:36,batch:269]:acc: 0.916319,loss:0.213241\n",
      "[epoch:36,batch:299]:acc: 0.918125,loss:0.213855\n",
      "[epoch:36,batch:299]: val_loss:0.381557,val_acc:0.856576,val_total:4539\n",
      "save new model loss,now loss is  0.38155651092529297\n",
      "[epoch:36,batch:329]:acc: 0.917424,loss:0.215015\n",
      "[epoch:36,batch:359]:acc: 0.915972,loss:0.216760\n",
      "[epoch:36,batch:389]:acc: 0.915545,loss:0.216226\n",
      "[epoch:36,batch:419]:acc: 0.915774,loss:0.216102\n",
      "[epoch:36,batch:449]:acc: 0.915625,loss:0.216492\n",
      "[epoch:36,batch:479]:acc: 0.915951,loss:0.216036\n",
      "[epoch:36,batch:509]:acc: 0.915809,loss:0.216345\n",
      "[epoch:36,batch:539]:acc: 0.915394,loss:0.217451\n",
      "[epoch:36,batch:569]:acc: 0.915077,loss:0.216845\n",
      "[epoch:36,batch:599]:acc: 0.913438,loss:0.218772\n",
      "[epoch:36,batch:599]: val_loss:0.386234,val_acc:0.850848,val_total:4539\n",
      "[epoch:36,batch:629]:acc: 0.913492,loss:0.218489\n",
      "[epoch:36,batch:659]:acc: 0.914299,loss:0.217013\n",
      "[epoch:36,batch:689]:acc: 0.914357,loss:0.217560\n",
      "[epoch:36,batch:719]:acc: 0.914106,loss:0.218144\n",
      "[epoch:36,batch:749]:acc: 0.914167,loss:0.218003\n",
      "[epoch:36,batch:779]:acc: 0.913742,loss:0.218836\n",
      "[epoch:36,batch:809]:acc: 0.913735,loss:0.218736\n",
      "[epoch:36,batch:839]:acc: 0.913653,loss:0.219307\n",
      "[epoch:36,batch:869]:acc: 0.913829,loss:0.218792\n",
      "[epoch:36,batch:899]:acc: 0.914132,loss:0.218074\n",
      "[epoch:36,batch:899]: val_loss:0.386928,val_acc:0.852390,val_total:4539\n",
      "[epoch:36,batch:929]:acc: 0.913945,loss:0.218302\n",
      "[epoch:36,batch:959]:acc: 0.914128,loss:0.218144\n",
      "[epoch:36,batch:989]:acc: 0.914268,loss:0.217886\n",
      "[epoch:36] :acc: 0.914368,loss:0.217848,lr:0.000001,patience:0\n",
      "[epoch:36]: val_loss:0.408228,val_acc:0.850628,\n",
      "Epoch 37/59\n",
      "----------\n",
      "[epoch:37,batch:29]:acc: 0.925000,loss:0.207096\n",
      "[epoch:37,batch:59]:acc: 0.916667,loss:0.221659\n",
      "[epoch:37,batch:89]:acc: 0.915625,loss:0.216670\n",
      "[epoch:37,batch:119]:acc: 0.911979,loss:0.219308\n",
      "[epoch:37,batch:149]:acc: 0.911875,loss:0.220595\n",
      "[epoch:37,batch:179]:acc: 0.910417,loss:0.221577\n",
      "[epoch:37,batch:209]:acc: 0.912351,loss:0.219233\n",
      "[epoch:37,batch:239]:acc: 0.914453,loss:0.216679\n",
      "[epoch:37,batch:269]:acc: 0.913773,loss:0.218492\n",
      "[epoch:37,batch:299]:acc: 0.912917,loss:0.219830\n",
      "[epoch:37,batch:299]: val_loss:0.389949,val_acc:0.855034,val_total:4539\n",
      "[epoch:37,batch:329]:acc: 0.913920,loss:0.217878\n",
      "[epoch:37,batch:359]:acc: 0.914583,loss:0.216441\n",
      "[epoch:37,batch:389]:acc: 0.913942,loss:0.218890\n",
      "[epoch:37,batch:419]:acc: 0.914211,loss:0.218803\n",
      "[epoch:37,batch:449]:acc: 0.914722,loss:0.217939\n",
      "[epoch:37,batch:479]:acc: 0.913411,loss:0.219571\n",
      "[epoch:37,batch:509]:acc: 0.913542,loss:0.218995\n",
      "[epoch:37,batch:539]:acc: 0.913542,loss:0.219606\n",
      "[epoch:37,batch:569]:acc: 0.913542,loss:0.219339\n",
      "[epoch:37,batch:599]:acc: 0.913125,loss:0.220121\n",
      "[epoch:37,batch:599]: val_loss:0.386852,val_acc:0.855254,val_total:4539\n",
      "[epoch:37,batch:629]:acc: 0.913938,loss:0.219324\n",
      "[epoch:37,batch:659]:acc: 0.913494,loss:0.219642\n",
      "[epoch:37,batch:689]:acc: 0.913813,loss:0.219299\n",
      "[epoch:37,batch:719]:acc: 0.914800,loss:0.217910\n",
      "[epoch:37,batch:749]:acc: 0.914625,loss:0.218634\n",
      "[epoch:37,batch:779]:acc: 0.914503,loss:0.218863\n",
      "[epoch:37,batch:809]:acc: 0.914660,loss:0.218335\n",
      "[epoch:37,batch:839]:acc: 0.914286,loss:0.218181\n",
      "[epoch:37,batch:869]:acc: 0.913649,loss:0.219016\n",
      "[epoch:37,batch:899]:acc: 0.913750,loss:0.218862\n",
      "[epoch:37,batch:899]: val_loss:0.384806,val_acc:0.856576,val_total:4539\n",
      "[epoch:37,batch:929]:acc: 0.913743,loss:0.218209\n",
      "[epoch:37,batch:959]:acc: 0.913444,loss:0.218573\n",
      "[epoch:37,batch:989]:acc: 0.913447,loss:0.218432\n",
      "[epoch:37] :acc: 0.913422,loss:0.218781,lr:0.000001,patience:1\n",
      "[epoch:37]: val_loss:0.407749,val_acc:0.851289,\n",
      "Epoch 38/59\n",
      "----------\n",
      "[epoch:38,batch:29]:acc: 0.927083,loss:0.187607\n",
      "[epoch:38,batch:59]:acc: 0.926042,loss:0.197435\n",
      "[epoch:38,batch:89]:acc: 0.922222,loss:0.204709\n",
      "[epoch:38,batch:119]:acc: 0.918750,loss:0.209733\n",
      "[epoch:38,batch:149]:acc: 0.918125,loss:0.211301\n",
      "[epoch:38,batch:179]:acc: 0.915625,loss:0.214847\n",
      "[epoch:38,batch:209]:acc: 0.918006,loss:0.211893\n",
      "[epoch:38,batch:239]:acc: 0.916927,loss:0.213368\n",
      "[epoch:38,batch:269]:acc: 0.917361,loss:0.211961\n",
      "[epoch:38,batch:299]:acc: 0.917188,loss:0.210597\n",
      "[epoch:38,batch:299]: val_loss:0.390907,val_acc:0.853933,val_total:4539\n",
      "[epoch:38,batch:329]:acc: 0.917235,loss:0.210528\n",
      "[epoch:38,batch:359]:acc: 0.916753,loss:0.211079\n",
      "[epoch:38,batch:389]:acc: 0.915625,loss:0.211702\n",
      "[epoch:38,batch:419]:acc: 0.915997,loss:0.211178\n",
      "[epoch:38,batch:449]:acc: 0.915417,loss:0.211716\n",
      "[epoch:38,batch:479]:acc: 0.914779,loss:0.213198\n",
      "[epoch:38,batch:509]:acc: 0.914706,loss:0.213563\n",
      "[epoch:38,batch:539]:acc: 0.914988,loss:0.213194\n",
      "[epoch:38,batch:569]:acc: 0.914529,loss:0.213907\n",
      "[epoch:38,batch:599]:acc: 0.914010,loss:0.215070\n",
      "[epoch:38,batch:599]: val_loss:0.385945,val_acc:0.856136,val_total:4539\n",
      "[epoch:38,batch:629]:acc: 0.914385,loss:0.214930\n",
      "[epoch:38,batch:659]:acc: 0.914299,loss:0.214569\n",
      "[epoch:38,batch:689]:acc: 0.913904,loss:0.214966\n",
      "[epoch:38,batch:719]:acc: 0.913889,loss:0.215213\n",
      "[epoch:38,batch:749]:acc: 0.913292,loss:0.216806\n",
      "[epoch:38,batch:779]:acc: 0.913702,loss:0.216575\n",
      "[epoch:38,batch:809]:acc: 0.913349,loss:0.216857\n",
      "[epoch:38,batch:839]:acc: 0.913021,loss:0.217746\n",
      "[epoch:38,batch:869]:acc: 0.913039,loss:0.218465\n",
      "[epoch:38,batch:899]:acc: 0.913403,loss:0.218489\n",
      "[epoch:38,batch:899]: val_loss:0.395799,val_acc:0.849747,val_total:4539\n",
      "[epoch:38,batch:929]:acc: 0.913978,loss:0.218135\n",
      "[epoch:38,batch:959]:acc: 0.914258,loss:0.217514\n",
      "[epoch:38,batch:989]:acc: 0.914299,loss:0.217573\n",
      "[epoch:38] :acc: 0.914242,loss:0.218065,lr:0.000001,patience:2\n",
      "[epoch:38]: val_loss:0.410026,val_acc:0.849526,\n",
      "Epoch 39/59\n",
      "----------\n",
      "lr desencd\n",
      "[epoch:39,batch:29]:acc: 0.901042,loss:0.226063\n",
      "[epoch:39,batch:59]:acc: 0.903125,loss:0.232270\n",
      "[epoch:39,batch:89]:acc: 0.907292,loss:0.228677\n",
      "[epoch:39,batch:119]:acc: 0.905208,loss:0.227253\n",
      "[epoch:39,batch:149]:acc: 0.908125,loss:0.220300\n",
      "[epoch:39,batch:179]:acc: 0.910417,loss:0.219022\n",
      "[epoch:39,batch:209]:acc: 0.909970,loss:0.219299\n",
      "[epoch:39,batch:239]:acc: 0.910156,loss:0.217513\n",
      "[epoch:39,batch:269]:acc: 0.911458,loss:0.216937\n",
      "[epoch:39,batch:299]:acc: 0.912188,loss:0.216073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:39,batch:299]: val_loss:0.386946,val_acc:0.855254,val_total:4539\n",
      "[epoch:39,batch:329]:acc: 0.912879,loss:0.216360\n",
      "[epoch:39,batch:359]:acc: 0.913802,loss:0.214906\n",
      "[epoch:39,batch:389]:acc: 0.913221,loss:0.217025\n",
      "[epoch:39,batch:419]:acc: 0.913244,loss:0.217232\n",
      "[epoch:39,batch:449]:acc: 0.913542,loss:0.217367\n",
      "[epoch:39,batch:479]:acc: 0.913477,loss:0.216991\n",
      "[epoch:39,batch:509]:acc: 0.913419,loss:0.217404\n",
      "[epoch:39,batch:539]:acc: 0.912847,loss:0.217664\n",
      "[epoch:39,batch:569]:acc: 0.912664,loss:0.217212\n",
      "[epoch:39,batch:599]:acc: 0.912813,loss:0.217046\n",
      "[epoch:39,batch:599]: val_loss:0.387742,val_acc:0.852390,val_total:4539\n",
      "[epoch:39,batch:629]:acc: 0.912996,loss:0.216534\n",
      "[epoch:39,batch:659]:acc: 0.912737,loss:0.216682\n",
      "[epoch:39,batch:689]:acc: 0.912455,loss:0.216867\n",
      "[epoch:39,batch:719]:acc: 0.911936,loss:0.217466\n",
      "[epoch:39,batch:749]:acc: 0.912125,loss:0.216917\n",
      "[epoch:39,batch:779]:acc: 0.912059,loss:0.216649\n",
      "[epoch:39,batch:809]:acc: 0.911921,loss:0.217432\n",
      "[epoch:39,batch:839]:acc: 0.912165,loss:0.217215\n",
      "[epoch:39,batch:869]:acc: 0.911566,loss:0.218384\n",
      "[epoch:39,batch:899]:acc: 0.911910,loss:0.218106\n",
      "[epoch:39,batch:899]: val_loss:0.385274,val_acc:0.853051,val_total:4539\n",
      "[epoch:39,batch:929]:acc: 0.911862,loss:0.218756\n",
      "[epoch:39,batch:959]:acc: 0.911979,loss:0.218428\n",
      "[epoch:39,batch:989]:acc: 0.912058,loss:0.218712\n",
      "[epoch:39] :acc: 0.912035,loss:0.219060,lr:0.000000,patience:0\n",
      "[epoch:39]: val_loss:0.399044,val_acc:0.851068,\n",
      "Epoch 40/59\n",
      "----------\n",
      "[epoch:40,batch:29]:acc: 0.922917,loss:0.209314\n",
      "[epoch:40,batch:59]:acc: 0.912500,loss:0.230981\n",
      "[epoch:40,batch:89]:acc: 0.911806,loss:0.227079\n",
      "[epoch:40,batch:119]:acc: 0.911979,loss:0.225987\n",
      "[epoch:40,batch:149]:acc: 0.916042,loss:0.219657\n",
      "[epoch:40,batch:179]:acc: 0.916493,loss:0.217575\n",
      "[epoch:40,batch:209]:acc: 0.916518,loss:0.217242\n",
      "[epoch:40,batch:239]:acc: 0.916797,loss:0.216399\n",
      "[epoch:40,batch:269]:acc: 0.915856,loss:0.215975\n",
      "[epoch:40,batch:299]:acc: 0.916979,loss:0.215197\n",
      "[epoch:40,batch:299]: val_loss:0.387390,val_acc:0.851729,val_total:4539\n",
      "[epoch:40,batch:329]:acc: 0.917045,loss:0.214935\n",
      "[epoch:40,batch:359]:acc: 0.917795,loss:0.213866\n",
      "[epoch:40,batch:389]:acc: 0.916587,loss:0.214951\n",
      "[epoch:40,batch:419]:acc: 0.916518,loss:0.215106\n",
      "[epoch:40,batch:449]:acc: 0.916319,loss:0.215133\n",
      "[epoch:40,batch:479]:acc: 0.916276,loss:0.214445\n",
      "[epoch:40,batch:509]:acc: 0.915380,loss:0.216109\n",
      "[epoch:40,batch:539]:acc: 0.915741,loss:0.215434\n",
      "[epoch:40,batch:569]:acc: 0.915625,loss:0.215180\n",
      "[epoch:40,batch:599]:acc: 0.914479,loss:0.216271\n",
      "[epoch:40,batch:599]: val_loss:0.387500,val_acc:0.853712,val_total:4539\n",
      "[epoch:40,batch:629]:acc: 0.914683,loss:0.216031\n",
      "[epoch:40,batch:659]:acc: 0.914347,loss:0.215543\n",
      "[epoch:40,batch:689]:acc: 0.914221,loss:0.215871\n",
      "[epoch:40,batch:719]:acc: 0.913802,loss:0.216280\n",
      "[epoch:40,batch:749]:acc: 0.913375,loss:0.216678\n",
      "[epoch:40,batch:779]:acc: 0.913462,loss:0.217247\n",
      "[epoch:40,batch:809]:acc: 0.913503,loss:0.217779\n",
      "[epoch:40,batch:839]:acc: 0.913281,loss:0.217829\n",
      "[epoch:40,batch:869]:acc: 0.913542,loss:0.217350\n",
      "[epoch:40,batch:899]:acc: 0.913958,loss:0.216229\n",
      "[epoch:40,batch:899]: val_loss:0.384250,val_acc:0.854814,val_total:4539\n",
      "[epoch:40,batch:929]:acc: 0.913810,loss:0.216572\n",
      "[epoch:40,batch:959]:acc: 0.913704,loss:0.216648\n",
      "[epoch:40,batch:989]:acc: 0.913826,loss:0.216794\n",
      "[epoch:40] :acc: 0.913737,loss:0.216889,lr:0.000000,patience:1\n",
      "[epoch:40]: val_loss:0.391957,val_acc:0.852611,\n",
      "Epoch 41/59\n",
      "----------\n",
      "[epoch:41,batch:29]:acc: 0.905208,loss:0.233141\n",
      "[epoch:41,batch:59]:acc: 0.910417,loss:0.224196\n",
      "[epoch:41,batch:89]:acc: 0.913194,loss:0.218951\n",
      "[epoch:41,batch:119]:acc: 0.910677,loss:0.221491\n",
      "[epoch:41,batch:149]:acc: 0.910208,loss:0.222962\n",
      "[epoch:41,batch:179]:acc: 0.910764,loss:0.220750\n",
      "[epoch:41,batch:209]:acc: 0.913393,loss:0.219669\n",
      "[epoch:41,batch:239]:acc: 0.913542,loss:0.219364\n",
      "[epoch:41,batch:269]:acc: 0.913773,loss:0.218688\n",
      "[epoch:41,batch:299]:acc: 0.914479,loss:0.218803\n",
      "[epoch:41,batch:299]: val_loss:0.383874,val_acc:0.854594,val_total:4539\n",
      "[epoch:41,batch:329]:acc: 0.914205,loss:0.219336\n",
      "[epoch:41,batch:359]:acc: 0.914149,loss:0.218578\n",
      "[epoch:41,batch:389]:acc: 0.915304,loss:0.217095\n",
      "[epoch:41,batch:419]:acc: 0.915327,loss:0.217348\n",
      "[epoch:41,batch:449]:acc: 0.915764,loss:0.216058\n",
      "[epoch:41,batch:479]:acc: 0.914974,loss:0.217145\n",
      "[epoch:41,batch:509]:acc: 0.914828,loss:0.217343\n",
      "[epoch:41,batch:539]:acc: 0.914873,loss:0.217530\n",
      "[epoch:41,batch:569]:acc: 0.915625,loss:0.216937\n",
      "[epoch:41,batch:599]:acc: 0.914896,loss:0.218209\n",
      "[epoch:41,batch:599]: val_loss:0.387537,val_acc:0.855915,val_total:4539\n",
      "[epoch:41,batch:629]:acc: 0.914980,loss:0.217690\n",
      "[epoch:41,batch:659]:acc: 0.915104,loss:0.217330\n",
      "[epoch:41,batch:689]:acc: 0.915036,loss:0.218012\n",
      "[epoch:41,batch:719]:acc: 0.915104,loss:0.218029\n",
      "[epoch:41,batch:749]:acc: 0.914958,loss:0.217752\n",
      "[epoch:41,batch:779]:acc: 0.915425,loss:0.217433\n",
      "[epoch:41,batch:809]:acc: 0.915201,loss:0.217735\n",
      "[epoch:41,batch:839]:acc: 0.915141,loss:0.217362\n",
      "[epoch:41,batch:869]:acc: 0.915266,loss:0.217092\n",
      "[epoch:41,batch:899]:acc: 0.915382,loss:0.217008\n",
      "[epoch:41,batch:899]: val_loss:0.389478,val_acc:0.856136,val_total:4539\n",
      "[epoch:41,batch:929]:acc: 0.915356,loss:0.216944\n",
      "[epoch:41,batch:959]:acc: 0.915234,loss:0.217035\n",
      "[epoch:41,batch:989]:acc: 0.915436,loss:0.216689\n",
      "[epoch:41] :acc: 0.915408,loss:0.217398,lr:0.000000,patience:2\n",
      "[epoch:41]: val_loss:0.489394,val_acc:0.833664,\n",
      "Epoch 42/59\n",
      "----------\n",
      "lr desencd\n",
      "[epoch:42,batch:29]:acc: 0.931250,loss:0.198528\n",
      "[epoch:42,batch:59]:acc: 0.923438,loss:0.202490\n",
      "[epoch:42,batch:89]:acc: 0.921181,loss:0.201599\n"
     ]
    }
   ],
   "source": [
    "TrainWithRawData('../model/InceptionV4/2018-11-01_acc_best.pth',60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_loss is :0.381557\n",
      "val_correct is 0.856576\n"
     ]
    }
   ],
   "source": [
    "modelParams=torch.load('../model/InceptionV4/2018-11-01_loss_best.pth')\n",
    "min_loss=modelParams['val_loss']\n",
    "print('min_loss is :%f'%(min_loss))\n",
    "print('val_correct is %f'%(modelParams['val_correct']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Conda_Env_Pytorch]",
   "language": "python",
   "name": "conda-env-Conda_Env_Pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
