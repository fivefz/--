{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import CropModels\n",
    "import utils\n",
    "from augmentation import five_crops, HorizontalFlip, make_transforms\n",
    "from CropDataset import MyDataSet, preprocess, preprocess_hflip, normalize_05, normalize_torch\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats.mstats import gmean\n",
    "NB_CLASS=59\n",
    "SEED=888\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE=32\n",
    "IMAGE_TRAIN_PRE='../data/AgriculturalDisease_trainingset/images/'\n",
    "ANNOTATION_TRAIN='../data/AgriculturalDisease_trainingset/AgriculturalDisease_train_annotations_deleteNoise.json' #是否需要剔除两类异常类\n",
    "IMAGE_VAL_PRE='../data/AgriculturalDisease_validationset/images/'\n",
    "ANNOTATION_VAL='../data/AgriculturalDisease_validationset/AgriculturalDisease_validation_annotations_deleteNoise.json' #是否需要剔除两类异常类\n",
    "IMAGE_TEST_PRE='../data/AgriculturalDisease_testA/images/'\n",
    "ANNOTATION_TEST='../data/AgriculturalDisease_testA/AgriculturalDisease_test_annotations.json'\n",
    "with open(ANNOTATION_TRAIN) as datafile1:\n",
    "    trainDataFram=pd.read_json(datafile1,orient='records')\n",
    "with open(ANNOTATION_VAL) as datafile2: #first check if it's a valid json file or not\n",
    "    validateDataFram =pd.read_json(datafile2,orient='records')\n",
    "def get_model(model_class):\n",
    "    print('[+] loading model... ', end='', flush=True)\n",
    "    model = model_class(NB_CLASS)\n",
    "    model.cuda()\n",
    "    print('done')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictAll(model_name, model_class, weight_pth, image_size, normalize):\n",
    "    print(f'[+] predict {model_name}')\n",
    "    model = get_model(model_class)\n",
    "    model.load_state_dict(torch.load(weight_pth)['state_dict'])\n",
    "    model.eval()\n",
    "    print('load state dict done')\n",
    "\n",
    "    tta_preprocess = [preprocess(normalize, image_size), preprocess_hflip(normalize, image_size)]\n",
    "    tta_preprocess += make_transforms([transforms.Resize((image_size + 20, image_size + 20))],\n",
    "                                      [transforms.ToTensor(), normalize],\n",
    "                                      five_crops(image_size))\n",
    "    tta_preprocess += make_transforms([transforms.Resize((image_size + 20, image_size + 20))],\n",
    "                                      [HorizontalFlip(), transforms.ToTensor(), normalize],\n",
    "                                      five_crops(image_size))\n",
    "    print(f'[+] tta size: {len(tta_preprocess)}')\n",
    "\n",
    "\n",
    "    data_loaders = []\n",
    "    for transform in tta_preprocess:\n",
    "\n",
    "        test_dataset = MyDataSet(json_Description=ANNOTATION_VAL,transform=transform,path_pre=IMAGE_VAL_PRE)\n",
    "        data_loader = DataLoader(dataset=test_dataset, num_workers=16,\n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 shuffle=False)\n",
    "        data_loaders.append(data_loader)\n",
    "        print('add transforms')\n",
    "\n",
    "    lx, px = utils.predict_tta(model, data_loaders)\n",
    "    data = {\n",
    "        'lx': lx.cpu(),\n",
    "        'px': px.cpu(),\n",
    "    }\n",
    "    if not os.path.exists('../feature/'+model_name):\n",
    "        os.makedirs('../feature/'+model_name)\n",
    "    torch.save(data, '../feature/'+model_name+'/val_all_prediction.pth')\n",
    "    print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictFlip(model_name, model_class, weight_pth, image_size, normalize):\n",
    "    print(f'[+] predict {model_name}')\n",
    "    model = get_model(model_class)\n",
    "    model.load_state_dict(torch.load(weight_pth)['state_dict'])\n",
    "    model.eval()\n",
    "    print('load state dict done')\n",
    "\n",
    "    tta_preprocess = [preprocess(normalize, image_size), preprocess_hflip(normalize, image_size)]\n",
    "\n",
    "#     tta_preprocess += make_transforms([transforms.Resize((image_size + 20, image_size + 20))],\n",
    "#                                       [transforms.ToTensor(), normalize],\n",
    "#                                       five_crops(image_size))\n",
    "#     tta_preprocess += make_transforms([transforms.Resize((image_size + 20, image_size + 20))],\n",
    "#                                       [HorizontalFlip(), transforms.ToTensor(), normalize],\n",
    "#                                       five_crops(image_size))\n",
    "    print(f'[+] tta size: {len(tta_preprocess)}')\n",
    "\n",
    "\n",
    "    data_loaders = []\n",
    "    for transform in tta_preprocess:\n",
    "\n",
    "        test_dataset = MyDataSet(json_Description=ANNOTATION_VAL,transform=transform,path_pre=IMAGE_VAL_PRE)\n",
    "        data_loader = DataLoader(dataset=test_dataset, num_workers=16,\n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 shuffle=False)\n",
    "        data_loaders.append(data_loader)\n",
    "        print('add transforms')\n",
    "\n",
    "    lx, px = utils.predict_tta(model, data_loaders)\n",
    "    data = {\n",
    "        'lx': lx.cpu(),\n",
    "        'px': px.cpu(),\n",
    "    }\n",
    "    if not os.path.exists('../feature/'+model_name):\n",
    "        os.makedirs('../feature/'+model_name)\n",
    "    torch.save(data, '../feature/'+model_name+'/val_flip_prediction.pth')\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictCrop(model_name, model_class, weight_pth, image_size, normalize):\n",
    "    print(f'[+] predict {model_name}')\n",
    "    model = get_model(model_class)\n",
    "    model.load_state_dict(torch.load(weight_pth)['state_dict'])\n",
    "    model.eval()\n",
    "    print('load state dict done')\n",
    "\n",
    "    \n",
    "    tta_preprocess=[preprocess(normalize, image_size)]\n",
    "    tta_preprocess += make_transforms([transforms.Resize((image_size + 20, image_size + 20))],\n",
    "                                      [transforms.ToTensor(), normalize],\n",
    "                                      five_crops(image_size))\n",
    "#     tta_preprocess += make_transforms([transforms.Resize((image_size + 20, image_size + 20))],\n",
    "#                                       [HorizontalFlip(), transforms.ToTensor(), normalize],\n",
    "#                                       five_crops(image_size))\n",
    "    print(f'[+] tta size: {len(tta_preprocess)}')\n",
    "\n",
    "\n",
    "    data_loaders = []\n",
    "    for transform in tta_preprocess:\n",
    "\n",
    "        test_dataset = MyDataSet(json_Description=ANNOTATION_VAL,transform=transform,path_pre=IMAGE_VAL_PRE)\n",
    "        data_loader = DataLoader(dataset=test_dataset, num_workers=16,\n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 shuffle=False)\n",
    "        data_loaders.append(data_loader)\n",
    "        print('add transforms')\n",
    "\n",
    "    lx, px = utils.predict_tta(model, data_loaders)\n",
    "    data = {\n",
    "        'lx': lx.cpu(),\n",
    "        'px': px.cpu(),\n",
    "    }\n",
    "    if not os.path.exists('../feature/'+model_name):\n",
    "        os.makedirs('../feature/'+model_name)\n",
    "    torch.save(data, '../feature/'+model_name+'/val_crop_prediction.pth')\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_raw(model_name, model_class, weight_pth, image_size, normalize):\n",
    "    print(f'[+] predict {model_name}')\n",
    "    model = get_model(model_class)\n",
    "    model.load_state_dict(torch.load(weight_pth)['state_dict'])\n",
    "    model.eval()\n",
    "    print('load state dict done')\n",
    "    test_dataset = MyDataSet(json_Description=ANNOTATION_VAL,transform=preprocess(normalize, image_size),path_pre=IMAGE_VAL_PRE)\n",
    "    data_loader = DataLoader(dataset=test_dataset, num_workers=16,\n",
    "                                 batch_size=BATCH_SIZE,\n",
    "                                 shuffle=False)\n",
    "    lx,px=utils.predict(model,data_loader)\n",
    "    data = {\n",
    "        'lx': lx.cpu(),\n",
    "        'px': px.cpu(),\n",
    "    }\n",
    "    if not os.path.exists('../feature/'+model_name):\n",
    "        os.makedirs('../feature/'+model_name)\n",
    "    torch.save(data, '../feature/'+model_name+'/val_raw_prediction.pth')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all():\n",
    "    \n",
    "    \n",
    "    \n",
    "#     predict_raw('Resnet50',CropModels.resnet50_finetune,'../model/ResNet50/2018-11-01_acc_best.pth',224,normalize_torch)\n",
    "#     predictAll('Resnet50',CropModels.resnet50_finetune,'../model/ResNet50/2018-11-01_loss_best.pth',224,normalize_torch)\n",
    "#     predictFlip('Resnet50',CropModels.resnet50_finetune,'../model/ResNet50/2018-11-01_loss_best.pth',224,normalize_torch)\n",
    "#     predictCrop('Resnet50',CropModels.resnet50_finetune,'../model/ResNet50/2018-11-01_loss_best.pth',224,normalize_torch)\n",
    "#     print('ResNet50 done')\n",
    "\n",
    "#     predict_raw('Resnet101',CropModels.resnet101_finetune,'../model/ResNet101/2018-11-04_acc_best.pth',224,normalize_torch)\n",
    "#     predictAll('Resnet101',CropModels.resnet101_finetune,'../model/ResNet101/2018-11-04_acc_best.pth',224,normalize_torch)\n",
    "#     predictFlip('Resnet101',CropModels.resnet101_finetune,'../model/ResNet101/2018-11-04_acc_best.pth',224,normalize_torch)\n",
    "#     predictCrop('Resnet101',CropModels.resnet101_finetune,'../model/ResNet101/2018-11-04_acc_best.pth',224,normalize_torch)\n",
    "#     print('ResNet101 done')\n",
    "\n",
    "    predict_raw('DesNet121',CropModels.densenet121_finetune,'../model/DesNet121/2018-11-04_acc_best.pth',224,normalize_torch)\n",
    "    predictAll('DesNet121',CropModels.densenet121_finetune,'../model/DesNet121/2018-11-04_acc_best.pth',224,normalize_torch)\n",
    "    predictFlip('DesNet121',CropModels.densenet121_finetune,'../model/DesNet121/2018-11-04_acc_best.pth',224,normalize_torch)\n",
    "    predictCrop('DesNet121',CropModels.densenet121_finetune,'../model/DesNet121/2018-11-04_acc_best.pth',224,normalize_torch)\n",
    "    print('DesNet121 done')\n",
    "\n",
    "\n",
    "#     predict_raw('Resnet152',CropModels.resnet152_finetune,'../model/ResNet/2018-10-31_acc_best.pth',224,normalize_torch)\n",
    "#     predictAll('Resnet152',CropModels.resnet152_finetune,'../model/ResNet/2018-10-31_acc_best.pth',224,normalize_torch)\n",
    "#     predictFlip('Resnet152',CropModels.resnet152_finetune,'../model/ResNet/2018-10-31_acc_best.pth',224,normalize_torch)\n",
    "#     predictCrop('Resnet152',CropModels.resnet152_finetune,'../model/ResNet/2018-10-31_acc_best.pth',224,normalize_torch)\n",
    "#     print('ResNet152 done')\n",
    "\n",
    "\n",
    "#     predict_raw('DesNet201',CropModels.densenet201_finetune,'../model/DesNet201/2018-11-01_acc_best.pth',224,normalize_torch) \n",
    "#     predictCrop('DesNet201',CropModels.densenet201_finetune,'../model/DesNet201/2018-11-01_acc_best.pth',224,normalize_torch) \n",
    "#     predictFlip('DesNet201',CropModels.densenet201_finetune,'../model/DesNet201/2018-11-01_acc_best.pth',224,normalize_torch) \n",
    "#     predictAll('DesNet201',CropModels.densenet201_finetune,'../model/DesNet201/2018-11-01_acc_best.pth',224,normalize_torch)\n",
    "#     print('DesNet201 done')\n",
    "\n",
    "    #DesNet161\n",
    "#     predict_raw('DesNet161',CropModels.densenet161_finetune,'../model/DesNet161/2018-11-02_acc_best.pth',224,normalize_torch) \n",
    "#     predictCrop('DesNet161',CropModels.densenet161_finetune,'../model/DesNet161/2018-11-02_acc_best.pth',224,normalize_torch) \n",
    "#     predictFlip('DesNet161',CropModels.densenet161_finetune,'../model/DesNet161/2018-11-02_acc_best.pth',224,normalize_torch) \n",
    "#     predictAll('DesNet161',CropModels.densenet161_finetune,'../model/DesNet161/2018-11-02_acc_best.pth',224,normalize_torch)\n",
    "#     print('Desnet161 Done')\n",
    "    \n",
    "    #Nasnetmobile   \n",
    "#     predict_raw('Nasnetmobile',CropModels.nasnetmobile,'../model/NasnetMobile/2018-11-02_acc_best.pth',224,normalize_05) \n",
    "#     predictCrop('Nasnetmobile',CropModels.nasnetmobile,'../model/NasnetMobile/2018-11-02_acc_best.pth',224,normalize_05) \n",
    "#     predictFlip('Nasnetmobile',CropModels.nasnetmobile,'../model/NasnetMobile/2018-11-02_acc_best.pth',224,normalize_05) \n",
    "#     predictAll('Nasnetmobile',CropModels.nasnetmobile,'../model/NasnetMobile/2018-11-02_acc_best.pth',224,normalize_05)\n",
    "#     print('nasnetmobile Done')\n",
    "    \n",
    "    # InceptionV4\n",
    "#     predict_raw('InceptionV4',CropModels.inceptionv4_finetune,'../model/InceptionV4/2018-11-01_acc_best.pth',299,normalize_05) \n",
    "#     predictCrop('InceptionV4',CropModels.inceptionv4_finetune,'../model/InceptionV4/2018-11-01_acc_best.pth',299,normalize_05) \n",
    "#     predictFlip('InceptionV4',CropModels.inceptionv4_finetune,'../model/InceptionV4/2018-11-01_acc_best.pth',299,normalize_05) \n",
    "#     predictAll('InceptionV4',CropModels.inceptionv4_finetune,'../model/InceptionV4/2018-11-01_acc_best.pth',299,normalize_05)\n",
    "#     print('InceptionV4 done')\n",
    "    #InceptionV3\n",
    "#     predict_raw('InceptionV3',CropModels.InceptionV3Finetune,'../model/InceptionV3/2018-11-03_acc_best.pth',299,normalize_05) \n",
    "#     predictAll('InceptionV3',CropModels.InceptionV3Finetune,'../model/InceptionV3/2018-11-03_acc_best.pth',299,normalize_05) \n",
    "#     predictCrop('InceptionV3',CropModels.InceptionV3Finetune,'../model/InceptionV3/2018-11-03_acc_best.pth',299,normalize_05) \n",
    "#     predictFlip('InceptionV3',CropModels.InceptionV3Finetune,'../model/InceptionV3/2018-11-03_acc_best.pth',299,normalize_05) \n",
    "    #Xception\n",
    "#     predict_raw('Xception',CropModels.xception_finetune,'../model/Xception/2018-11-03_acc_best.pth',299,normalize_torch) \n",
    "#     predictCrop('Xception',CropModels.xception_finetune,'../model/Xception/2018-11-03_acc_best.pth',299,normalize_torch) \n",
    "#     predictFlip('Xception',CropModels.xception_finetune,'../model/Xception/2018-11-03_acc_best.pth',299,normalize_torch) \n",
    "#     predictAll('Xception',CropModels.xception_finetune,'../model/Xception/2018-11-03_acc_best.pth',299,normalize_torch)\n",
    "#     print('Xception done')\n",
    "    #Inception-Resnetv2\n",
    "#     predict_raw('InceptionResnet',CropModels.inceptionresnetv2_finetune,'../model/Inception_Resnet/2018-11-02_acc_best.pth',299,normalize_05) \n",
    "#     predictAll('InceptionResnet',CropModels.inceptionresnetv2_finetune,'../model/Inception_Resnet/2018-11-02_acc_best.pth',299,normalize_05)\n",
    "#     predictCrop('InceptionResnet',CropModels.inceptionresnetv2_finetune,'../model/Inception_Resnet/2018-11-02_acc_best.pth',299,normalize_05)\n",
    "#     predictFlip('InceptionResnet',CropModels.inceptionresnetv2_finetune,'../model/Inception_Resnet/2018-11-02_acc_best.pth',299,normalize_05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8733)\n",
      "accurarysAfterCal is 0.8730998017184401\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predict=torch.load('../feature/DesNet121/val_raw_prediction.pth')\n",
    "val_Predict=predict['px']\n",
    "predictSoftMax=F.softmax(val_Predict,dim=1)\n",
    "_,px=torch.max(predictSoftMax,dim=1)\n",
    "print(torch.mean((predict['lx']==px).float()))\n",
    "scoreAfter=utils.calibrate_probs(trainDataFram,validateDataFram,predictSoftMax,59)\n",
    "val_preAfter=np.argmax(scoreAfter,axis=1)\n",
    "accurarysAfterCal=np.mean(val_preAfter == predict['lx'].numpy())\n",
    "print('accurarysAfterCal is',accurarysAfterCal)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8735404274069178\n",
      "accurarysAfterCal is 0.8735404274069178\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from scipy.stats.mstats import gmean\n",
    "predict=torch.load('../feature/DesNet121/val_flip_prediction.pth')\n",
    "predictScore=predict['px']\n",
    "predictSoftMax=F.softmax(predictScore,dim=1)\n",
    "val_prob =gmean(predictSoftMax, axis=2) #np.mean(score,axis=2) #\n",
    "#val_prob=np.mean(predictSoftMax.numpy(),axis=2)\n",
    "val_pred = np.argmax(val_prob, axis=1)\n",
    "print(np.mean(val_pred == predict['lx'].numpy()))\n",
    "\n",
    "scoreAfter=utils.calibrate_probs2(trainDataFram,validateDataFram,val_prob,59)\n",
    "val_preAfter=np.argmax(scoreAfter,axis=1)\n",
    "accurarysAfterCal=np.mean(val_preAfter == predict['lx'].numpy())\n",
    "print('accurarysAfterCal is',accurarysAfterCal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8830138797091871\n",
      "0.8821326283322317\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "测试结果\n",
    "\n",
    "'''\n",
    "\n",
    "label=validateDataFram['disease_class'].values\n",
    "val_predResnet50=torch.load('../feature/Resnet50/val_raw_prediction.pth')['px']\n",
    "val_predResnet50=torch.unsqueeze(val_predResnet50,2)\n",
    "\n",
    "val_predResnet101=torch.load('../feature/Resnet101/val_raw_prediction.pth')['px']\n",
    "val_predResnet101=torch.unsqueeze(val_predResnet101,2)\n",
    "\n",
    "val_predResnet152=torch.load('../feature/Resnet152/val_raw_prediction.pth')['px']\n",
    "val_predResnet152=torch.unsqueeze(val_predResnet152,2)\n",
    "\n",
    "val_predDesnet201=torch.load('../feature/DesNet201/val_crop_prediction.pth')['px']\n",
    "#val_predDesnet201=torch.unsqueeze(val_predDesnet201,2)\n",
    "\n",
    "val_predDesnet121=torch.load('../feature/DesNet121/val_raw_prediction.pth')['px']\n",
    "val_predDesnet121=torch.unsqueeze(val_predDesnet121,2)\n",
    "\n",
    "val_predDesnet161=torch.load('../feature/DesNet161/val_raw_prediction.pth')['px']\n",
    "val_predDesnet161=torch.unsqueeze(val_predDesnet161,2)\n",
    "\n",
    "\n",
    "val_predNasnetMobile=torch.load('../feature/Nasnetmobile/val_raw_prediction.pth')['px']\n",
    "val_predNasnetMobile=torch.unsqueeze(val_predNasnetMobile,2)\n",
    "# val_predNasnetMobile=torch.load('../feature/Nasnetmobile/val_all_prediction.pth')['px']\n",
    "val_preInceptionv4=torch.load('../feature/InceptionV4/val_raw_prediction.pth')['px']\n",
    "val_preInceptionv4=torch.unsqueeze(val_preInceptionv4,2)\n",
    "\n",
    "val_preInceptionv3=torch.load('../feature/InceptionV3/val_raw_prediction.pth')['px']\n",
    "val_preInceptionv3=torch.unsqueeze(val_preInceptionv3,2)\n",
    "\n",
    "val_preInceptionResnet=torch.load('../feature/InceptionResnet/val_raw_prediction.pth')['px']\n",
    "val_preInceptionResnet=torch.unsqueeze(val_preInceptionResnet,2)\n",
    "\n",
    "#val_preInceptionResnet=torch.load('../feature/InceptionResnet/val_all_prediction.pth')['px']\n",
    "val_preXception=torch.load('../feature/Xception/val_raw_prediction.pth')['px']\n",
    "val_preXception=torch.unsqueeze(val_preXception,2)\n",
    "\n",
    "val_prob=F.softmax(torch.cat((val_predResnet50,3*val_predResnet101,val_predResnet152,val_predDesnet121,3*val_predDesnet161,val_predDesnet201,val_predNasnetMobile,val_preInceptionv3,val_preInceptionv4),dim=2),dim=1).numpy()\n",
    "val_prob=np.mean(val_prob,axis=2)\n",
    "val_predict=np.argmax(val_prob,axis=1)\n",
    "print(np.mean(val_predict == label ))\n",
    "val_prob_after=utils.calibrate_probs2(trainDataFram,validateDataFram,val_prob,59)\n",
    "val_predict_after=np.argmax(val_prob_after,axis=1)\n",
    "print(np.mean(val_predict_after == label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def getPredictJson():\n",
    "    img_list=os.listdir('../data/AgriculturalDisease_testA/images/')\n",
    "    result=[]\n",
    "    for img_name in img_list:\n",
    "        a={\"image_id\":img_name,\"disease_class\":-1}\n",
    "        result.append(a.copy())\n",
    "    with open(\"../data/AgriculturalDisease_testA/AgriculturalDisease_test_annotations.json\",'w') as f:\n",
    "        json.dump(result,f,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "生成提交结果\n",
    "'''\n",
    "import json\n",
    "label=validateDataFram['disease_class'].values\n",
    "test_predResnet50=torch.load('../feature/Resnet50/test_all_prediction.pth')['px']\n",
    "test_predResnet152=torch.load('../feature/Resnet152/test_all_prediction.pth')['px']\n",
    "test_predDesnet201=torch.load('../feature/DesNet201/test_crop_prediction.pth')['px']\n",
    "test_predDesnet161=torch.load('../feature/DesNet161/test_all_prediction.pth')['px']\n",
    "# val_predNasnetMobile=torch.load('../feature/Nasnetmobile/val_all_prediction.pth')['px']\n",
    "test_preInceptionv3=torch.load('../feature/InceptionV3/test_raw_prediction.pth')['px']\n",
    "test_preInceptionv3=torch.unsqueeze(test_preInceptionv3,2)\n",
    "test_preInceptionResnet=torch.load('../feature/InceptionResnet/test_raw_prediction.pth')['px']\n",
    "test_preInceptionResnet=torch.unsqueeze(test_preInceptionResnet,2)\n",
    "#val_preInceptionResnet=torch.load('../feature/InceptionResnet/val_all_prediction.pth')['px']\n",
    "test_preXception=torch.load('../feature/Xception/test_raw_prediction.pth')['px']\n",
    "test_preXception=torch.unsqueeze(test_preXception,2)\n",
    "test_prob=F.softmax(torch.cat((test_predResnet50,test_predResnet152,test_predDesnet201,test_predDesnet161,test_preInceptionResnet,test_preInceptionv3,test_preXception),dim=2),dim=1).numpy()\n",
    "test_prob=gmean(test_prob,axis=2)\n",
    "test_predict=np.argmax(test_prob,axis=1)\n",
    "print(len(test_predict))\n",
    "img_list=os.listdir('../data/AgriculturalDisease_testA/images/')\n",
    "result=[]\n",
    "for index,img_name in enumerate(img_list):\n",
    "    if test_predict[index]>=44:\n",
    "        test_predict[index]+=2\n",
    "    a={\"image_id\":img_name,\"disease_class\":int(test_predict[index]}\n",
    "    result.append(a.copy())\n",
    "with open(\"result.json\",'w') as f:\n",
    "    json.dump(result,f,ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Conda_Env_Pytorch]",
   "language": "python",
   "name": "conda-env-Conda_Env_Pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
