{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CropModels\n",
    "from CropDataset import MyDataSet,normalize_torch,normalize_05,normalize_dataset,preprocess,preprocess_hflip,preprocess_with_augmentation\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from tensorboardX import SummaryWriter\n",
    "import datetime\n",
    "import os\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from utils import RunningMean\n",
    "import utils\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "torch.backends.cudnn.benchmark = True\n",
    "NB_CLASS=59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "IMAGE_SIZE=224    # 不同模型修改不同的Size\n",
    "IMAGE_TRAIN_PRE='../data/AgriculturalDisease_trainingset/images/'\n",
    "ANNOTATION_TRAIN='../data/AgriculturalDisease_trainingset/AgriculturalDisease_train_annotations_deleteNoise.json' #是否需要剔除两类异常类\n",
    "IMAGE_VAL_PRE='../data/AgriculturalDisease_validationset/images/'\n",
    "ANNOTATION_VAL='../data/AgriculturalDisease_validationset/AgriculturalDisease_validation_annotations_deleteNoise.json' #是否需要剔除两类异常类\n",
    "date=str(datetime.date.today())\n",
    "with open(ANNOTATION_TRAIN) as datafile1:\n",
    "    trainDataFram=pd.read_json(datafile1,orient='records')\n",
    "with open(ANNOTATION_VAL) as datafile2: #first check if it's a valid json file or not\n",
    "    validateDataFram =pd.read_json(datafile2,orient='records')    \n",
    "def getmodel():\n",
    "    print('[+] loading model... ', end='', flush=True)\n",
    "    model=CropModels.densenet121_finetune(NB_CLASS)\n",
    "    model.cuda()\n",
    "    print('Done')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochNum):\n",
    "    writer=SummaryWriter('log/'+date+'/DesNet121/') # 创建 /log/日期/InceptionResnet的组织形式  不同模型需要修改不同名称\n",
    "    train_dataset=MyDataSet(json_Description=ANNOTATION_TRAIN,transform=preprocess_with_augmentation(normalize_torch,IMAGE_SIZE),path_pre=IMAGE_TRAIN_PRE)\n",
    "    val_dataset=MyDataSet(json_Description=ANNOTATION_VAL,transform=preprocess(normalize_torch,IMAGE_SIZE),path_pre=IMAGE_VAL_PRE)\n",
    "    train_dataLoader=DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE,num_workers=16,shuffle=True)\n",
    "    val_dataLoader=DataLoader(dataset=val_dataset,batch_size=BATCH_SIZE,num_workers=1,shuffle=False)\n",
    "    model=getmodel()\n",
    "    weight=torch.Tensor([1,3,3,3,3,4,2,3,3,3,3,3,3,3,3,3,2,3,3,3,2,3,4,2,3,1,1,3,2,2,1,3,3,1,3,2,3,3,3,3,2,1,3,2,3,3,3,1,3,3,4,4,3,2,2,3,1,1,3]).cuda()\n",
    "    criterion=nn.CrossEntropyLoss(weight=weight).cuda()\n",
    "#     lx, px = utils.predict(model,val_dataLoader)\n",
    "#     min_loss = criterion(Variable(px), Variable(lx)).item()\n",
    "    min_loss=4.1\n",
    "    print('min_loss is :%f'%(min_loss))\n",
    "    min_acc=0.80\n",
    "    patience=0\n",
    "    lr=0.0\n",
    "    momentum=0.0\n",
    "    for epoch in range(epochNum):\n",
    "        print('Epoch {}/{}'.format(epoch, epochNum - 1))\n",
    "        print('-' * 10)\n",
    "        if epoch==3:\n",
    "            lr=1e-4\n",
    "            momentum=0.9\n",
    "            print('set lr=:%f,momentum=%f'%(lr,momentum))\n",
    "        if patience==2:\n",
    "            patience=0\n",
    "            model.load_state_dict(torch.load('../model/DesNet121/'+date+'_loss_best.pth')['state_dict'])\n",
    "            lr=lr/10\n",
    "            print('loss has increased lr divide 10 lr now is :%f'%(lr))\n",
    "        if epoch==0 or epoch==1 or epoch==2: #第一轮首先训练全连接层\n",
    "            lr=1e-3\n",
    "#             optimizer=torch.optim.SGD(params=model.fresh_params(),lr=lr,momentum=0.9)\n",
    "            optimizer = torch.optim.Adam(model.fresh_params(),lr = lr,amsgrad=True,weight_decay=1e-4)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(model.parameters(),lr = lr,amsgrad=True,weight_decay=1e-4)\n",
    "#             optimizer=torch.optim.SGD(params=model.parameters(),lr=lr,momentum=momentum)\n",
    "        running_loss = RunningMean()\n",
    "        running_corrects = RunningMean()\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_dataLoader):\n",
    "            model.train(True)\n",
    "            n_batchsize=inputs.size(0)\n",
    "            inputs = Variable(inputs).cuda()\n",
    "            labels = Variable(labels).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            if isinstance(outputs,tuple):\n",
    "                loss=sum((criterion(o,labels)) for o in outputs)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "            running_loss.update(loss.item(),1)\n",
    "            running_corrects.update(torch.sum(preds == labels.data).data,n_batchsize)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx%30==29:\n",
    "                print('[epoch:%d,batch:%d]:acc: %f,loss:%f'%(epoch,batch_idx,running_corrects.value,running_loss.value))\n",
    "                if batch_idx%300==299: \n",
    "                    niter = epoch * len(train_dataset)/BATCH_SIZE + batch_idx\n",
    "                    writer.add_scalar('Train/Acc',running_corrects.value,niter)\n",
    "                    writer.add_scalar('Train/Loss',running_loss.value,niter)\n",
    "                    lx,px=utils.predict(model,val_dataLoader)\n",
    "                    log_loss = criterion(Variable(px), Variable(lx))\n",
    "                    log_loss = log_loss.item()\n",
    "                    _, preds = torch.max(px, dim=1)\n",
    "                    accuracy = torch.mean((preds == lx).float())\n",
    "                    writer.add_scalar('Val/Acc',accuracy,niter)\n",
    "                    writer.add_scalar('Val/Loss',log_loss,niter)\n",
    "                    print('[epoch:%d,batch:%d]: val_loss:%f,val_acc:%f,val_total:%d'%(epoch,batch_idx,log_loss,accuracy,len(val_dataset)))\n",
    "        print('[epoch:%d] :acc: %f,loss:%f,lr:%f,patience:%d'%(epoch,running_corrects.value,running_loss.value,lr,patience))       \n",
    "        lx,px=utils.predict(model,val_dataLoader)\n",
    "        log_loss = criterion(Variable(px), Variable(lx))\n",
    "        log_loss = log_loss.item()\n",
    "        _, preds = torch.max(px, dim=1)\n",
    "        accuracy = torch.mean((preds == lx).float())\n",
    "        writer.add_scalar('Val/Acc',accuracy,(epoch+1) * len(train_dataset)/BATCH_SIZE)\n",
    "        writer.add_scalar('Val/Loss',log_loss,(epoch+1) * len(train_dataset)/BATCH_SIZE)\n",
    "        print('[epoch:%d]: val_loss:%f,val_acc:%f,'%(epoch,log_loss,accuracy))\n",
    "        if  log_loss < min_loss:\n",
    "            utils.snapshot('../model/', 'DesNet121', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy })          \n",
    "            patience = 0\n",
    "            min_loss=log_loss\n",
    "            print('save new model loss,now loss is ',min_loss)\n",
    "        else:\n",
    "            patience += 1\n",
    "        if accuracy>min_acc:\n",
    "            utils.snapshot('../model/', 'DesNet121', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy },key='acc') \n",
    "            min_acc=accuracy\n",
    "            print('save new model acc,now acc is ',min_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] loading model... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lujunfeng/anaconda3/envs/Conda_Env_Pytorch/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /home/lujunfeng/.torch/models/densenet121-a639ec97.pth\n",
      "100%|██████████| 32342954/32342954 [00:02<00:00, 11128266.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "min_loss is :4.100000\n",
      "Epoch 0/59\n",
      "----------\n",
      "[epoch:0,batch:29]:acc: 0.109375,loss:3.912766\n",
      "[epoch:0,batch:59]:acc: 0.228125,loss:3.528083\n",
      "[epoch:0,batch:89]:acc: 0.303125,loss:3.221697\n",
      "[epoch:0,batch:119]:acc: 0.357031,loss:2.988649\n",
      "[epoch:0,batch:149]:acc: 0.396042,loss:2.811107\n",
      "[epoch:0,batch:179]:acc: 0.433854,loss:2.642925\n",
      "[epoch:0,batch:209]:acc: 0.465923,loss:2.496808\n",
      "[epoch:0,batch:239]:acc: 0.488932,loss:2.383076\n",
      "[epoch:0,batch:269]:acc: 0.506134,loss:2.284198\n",
      "[epoch:0,batch:299]:acc: 0.519583,loss:2.201410\n",
      "[epoch:0,batch:299]: val_loss:1.248647,val_acc:0.693985,val_total:4539\n",
      "[epoch:0,batch:329]:acc: 0.532955,loss:2.122236\n",
      "[epoch:0,batch:359]:acc: 0.544271,loss:2.054663\n",
      "[epoch:0,batch:389]:acc: 0.554968,loss:1.991157\n",
      "[epoch:0,batch:419]:acc: 0.564807,loss:1.935031\n",
      "[epoch:0,batch:449]:acc: 0.574306,loss:1.883060\n",
      "[epoch:0,batch:479]:acc: 0.580404,loss:1.839431\n",
      "[epoch:0,batch:509]:acc: 0.585355,loss:1.797372\n",
      "[epoch:0,batch:539]:acc: 0.592766,loss:1.758086\n",
      "[epoch:0,batch:569]:acc: 0.599781,loss:1.719440\n",
      "[epoch:0,batch:599]:acc: 0.605573,loss:1.687040\n",
      "[epoch:0,batch:599]: val_loss:0.917939,val_acc:0.729456,val_total:4539\n",
      "[epoch:0,batch:629]:acc: 0.610069,loss:1.654636\n",
      "[epoch:0,batch:659]:acc: 0.614962,loss:1.625190\n",
      "[epoch:0,batch:689]:acc: 0.618478,loss:1.600594\n",
      "[epoch:0,batch:719]:acc: 0.622005,loss:1.576119\n",
      "[epoch:0,batch:749]:acc: 0.626542,loss:1.551736\n",
      "[epoch:0,batch:779]:acc: 0.630248,loss:1.529463\n",
      "[epoch:0,batch:809]:acc: 0.633912,loss:1.508045\n",
      "[epoch:0,batch:839]:acc: 0.636756,loss:1.487635\n",
      "[epoch:0,batch:869]:acc: 0.639907,loss:1.467901\n",
      "[epoch:0,batch:899]:acc: 0.642917,loss:1.447980\n",
      "[epoch:0,batch:899]: val_loss:0.774479,val_acc:0.760300,val_total:4539\n",
      "[epoch:0,batch:929]:acc: 0.644624,loss:1.431798\n",
      "[epoch:0,batch:959]:acc: 0.647591,loss:1.414828\n",
      "[epoch:0,batch:989]:acc: 0.650410,loss:1.399149\n",
      "[epoch:0] :acc: 0.650377,loss:1.398894,lr:0.001000,patience:0\n",
      "[epoch:0]: val_loss:0.763951,val_acc:0.760961,\n",
      "save new model loss,now loss is  0.763951301574707\n",
      "Epoch 1/59\n",
      "----------\n",
      "[epoch:1,batch:29]:acc: 0.714583,loss:0.962401\n",
      "[epoch:1,batch:59]:acc: 0.732292,loss:0.903417\n",
      "[epoch:1,batch:89]:acc: 0.735069,loss:0.900268\n",
      "[epoch:1,batch:119]:acc: 0.739062,loss:0.888955\n",
      "[epoch:1,batch:149]:acc: 0.743125,loss:0.869898\n",
      "[epoch:1,batch:179]:acc: 0.741840,loss:0.866032\n",
      "[epoch:1,batch:209]:acc: 0.739435,loss:0.857475\n",
      "[epoch:1,batch:239]:acc: 0.739583,loss:0.852981\n",
      "[epoch:1,batch:269]:acc: 0.739005,loss:0.850194\n",
      "[epoch:1,batch:299]:acc: 0.743229,loss:0.839955\n",
      "[epoch:1,batch:299]: val_loss:0.700387,val_acc:0.771315,val_total:4539\n",
      "[epoch:1,batch:329]:acc: 0.745076,loss:0.832794\n",
      "[epoch:1,batch:359]:acc: 0.746094,loss:0.826049\n",
      "[epoch:1,batch:389]:acc: 0.747035,loss:0.821478\n",
      "[epoch:1,batch:419]:acc: 0.746652,loss:0.822956\n",
      "[epoch:1,batch:449]:acc: 0.746181,loss:0.821032\n",
      "[epoch:1,batch:479]:acc: 0.744596,loss:0.822184\n",
      "[epoch:1,batch:509]:acc: 0.744118,loss:0.822273\n",
      "[epoch:1,batch:539]:acc: 0.744097,loss:0.820086\n",
      "[epoch:1,batch:569]:acc: 0.743092,loss:0.820819\n",
      "[epoch:1,batch:599]:acc: 0.742708,loss:0.820937\n",
      "[epoch:1,batch:599]: val_loss:0.670473,val_acc:0.766028,val_total:4539\n",
      "[epoch:1,batch:629]:acc: 0.742708,loss:0.819596\n",
      "[epoch:1,batch:659]:acc: 0.743466,loss:0.815239\n",
      "[epoch:1,batch:689]:acc: 0.743976,loss:0.811751\n",
      "[epoch:1,batch:719]:acc: 0.744097,loss:0.808876\n",
      "[epoch:1,batch:749]:acc: 0.743792,loss:0.807410\n",
      "[epoch:1,batch:779]:acc: 0.744431,loss:0.803458\n",
      "[epoch:1,batch:809]:acc: 0.744637,loss:0.802579\n",
      "[epoch:1,batch:839]:acc: 0.744606,loss:0.801009\n",
      "[epoch:1,batch:869]:acc: 0.744576,loss:0.799112\n",
      "[epoch:1,batch:899]:acc: 0.744618,loss:0.797208\n",
      "[epoch:1,batch:899]: val_loss:0.649175,val_acc:0.776382,val_total:4539\n",
      "[epoch:1,batch:929]:acc: 0.744254,loss:0.795229\n",
      "[epoch:1,batch:959]:acc: 0.744987,loss:0.794291\n",
      "[epoch:1,batch:989]:acc: 0.745423,loss:0.792401\n",
      "[epoch:1] :acc: 0.745468,loss:0.792416,lr:0.001000,patience:0\n",
      "[epoch:1]: val_loss:0.625745,val_acc:0.786957,\n",
      "save new model loss,now loss is  0.6257445216178894\n",
      "Epoch 2/59\n",
      "----------\n",
      "[epoch:2,batch:29]:acc: 0.761458,loss:0.734394\n",
      "[epoch:2,batch:59]:acc: 0.759375,loss:0.740143\n",
      "[epoch:2,batch:89]:acc: 0.759375,loss:0.740450\n",
      "[epoch:2,batch:119]:acc: 0.760417,loss:0.732537\n",
      "[epoch:2,batch:149]:acc: 0.760208,loss:0.727649\n",
      "[epoch:2,batch:179]:acc: 0.763194,loss:0.715743\n",
      "[epoch:2,batch:209]:acc: 0.759673,loss:0.727673\n",
      "[epoch:2,batch:239]:acc: 0.756380,loss:0.729559\n",
      "[epoch:2,batch:269]:acc: 0.754630,loss:0.734262\n",
      "[epoch:2,batch:299]:acc: 0.753021,loss:0.740789\n",
      "[epoch:2,batch:299]: val_loss:0.647237,val_acc:0.762723,val_total:4539\n",
      "[epoch:2,batch:329]:acc: 0.754545,loss:0.737485\n",
      "[epoch:2,batch:359]:acc: 0.753819,loss:0.740122\n",
      "[epoch:2,batch:389]:acc: 0.754327,loss:0.738553\n",
      "[epoch:2,batch:419]:acc: 0.754464,loss:0.738757\n",
      "[epoch:2,batch:449]:acc: 0.755278,loss:0.736808\n",
      "[epoch:2,batch:479]:acc: 0.757031,loss:0.730273\n",
      "[epoch:2,batch:509]:acc: 0.757230,loss:0.728737\n",
      "[epoch:2,batch:539]:acc: 0.756944,loss:0.729753\n",
      "[epoch:2,batch:569]:acc: 0.756853,loss:0.729814\n",
      "[epoch:2,batch:599]:acc: 0.757969,loss:0.725868\n",
      "[epoch:2,batch:599]: val_loss:0.646125,val_acc:0.769112,val_total:4539\n",
      "[epoch:2,batch:629]:acc: 0.757242,loss:0.724348\n",
      "[epoch:2,batch:659]:acc: 0.756676,loss:0.725701\n",
      "[epoch:2,batch:689]:acc: 0.755978,loss:0.726377\n",
      "[epoch:2,batch:719]:acc: 0.757031,loss:0.722221\n",
      "[epoch:2,batch:749]:acc: 0.757667,loss:0.718637\n",
      "[epoch:2,batch:779]:acc: 0.757011,loss:0.719000\n",
      "[epoch:2,batch:809]:acc: 0.757099,loss:0.718704\n",
      "[epoch:2,batch:839]:acc: 0.757106,loss:0.717776\n",
      "[epoch:2,batch:869]:acc: 0.756609,loss:0.717610\n",
      "[epoch:2,batch:899]:acc: 0.757083,loss:0.716302\n",
      "[epoch:2,batch:899]: val_loss:0.614287,val_acc:0.781229,val_total:4539\n",
      "[epoch:2,batch:929]:acc: 0.757796,loss:0.715177\n",
      "[epoch:2,batch:959]:acc: 0.758594,loss:0.713118\n",
      "[epoch:2,batch:989]:acc: 0.758239,loss:0.711262\n",
      "[epoch:2] :acc: 0.758331,loss:0.711812,lr:0.001000,patience:0\n",
      "[epoch:2]: val_loss:0.614661,val_acc:0.780568,\n",
      "save new model loss,now loss is  0.6146612167358398\n",
      "Epoch 3/59\n",
      "----------\n",
      "set lr=:0.000100,momentum=0.900000\n",
      "[epoch:3,batch:29]:acc: 0.780208,loss:0.645275\n",
      "[epoch:3,batch:59]:acc: 0.785937,loss:0.627692\n",
      "[epoch:3,batch:89]:acc: 0.790972,loss:0.621582\n",
      "[epoch:3,batch:119]:acc: 0.792448,loss:0.608006\n",
      "[epoch:3,batch:149]:acc: 0.791458,loss:0.598914\n",
      "[epoch:3,batch:179]:acc: 0.792882,loss:0.594801\n",
      "[epoch:3,batch:209]:acc: 0.795238,loss:0.600300\n",
      "[epoch:3,batch:239]:acc: 0.794922,loss:0.592809\n",
      "[epoch:3,batch:269]:acc: 0.797106,loss:0.581810\n",
      "[epoch:3,batch:299]:acc: 0.797604,loss:0.582435\n",
      "[epoch:3,batch:299]: val_loss:0.509969,val_acc:0.813615,val_total:4539\n",
      "[epoch:3,batch:329]:acc: 0.798958,loss:0.575246\n",
      "[epoch:3,batch:359]:acc: 0.800174,loss:0.568720\n",
      "[epoch:3,batch:389]:acc: 0.797837,loss:0.570817\n",
      "[epoch:3,batch:419]:acc: 0.799851,loss:0.563650\n",
      "[epoch:3,batch:449]:acc: 0.800417,loss:0.560974\n",
      "[epoch:3,batch:479]:acc: 0.802279,loss:0.554934\n",
      "[epoch:3,batch:509]:acc: 0.803125,loss:0.552777\n",
      "[epoch:3,batch:539]:acc: 0.802951,loss:0.553232\n",
      "[epoch:3,batch:569]:acc: 0.803454,loss:0.550920\n",
      "[epoch:3,batch:599]:acc: 0.803438,loss:0.548634\n",
      "[epoch:3,batch:599]: val_loss:0.493994,val_acc:0.807887,val_total:4539\n",
      "[epoch:3,batch:629]:acc: 0.803175,loss:0.548050\n",
      "[epoch:3,batch:659]:acc: 0.803409,loss:0.544008\n",
      "[epoch:3,batch:689]:acc: 0.803714,loss:0.540054\n",
      "[epoch:3,batch:719]:acc: 0.804384,loss:0.537144\n",
      "[epoch:3,batch:749]:acc: 0.805042,loss:0.533770\n",
      "[epoch:3,batch:779]:acc: 0.804768,loss:0.533948\n",
      "[epoch:3,batch:809]:acc: 0.805478,loss:0.530762\n",
      "[epoch:3,batch:839]:acc: 0.806138,loss:0.528747\n",
      "[epoch:3,batch:869]:acc: 0.806106,loss:0.527205\n",
      "[epoch:3,batch:899]:acc: 0.806076,loss:0.525339\n",
      "[epoch:3,batch:899]: val_loss:0.464554,val_acc:0.822428,val_total:4539\n",
      "[epoch:3,batch:929]:acc: 0.806653,loss:0.523308\n",
      "[epoch:3,batch:959]:acc: 0.806608,loss:0.522503\n",
      "[epoch:3,batch:989]:acc: 0.807008,loss:0.520897\n",
      "[epoch:3] :acc: 0.806980,loss:0.520726,lr:0.000100,patience:0\n",
      "[epoch:3]: val_loss:0.475576,val_acc:0.814056,\n",
      "save new model loss,now loss is  0.47557637095451355\n",
      "save new model acc,now acc is  tensor(0.8141, device='cuda:0')\n",
      "Epoch 4/59\n",
      "----------\n",
      "[epoch:4,batch:29]:acc: 0.816667,loss:0.473020\n",
      "[epoch:4,batch:59]:acc: 0.829688,loss:0.437626\n",
      "[epoch:4,batch:89]:acc: 0.832292,loss:0.431557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:4,batch:119]:acc: 0.829427,loss:0.440178\n",
      "[epoch:4,batch:149]:acc: 0.830625,loss:0.433789\n",
      "[epoch:4,batch:179]:acc: 0.829861,loss:0.437938\n",
      "[epoch:4,batch:209]:acc: 0.828423,loss:0.444028\n",
      "[epoch:4,batch:239]:acc: 0.827214,loss:0.442134\n",
      "[epoch:4,batch:269]:acc: 0.829514,loss:0.436706\n",
      "[epoch:4,batch:299]:acc: 0.828125,loss:0.437437\n",
      "[epoch:4,batch:299]: val_loss:0.447014,val_acc:0.827936,val_total:4539\n",
      "[epoch:4,batch:329]:acc: 0.829451,loss:0.436036\n",
      "[epoch:4,batch:359]:acc: 0.828125,loss:0.439168\n",
      "[epoch:4,batch:389]:acc: 0.829327,loss:0.437786\n",
      "[epoch:4,batch:419]:acc: 0.827902,loss:0.443789\n",
      "[epoch:4,batch:449]:acc: 0.827361,loss:0.442603\n",
      "[epoch:4,batch:479]:acc: 0.829232,loss:0.439954\n",
      "[epoch:4,batch:509]:acc: 0.829596,loss:0.439197\n",
      "[epoch:4,batch:539]:acc: 0.829340,loss:0.441492\n",
      "[epoch:4,batch:569]:acc: 0.829550,loss:0.441020\n",
      "[epoch:4,batch:599]:acc: 0.830313,loss:0.438130\n",
      "[epoch:4,batch:599]: val_loss:0.450217,val_acc:0.825733,val_total:4539\n",
      "[epoch:4,batch:629]:acc: 0.829812,loss:0.437654\n",
      "[epoch:4,batch:659]:acc: 0.830019,loss:0.435436\n",
      "[epoch:4,batch:689]:acc: 0.830163,loss:0.435071\n",
      "[epoch:4,batch:719]:acc: 0.829861,loss:0.436043\n",
      "[epoch:4,batch:749]:acc: 0.829000,loss:0.439595\n",
      "[epoch:4,batch:779]:acc: 0.828646,loss:0.440632\n",
      "[epoch:4,batch:809]:acc: 0.829167,loss:0.439102\n",
      "[epoch:4,batch:839]:acc: 0.829241,loss:0.437623\n",
      "[epoch:4,batch:869]:acc: 0.829454,loss:0.438235\n",
      "[epoch:4,batch:899]:acc: 0.829236,loss:0.437136\n",
      "[epoch:4,batch:899]: val_loss:0.427061,val_acc:0.837850,val_total:4539\n",
      "[epoch:4,batch:929]:acc: 0.829402,loss:0.436185\n",
      "[epoch:4,batch:959]:acc: 0.829362,loss:0.435993\n",
      "[epoch:4,batch:989]:acc: 0.829388,loss:0.436555\n",
      "[epoch:4] :acc: 0.829334,loss:0.437074,lr:0.000100,patience:0\n",
      "[epoch:4]: val_loss:0.428938,val_acc:0.842697,\n",
      "save new model loss,now loss is  0.42893826961517334\n",
      "save new model acc,now acc is  tensor(0.8427, device='cuda:0')\n",
      "Epoch 5/59\n",
      "----------\n",
      "[epoch:5,batch:29]:acc: 0.851042,loss:0.419533\n",
      "[epoch:5,batch:59]:acc: 0.839063,loss:0.410290\n",
      "[epoch:5,batch:89]:acc: 0.836111,loss:0.406476\n",
      "[epoch:5,batch:119]:acc: 0.838542,loss:0.402007\n",
      "[epoch:5,batch:149]:acc: 0.844375,loss:0.389906\n",
      "[epoch:5,batch:179]:acc: 0.846701,loss:0.387389\n",
      "[epoch:5,batch:209]:acc: 0.849107,loss:0.385256\n",
      "[epoch:5,batch:239]:acc: 0.848568,loss:0.385926\n",
      "[epoch:5,batch:269]:acc: 0.848727,loss:0.386485\n",
      "[epoch:5,batch:299]:acc: 0.847604,loss:0.386879\n",
      "[epoch:5,batch:299]: val_loss:0.447076,val_acc:0.828156,val_total:4539\n",
      "[epoch:5,batch:329]:acc: 0.847348,loss:0.386342\n",
      "[epoch:5,batch:359]:acc: 0.846615,loss:0.385325\n",
      "[epoch:5,batch:389]:acc: 0.845513,loss:0.384463\n",
      "[epoch:5,batch:419]:acc: 0.845164,loss:0.385096\n",
      "[epoch:5,batch:449]:acc: 0.845000,loss:0.385493\n",
      "[epoch:5,batch:479]:acc: 0.843750,loss:0.387294\n",
      "[epoch:5,batch:509]:acc: 0.844179,loss:0.384994\n",
      "[epoch:5,batch:539]:acc: 0.844792,loss:0.384400\n",
      "[epoch:5,batch:569]:acc: 0.845559,loss:0.382883\n",
      "[epoch:5,batch:599]:acc: 0.845521,loss:0.385153\n",
      "[epoch:5,batch:599]: val_loss:0.437720,val_acc:0.838951,val_total:4539\n",
      "[epoch:5,batch:629]:acc: 0.845833,loss:0.384499\n",
      "[epoch:5,batch:659]:acc: 0.845123,loss:0.386102\n",
      "[epoch:5,batch:689]:acc: 0.845290,loss:0.386591\n",
      "[epoch:5,batch:719]:acc: 0.845313,loss:0.385057\n",
      "[epoch:5,batch:749]:acc: 0.844500,loss:0.386185\n",
      "[epoch:5,batch:779]:acc: 0.844832,loss:0.386141\n",
      "[epoch:5,batch:809]:acc: 0.844985,loss:0.386471\n",
      "[epoch:5,batch:839]:acc: 0.844606,loss:0.387846\n",
      "[epoch:5,batch:869]:acc: 0.844325,loss:0.388838\n",
      "[epoch:5,batch:899]:acc: 0.843993,loss:0.389371\n",
      "[epoch:5,batch:899]: val_loss:0.440700,val_acc:0.840053,val_total:4539\n",
      "[epoch:5,batch:929]:acc: 0.844825,loss:0.387785\n",
      "[epoch:5,batch:959]:acc: 0.844987,loss:0.387355\n",
      "[epoch:5,batch:989]:acc: 0.845107,loss:0.387614\n",
      "[epoch:5] :acc: 0.845193,loss:0.387316,lr:0.000100,patience:0\n",
      "[epoch:5]: val_loss:0.423850,val_acc:0.837850,\n",
      "save new model loss,now loss is  0.4238504469394684\n",
      "Epoch 6/59\n",
      "----------\n",
      "[epoch:6,batch:29]:acc: 0.868750,loss:0.370289\n",
      "[epoch:6,batch:59]:acc: 0.867708,loss:0.349001\n",
      "[epoch:6,batch:89]:acc: 0.867708,loss:0.340749\n",
      "[epoch:6,batch:119]:acc: 0.868750,loss:0.345413\n",
      "[epoch:6,batch:149]:acc: 0.860833,loss:0.358161\n",
      "[epoch:6,batch:179]:acc: 0.862847,loss:0.347472\n",
      "[epoch:6,batch:209]:acc: 0.861310,loss:0.349587\n",
      "[epoch:6,batch:239]:acc: 0.860417,loss:0.354916\n",
      "[epoch:6,batch:269]:acc: 0.860995,loss:0.353198\n",
      "[epoch:6,batch:299]:acc: 0.862292,loss:0.351699\n",
      "[epoch:6,batch:299]: val_loss:0.428305,val_acc:0.843358,val_total:4539\n",
      "[epoch:6,batch:329]:acc: 0.863163,loss:0.349327\n",
      "[epoch:6,batch:359]:acc: 0.863281,loss:0.346849\n",
      "[epoch:6,batch:389]:acc: 0.862660,loss:0.345656\n",
      "[epoch:6,batch:419]:acc: 0.862351,loss:0.347000\n",
      "[epoch:6,batch:449]:acc: 0.861528,loss:0.348277\n",
      "[epoch:6,batch:479]:acc: 0.861458,loss:0.347171\n",
      "[epoch:6,batch:509]:acc: 0.861213,loss:0.347786\n",
      "[epoch:6,batch:539]:acc: 0.861111,loss:0.349198\n",
      "[epoch:6,batch:569]:acc: 0.860417,loss:0.349852\n",
      "[epoch:6,batch:599]:acc: 0.859115,loss:0.352958\n",
      "[epoch:6,batch:599]: val_loss:0.417223,val_acc:0.840053,val_total:4539\n",
      "[epoch:6,batch:629]:acc: 0.858879,loss:0.353216\n",
      "[epoch:6,batch:659]:acc: 0.858428,loss:0.353486\n",
      "[epoch:6,batch:689]:acc: 0.858696,loss:0.354081\n",
      "[epoch:6,batch:719]:acc: 0.858030,loss:0.355026\n",
      "[epoch:6,batch:749]:acc: 0.858000,loss:0.354662\n",
      "[epoch:6,batch:779]:acc: 0.856851,loss:0.356723\n",
      "[epoch:6,batch:809]:acc: 0.856674,loss:0.356870\n",
      "[epoch:6,batch:839]:acc: 0.856473,loss:0.356568\n",
      "[epoch:6,batch:869]:acc: 0.855927,loss:0.355706\n",
      "[epoch:6,batch:899]:acc: 0.856493,loss:0.354446\n",
      "[epoch:6,batch:899]: val_loss:0.405890,val_acc:0.847323,val_total:4539\n",
      "[epoch:6,batch:929]:acc: 0.856317,loss:0.354175\n",
      "[epoch:6,batch:959]:acc: 0.856510,loss:0.354615\n",
      "[epoch:6,batch:989]:acc: 0.856029,loss:0.354302\n",
      "[epoch:6] :acc: 0.856102,loss:0.354178,lr:0.000100,patience:0\n",
      "[epoch:6]: val_loss:0.435168,val_acc:0.845561,\n",
      "save new model acc,now acc is  tensor(0.8456, device='cuda:0')\n",
      "Epoch 7/59\n",
      "----------\n",
      "[epoch:7,batch:29]:acc: 0.869792,loss:0.315589\n",
      "[epoch:7,batch:59]:acc: 0.878125,loss:0.303248\n",
      "[epoch:7,batch:89]:acc: 0.878819,loss:0.302894\n",
      "[epoch:7,batch:119]:acc: 0.877604,loss:0.304082\n",
      "[epoch:7,batch:149]:acc: 0.875833,loss:0.303985\n",
      "[epoch:7,batch:179]:acc: 0.872917,loss:0.308128\n",
      "[epoch:7,batch:209]:acc: 0.874405,loss:0.304832\n",
      "[epoch:7,batch:239]:acc: 0.869792,loss:0.314082\n",
      "[epoch:7,batch:269]:acc: 0.871528,loss:0.311951\n",
      "[epoch:7,batch:299]:acc: 0.870000,loss:0.312345\n",
      "[epoch:7,batch:299]: val_loss:0.423219,val_acc:0.846442,val_total:4539\n",
      "[epoch:7,batch:329]:acc: 0.870170,loss:0.312210\n",
      "[epoch:7,batch:359]:acc: 0.869444,loss:0.316446\n",
      "[epoch:7,batch:389]:acc: 0.868269,loss:0.316892\n",
      "[epoch:7,batch:419]:acc: 0.867932,loss:0.316587\n",
      "[epoch:7,batch:449]:acc: 0.867639,loss:0.317089\n",
      "[epoch:7,batch:479]:acc: 0.865885,loss:0.319070\n",
      "[epoch:7,batch:509]:acc: 0.865196,loss:0.319748\n",
      "[epoch:7,batch:539]:acc: 0.864062,loss:0.321905\n",
      "[epoch:7,batch:569]:acc: 0.864364,loss:0.321409\n",
      "[epoch:7,batch:599]:acc: 0.863750,loss:0.322012\n",
      "[epoch:7,batch:599]: val_loss:0.445939,val_acc:0.836528,val_total:4539\n",
      "[epoch:7,batch:629]:acc: 0.864187,loss:0.322495\n",
      "[epoch:7,batch:659]:acc: 0.865057,loss:0.321307\n",
      "[epoch:7,batch:689]:acc: 0.865580,loss:0.321564\n",
      "[epoch:7,batch:719]:acc: 0.865885,loss:0.321016\n",
      "[epoch:7,batch:749]:acc: 0.866083,loss:0.321514\n",
      "[epoch:7,batch:779]:acc: 0.865425,loss:0.322651\n",
      "[epoch:7,batch:809]:acc: 0.865123,loss:0.323427\n",
      "[epoch:7,batch:839]:acc: 0.864509,loss:0.324938\n",
      "[epoch:7,batch:869]:acc: 0.865086,loss:0.324107\n",
      "[epoch:7,batch:899]:acc: 0.865104,loss:0.323519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-159:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lujunfeng/anaconda3/envs/Conda_Env_Pytorch/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/lujunfeng/anaconda3/envs/Conda_Env_Pytorch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/lujunfeng/anaconda3/envs/Conda_Env_Pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 110, in _worker_loop\n",
      "    data_queue.put((idx, samples))\n",
      "  File \"/home/lujunfeng/anaconda3/envs/Conda_Env_Pytorch/lib/python3.6/multiprocessing/queues.py\", line 341, in put\n",
      "    obj = _ForkingPickler.dumps(obj)\n",
      "  File \"/home/lujunfeng/anaconda3/envs/Conda_Env_Pytorch/lib/python3.6/multiprocessing/reduction.py\", line 51, in dumps\n",
      "    cls(buf, protocol).dump(obj)\n",
      "  File \"/home/lujunfeng/anaconda3/envs/Conda_Env_Pytorch/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 194, in reduce_storage\n",
      "    df = multiprocessing.reduction.DupFd(fd)\n",
      "  File \"/home/lujunfeng/anaconda3/envs/Conda_Env_Pytorch/lib/python3.6/multiprocessing/reduction.py\", line 191, in DupFd\n",
      "    return resource_sharer.DupFd(fd)\n",
      "  File \"/home/lujunfeng/anaconda3/envs/Conda_Env_Pytorch/lib/python3.6/multiprocessing/resource_sharer.py\", line 53, in __init__\n",
      "    self._id = _resource_sharer.register(send, close)\n",
      "  File \"/home/lujunfeng/anaconda3/envs/Conda_Env_Pytorch/lib/python3.6/multiprocessing/resource_sharer.py\", line 77, in register\n",
      "    self._start()\n",
      "  File \"/home/lujunfeng/anaconda3/envs/Conda_Env_Pytorch/lib/python3.6/multiprocessing/resource_sharer.py\", line 130, in _start\n",
      "    self._listener = Listener(authkey=process.current_process().authkey)\n",
      "  File \"/home/lujunfeng/anaconda3/envs/Conda_Env_Pytorch/lib/python3.6/multiprocessing/connection.py\", line 432, in __init__\n",
      "    address = address or arbitrary_address(family)\n",
      "  File \"/home/lujunfeng/anaconda3/envs/Conda_Env_Pytorch/lib/python3.6/multiprocessing/connection.py\", line 76, in arbitrary_address\n",
      "    return tempfile.mktemp(prefix='listener-', dir=util.get_temp_dir())\n",
      "  File \"/home/lujunfeng/anaconda3/envs/Conda_Env_Pytorch/lib/python3.6/multiprocessing/util.py\", line 114, in get_temp_dir\n",
      "    tempdir = tempfile.mkdtemp(prefix='pymp-')\n",
      "  File \"/home/lujunfeng/anaconda3/envs/Conda_Env_Pytorch/lib/python3.6/tempfile.py\", line 370, in mkdtemp\n",
      "    _os.mkdir(file, 0o700)\n",
      "OSError: [Errno 28] No space left on device: '/tmp/pymp-azpf1hk1'\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid 28821) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1bd81551cddb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-641b84b1a2a0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochNum)\u001b[0m\n\u001b[1;32m     59\u001b[0m                     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train/Acc'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrunning_corrects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mniter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train/Loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrunning_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mniter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                     \u001b[0mlx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_dataLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                     \u001b[0mlog_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                     \u001b[0mlog_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/lujunfeng/CropNewBackUp/code/utils.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mall_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Conda_Env_Pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Conda_Env_Pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    307\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader timed out after {} seconds'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Conda_Env_Pytorch/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Conda_Env_Pytorch/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"negative maxlength\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bad_message_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Conda_Env_Pytorch/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Conda_Env_Pytorch/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Conda_Env_Pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# Python can still get and update the process status successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0m_error_if_any_worker_fails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprevious_handler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mprevious_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 28821) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace."
     ]
    }
   ],
   "source": [
    "train(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reuseTrain(path,epochNum):\n",
    "    writer=SummaryWriter('log/'+date+'/DesNet121/') # 创建 /log/日期/InceptionResnet的组织形式\n",
    "    train_dataset=MyDataSet(json_Description=ANNOTATION_TRAIN,transform=preprocess_with_augmentation(normalize_torch,IMAGE_SIZE),path_pre=IMAGE_TRAIN_PRE)\n",
    "    val_dataset=MyDataSet(json_Description=ANNOTATION_VAL,transform=preprocess(normalize_torch,IMAGE_SIZE),path_pre=IMAGE_VAL_PRE)\n",
    "    train_dataLoader=DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE,num_workers=16,shuffle=True)\n",
    "    val_dataLoader=DataLoader(dataset=val_dataset,batch_size=BATCH_SIZE,num_workers=1,shuffle=False)\n",
    "    model=getmodel()\n",
    "    criterion=nn.CrossEntropyLoss().cuda()\n",
    "    modelParams=torch.load(path)\n",
    "    model.load_state_dict(modelParams['state_dict'])\n",
    "    min_loss=modelParams['val_loss']\n",
    "    print('min_loss is :%f'%(min_loss))   \n",
    "    min_acc=max(modelParams['val_correct'],0.81)\n",
    "    print('val_correct is %f'%(min_acc))\n",
    "    patience=0\n",
    "    lr=1e-4\n",
    "    momentum=0.9\n",
    "    beginepoch=modelParams['epoch']\n",
    "    for epoch in range(beginepoch,epochNum):\n",
    "        print('Epoch {}/{}'.format(epoch, epochNum - 1))\n",
    "        print('-' * 10)\n",
    "        if patience==2:\n",
    "            patience=0\n",
    "            model.load_state_dict(torch.load('../model/DesNet121/'+date+'_loss_best.pth')['state_dict'])\n",
    "            lr=lr/10\n",
    "        optimizer = torch.optim.Adam(model.parameters(),lr = lr,amsgrad=True,weight_decay=1e-4)\n",
    "        print('lr now is %f'%(lr))\n",
    "        print('now patience is %d '%(patience))\n",
    "        running_loss = RunningMean()\n",
    "        running_corrects = RunningMean()\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_dataLoader):\n",
    "            model.train(True)\n",
    "            n_batchsize=inputs.size(0)\n",
    "            inputs = Variable(inputs).cuda()\n",
    "            labels = Variable(labels).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            if isinstance(outputs,tuple):\n",
    "                loss=sum((criterion(o,labels)) for o in outputs)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "            running_loss.update(loss.item(),1)\n",
    "            running_corrects.update(torch.sum(preds == labels.data).data,n_batchsize)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx%30==29:\n",
    "                print('[epoch:%d,batch:%d]:acc: %f,loss:%f'%(epoch,batch_idx,running_corrects.value,running_loss.value))\n",
    "                if batch_idx%300==299: \n",
    "                    niter = epoch * len(train_dataset)/BATCH_SIZE + batch_idx\n",
    "                    writer.add_scalar('Train/Acc',running_corrects.value,niter)\n",
    "                    writer.add_scalar('Train/Loss',running_loss.value,niter)\n",
    "                    lx,px=utils.predict(model,val_dataLoader)\n",
    "                    log_loss = criterion(Variable(px), Variable(lx))\n",
    "                    log_loss = log_loss.item()\n",
    "                    _, preds = torch.max(px, dim=1)\n",
    "                    accuracy = torch.mean((preds == lx).float())\n",
    "                    writer.add_scalar('Val/Acc',accuracy,niter)\n",
    "                    writer.add_scalar('Val/Loss',log_loss,niter)\n",
    "                    print('[epoch:%d,batch:%d]: val_loss:%f,val_acc:%f,val_total:%d'%(epoch,batch_idx,log_loss,accuracy,len(val_dataset)))\n",
    "        print('[epoch:%d] :acc: %f,loss:%f,lr:%f,patience:%d'%(epoch,running_corrects.value,running_loss.value,lr,patience))      \n",
    "        lx,px=utils.predict(model,val_dataLoader)\n",
    "        log_loss = criterion(Variable(px), Variable(lx))\n",
    "        log_loss = log_loss.item()\n",
    "        _, preds = torch.max(px, dim=1)\n",
    "        accuracy = torch.mean((preds == lx).float())\n",
    "        writer.add_scalar('Val/Acc',accuracy,(epoch+1) * len(train_dataset)/BATCH_SIZE)\n",
    "        writer.add_scalar('Val/Loss',log_loss,(epoch+1) * len(train_dataset)/BATCH_SIZE)\n",
    "        print('[epoch:%d]: val_loss:%f,val_acc:%f,'%(epoch,log_loss,accuracy))\n",
    "        if  log_loss < min_loss:\n",
    "            utils.snapshot('../model/', 'DesNet121', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy })          \n",
    "            patience = 0\n",
    "            min_loss=log_loss\n",
    "            print('save new model loss,now loss is ',min_loss)\n",
    "        else:\n",
    "            patience += 1\n",
    "        if accuracy>min_acc:\n",
    "            utils.snapshot('../model/', 'DesNet121', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy },key='acc') \n",
    "            min_acc=accuracy\n",
    "            print('save new model acc,now acc is ',min_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] loading model... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lujunfeng/anaconda3/envs/Conda_Env_Pytorch/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "min_loss is :0.435168\n",
      "val_correct is 0.845561\n",
      "Epoch 7/59\n",
      "----------\n",
      "lr now is 0.000100\n",
      "now patience is 0 \n",
      "[epoch:7,batch:29]:acc: 0.842708,loss:0.358491\n",
      "[epoch:7,batch:59]:acc: 0.851562,loss:0.333651\n",
      "[epoch:7,batch:89]:acc: 0.861458,loss:0.315530\n",
      "[epoch:7,batch:119]:acc: 0.861979,loss:0.314100\n",
      "[epoch:7,batch:149]:acc: 0.866042,loss:0.306455\n",
      "[epoch:7,batch:179]:acc: 0.867708,loss:0.306235\n",
      "[epoch:7,batch:209]:acc: 0.868155,loss:0.306727\n",
      "[epoch:7,batch:239]:acc: 0.865755,loss:0.307462\n",
      "[epoch:7,batch:269]:acc: 0.866435,loss:0.304891\n",
      "[epoch:7,batch:299]:acc: 0.865833,loss:0.305342\n",
      "[epoch:7,batch:299]: val_loss:0.406287,val_acc:0.840934,val_total:4539\n",
      "[epoch:7,batch:329]:acc: 0.865814,loss:0.305691\n",
      "[epoch:7,batch:359]:acc: 0.865799,loss:0.306098\n",
      "[epoch:7,batch:389]:acc: 0.864984,loss:0.307426\n",
      "[epoch:7,batch:419]:acc: 0.864807,loss:0.308842\n",
      "[epoch:7,batch:449]:acc: 0.865139,loss:0.308225\n",
      "[epoch:7,batch:479]:acc: 0.864648,loss:0.308805\n",
      "[epoch:7,batch:509]:acc: 0.865196,loss:0.308384\n",
      "[epoch:7,batch:539]:acc: 0.865567,loss:0.308288\n",
      "[epoch:7,batch:569]:acc: 0.865461,loss:0.308884\n",
      "[epoch:7,batch:599]:acc: 0.865521,loss:0.309693\n",
      "[epoch:7,batch:599]: val_loss:0.387506,val_acc:0.841815,val_total:4539\n",
      "[epoch:7,batch:629]:acc: 0.866915,loss:0.307842\n",
      "[epoch:7,batch:659]:acc: 0.866572,loss:0.308943\n",
      "[epoch:7,batch:689]:acc: 0.867074,loss:0.309073\n",
      "[epoch:7,batch:719]:acc: 0.866102,loss:0.310447\n",
      "[epoch:7,batch:749]:acc: 0.866792,loss:0.310044\n",
      "[epoch:7,batch:779]:acc: 0.866026,loss:0.310419\n",
      "[epoch:7,batch:809]:acc: 0.865779,loss:0.311756\n",
      "[epoch:7,batch:839]:acc: 0.865253,loss:0.313348\n",
      "[epoch:7,batch:869]:acc: 0.864691,loss:0.314989\n",
      "[epoch:7,batch:899]:acc: 0.863750,loss:0.316417\n",
      "[epoch:7,batch:899]: val_loss:0.383793,val_acc:0.846662,val_total:4539\n",
      "[epoch:7,batch:929]:acc: 0.863810,loss:0.316331\n",
      "[epoch:7,batch:959]:acc: 0.864160,loss:0.316207\n",
      "[epoch:7,batch:989]:acc: 0.864994,loss:0.314916\n",
      "[epoch:7] :acc: 0.864994,loss:0.314812,lr:0.000100,patience:0\n",
      "[epoch:7]: val_loss:0.374975,val_acc:0.854373,\n",
      "save new model loss,now loss is  0.3749748766422272\n",
      "save new model acc,now acc is  tensor(0.8544, device='cuda:0')\n",
      "Epoch 8/59\n",
      "----------\n",
      "lr now is 0.000100\n",
      "now patience is 0 \n",
      "[epoch:8,batch:29]:acc: 0.864583,loss:0.315008\n",
      "[epoch:8,batch:59]:acc: 0.878646,loss:0.287013\n",
      "[epoch:8,batch:89]:acc: 0.885417,loss:0.274786\n",
      "[epoch:8,batch:119]:acc: 0.887500,loss:0.275129\n",
      "[epoch:8,batch:149]:acc: 0.884167,loss:0.280993\n",
      "[epoch:8,batch:179]:acc: 0.881771,loss:0.283136\n",
      "[epoch:8,batch:209]:acc: 0.881399,loss:0.282234\n",
      "[epoch:8,batch:239]:acc: 0.880599,loss:0.284152\n",
      "[epoch:8,batch:269]:acc: 0.880324,loss:0.285035\n",
      "[epoch:8,batch:299]:acc: 0.879271,loss:0.285224\n",
      "[epoch:8,batch:299]: val_loss:0.396764,val_acc:0.836308,val_total:4539\n",
      "[epoch:8,batch:329]:acc: 0.878788,loss:0.285469\n",
      "[epoch:8,batch:359]:acc: 0.878906,loss:0.284056\n",
      "[epoch:8,batch:389]:acc: 0.879006,loss:0.282735\n",
      "[epoch:8,batch:419]:acc: 0.876935,loss:0.284791\n",
      "[epoch:8,batch:449]:acc: 0.876319,loss:0.286256\n",
      "[epoch:8,batch:479]:acc: 0.876953,loss:0.286282\n",
      "[epoch:8,batch:509]:acc: 0.877390,loss:0.285565\n",
      "[epoch:8,batch:539]:acc: 0.876968,loss:0.286441\n",
      "[epoch:8,batch:569]:acc: 0.876864,loss:0.287151\n",
      "[epoch:8,batch:599]:acc: 0.877292,loss:0.285942\n",
      "[epoch:8,batch:599]: val_loss:0.386680,val_acc:0.847764,val_total:4539\n",
      "[epoch:8,batch:629]:acc: 0.876687,loss:0.286779\n",
      "[epoch:8,batch:659]:acc: 0.877178,loss:0.286857\n",
      "[epoch:8,batch:689]:acc: 0.877083,loss:0.287136\n",
      "[epoch:8,batch:719]:acc: 0.876693,loss:0.287894\n",
      "[epoch:8,batch:749]:acc: 0.876875,loss:0.287463\n",
      "[epoch:8,batch:779]:acc: 0.876282,loss:0.288617\n",
      "[epoch:8,batch:809]:acc: 0.876312,loss:0.288556\n",
      "[epoch:8,batch:839]:acc: 0.876265,loss:0.289205\n",
      "[epoch:8,batch:869]:acc: 0.876114,loss:0.289133\n",
      "[epoch:8,batch:899]:acc: 0.875972,loss:0.288529\n",
      "[epoch:8,batch:899]: val_loss:0.408918,val_acc:0.838290,val_total:4539\n",
      "[epoch:8,batch:929]:acc: 0.876142,loss:0.288214\n",
      "[epoch:8,batch:959]:acc: 0.876107,loss:0.288514\n",
      "[epoch:8,batch:989]:acc: 0.875979,loss:0.289181\n",
      "[epoch:8] :acc: 0.875997,loss:0.289875,lr:0.000100,patience:0\n",
      "[epoch:8]: val_loss:0.403059,val_acc:0.846001,\n",
      "Epoch 9/59\n",
      "----------\n",
      "lr now is 0.000100\n",
      "now patience is 1 \n",
      "[epoch:9,batch:29]:acc: 0.889583,loss:0.257054\n",
      "[epoch:9,batch:59]:acc: 0.889583,loss:0.259741\n",
      "[epoch:9,batch:89]:acc: 0.893750,loss:0.253338\n",
      "[epoch:9,batch:119]:acc: 0.889583,loss:0.265608\n",
      "[epoch:9,batch:149]:acc: 0.891458,loss:0.259573\n",
      "[epoch:9,batch:179]:acc: 0.889757,loss:0.262330\n",
      "[epoch:9,batch:209]:acc: 0.889137,loss:0.261738\n",
      "[epoch:9,batch:239]:acc: 0.889714,loss:0.260661\n",
      "[epoch:9,batch:269]:acc: 0.888657,loss:0.262800\n",
      "[epoch:9,batch:299]:acc: 0.888021,loss:0.263427\n",
      "[epoch:9,batch:299]: val_loss:0.398249,val_acc:0.846222,val_total:4539\n",
      "[epoch:9,batch:329]:acc: 0.886553,loss:0.266777\n",
      "[epoch:9,batch:359]:acc: 0.886979,loss:0.266157\n",
      "[epoch:9,batch:389]:acc: 0.885657,loss:0.267742\n",
      "[epoch:9,batch:419]:acc: 0.884375,loss:0.268804\n",
      "[epoch:9,batch:449]:acc: 0.884514,loss:0.267359\n",
      "[epoch:9,batch:479]:acc: 0.884375,loss:0.269426\n",
      "[epoch:9,batch:509]:acc: 0.883762,loss:0.270461\n",
      "[epoch:9,batch:539]:acc: 0.883738,loss:0.270449\n",
      "[epoch:9,batch:569]:acc: 0.883717,loss:0.270843\n",
      "[epoch:9,batch:599]:acc: 0.883490,loss:0.270554\n",
      "[epoch:9,batch:599]: val_loss:0.404734,val_acc:0.847543,val_total:4539\n",
      "[epoch:9,batch:629]:acc: 0.884276,loss:0.269394\n",
      "[epoch:9,batch:659]:acc: 0.884659,loss:0.269555\n",
      "[epoch:9,batch:689]:acc: 0.884647,loss:0.269224\n",
      "[epoch:9,batch:719]:acc: 0.884071,loss:0.270002\n",
      "[epoch:9,batch:749]:acc: 0.885208,loss:0.269266\n",
      "[epoch:9,batch:779]:acc: 0.884615,loss:0.270904\n",
      "[epoch:9,batch:809]:acc: 0.883796,loss:0.272544\n",
      "[epoch:9,batch:839]:acc: 0.884077,loss:0.272226\n",
      "[epoch:9,batch:869]:acc: 0.883872,loss:0.273316\n",
      "[epoch:9,batch:899]:acc: 0.883854,loss:0.273394\n",
      "[epoch:9,batch:899]: val_loss:0.391949,val_acc:0.842917,val_total:4539\n",
      "[epoch:9,batch:929]:acc: 0.883737,loss:0.273731\n",
      "[epoch:9,batch:959]:acc: 0.883952,loss:0.273007\n",
      "[epoch:9,batch:989]:acc: 0.883744,loss:0.273621\n",
      "[epoch:9] :acc: 0.883785,loss:0.273799,lr:0.000100,patience:1\n",
      "[epoch:9]: val_loss:0.402277,val_acc:0.842917,\n",
      "Epoch 10/59\n",
      "----------\n",
      "lr now is 0.000010\n",
      "now patience is 0 \n",
      "[epoch:10,batch:29]:acc: 0.893750,loss:0.252251\n",
      "[epoch:10,batch:59]:acc: 0.881771,loss:0.271981\n",
      "[epoch:10,batch:89]:acc: 0.884375,loss:0.268765\n",
      "[epoch:10,batch:119]:acc: 0.881250,loss:0.271503\n",
      "[epoch:10,batch:149]:acc: 0.885833,loss:0.264800\n",
      "[epoch:10,batch:179]:acc: 0.886111,loss:0.262533\n",
      "[epoch:10,batch:209]:acc: 0.888542,loss:0.261588\n",
      "[epoch:10,batch:239]:acc: 0.889062,loss:0.259215\n",
      "[epoch:10,batch:269]:acc: 0.891551,loss:0.255163\n",
      "[epoch:10,batch:299]:acc: 0.891458,loss:0.255052\n",
      "[epoch:10,batch:299]: val_loss:0.364233,val_acc:0.857458,val_total:4539\n",
      "[epoch:10,batch:329]:acc: 0.893371,loss:0.253969\n",
      "[epoch:10,batch:359]:acc: 0.893403,loss:0.252836\n",
      "[epoch:10,batch:389]:acc: 0.893830,loss:0.251527\n",
      "[epoch:10,batch:419]:acc: 0.893676,loss:0.250434\n",
      "[epoch:10,batch:449]:acc: 0.892708,loss:0.251653\n",
      "[epoch:10,batch:479]:acc: 0.892773,loss:0.250877\n",
      "[epoch:10,batch:509]:acc: 0.893382,loss:0.248568\n",
      "[epoch:10,batch:539]:acc: 0.893924,loss:0.247715\n",
      "[epoch:10,batch:569]:acc: 0.893750,loss:0.247376\n",
      "[epoch:10,batch:599]:acc: 0.894323,loss:0.247301\n",
      "[epoch:10,batch:599]: val_loss:0.353606,val_acc:0.860101,val_total:4539\n",
      "[epoch:10,batch:629]:acc: 0.894692,loss:0.246599\n",
      "[epoch:10,batch:659]:acc: 0.895644,loss:0.244677\n",
      "[epoch:10,batch:689]:acc: 0.896784,loss:0.242498\n",
      "[epoch:10,batch:719]:acc: 0.897005,loss:0.242454\n",
      "[epoch:10,batch:749]:acc: 0.897000,loss:0.242008\n",
      "[epoch:10,batch:779]:acc: 0.897356,loss:0.241163\n",
      "[epoch:10,batch:809]:acc: 0.897454,loss:0.240780\n",
      "[epoch:10,batch:839]:acc: 0.897731,loss:0.239866\n",
      "[epoch:10,batch:869]:acc: 0.898240,loss:0.239049\n",
      "[epoch:10,batch:899]:acc: 0.898333,loss:0.238815\n",
      "[epoch:10,batch:899]: val_loss:0.358483,val_acc:0.855034,val_total:4539\n",
      "[epoch:10,batch:929]:acc: 0.898387,loss:0.239156\n",
      "[epoch:10,batch:959]:acc: 0.898177,loss:0.239871\n",
      "[epoch:10,batch:989]:acc: 0.898327,loss:0.239770\n",
      "[epoch:10] :acc: 0.898256,loss:0.240440,lr:0.000010,patience:0\n",
      "[epoch:10]: val_loss:0.355801,val_acc:0.858779,\n",
      "save new model loss,now loss is  0.35580143332481384\n",
      "save new model acc,now acc is  tensor(0.8588, device='cuda:0')\n",
      "Epoch 11/59\n",
      "----------\n",
      "lr now is 0.000010\n",
      "now patience is 0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:11,batch:29]:acc: 0.911458,loss:0.228214\n",
      "[epoch:11,batch:59]:acc: 0.901563,loss:0.229808\n",
      "[epoch:11,batch:89]:acc: 0.900694,loss:0.231198\n",
      "[epoch:11,batch:119]:acc: 0.907292,loss:0.220020\n",
      "[epoch:11,batch:149]:acc: 0.905833,loss:0.222408\n",
      "[epoch:11,batch:179]:acc: 0.903472,loss:0.226043\n",
      "[epoch:11,batch:209]:acc: 0.905208,loss:0.226821\n",
      "[epoch:11,batch:239]:acc: 0.905859,loss:0.224206\n",
      "[epoch:11,batch:269]:acc: 0.906713,loss:0.222694\n",
      "[epoch:11,batch:299]:acc: 0.908750,loss:0.222007\n",
      "[epoch:11,batch:299]: val_loss:0.352736,val_acc:0.862745,val_total:4539\n",
      "[epoch:11,batch:329]:acc: 0.908807,loss:0.222475\n",
      "[epoch:11,batch:359]:acc: 0.907813,loss:0.223383\n",
      "[epoch:11,batch:389]:acc: 0.907452,loss:0.223969\n",
      "[epoch:11,batch:419]:acc: 0.907515,loss:0.222870\n",
      "[epoch:11,batch:449]:acc: 0.908264,loss:0.222162\n",
      "[epoch:11,batch:479]:acc: 0.908268,loss:0.222868\n",
      "[epoch:11,batch:509]:acc: 0.907843,loss:0.222624\n",
      "[epoch:11,batch:539]:acc: 0.908218,loss:0.222448\n",
      "[epoch:11,batch:569]:acc: 0.907730,loss:0.222596\n",
      "[epoch:11,batch:599]:acc: 0.908281,loss:0.221889\n",
      "[epoch:11,batch:599]: val_loss:0.353336,val_acc:0.861864,val_total:4539\n",
      "[epoch:11,batch:629]:acc: 0.909127,loss:0.220504\n",
      "[epoch:11,batch:659]:acc: 0.909138,loss:0.220437\n",
      "[epoch:11,batch:689]:acc: 0.908741,loss:0.220605\n",
      "[epoch:11,batch:719]:acc: 0.908507,loss:0.220833\n",
      "[epoch:11,batch:749]:acc: 0.908583,loss:0.220450\n",
      "[epoch:11,batch:779]:acc: 0.908654,loss:0.221123\n",
      "[epoch:11,batch:809]:acc: 0.907677,loss:0.222356\n",
      "[epoch:11,batch:839]:acc: 0.907292,loss:0.222355\n",
      "[epoch:11,batch:869]:acc: 0.907076,loss:0.222349\n",
      "[epoch:11,batch:899]:acc: 0.907743,loss:0.221267\n",
      "[epoch:11,batch:899]: val_loss:0.351703,val_acc:0.864948,val_total:4539\n",
      "[epoch:11,batch:929]:acc: 0.907661,loss:0.221080\n",
      "[epoch:11,batch:959]:acc: 0.906966,loss:0.221797\n",
      "[epoch:11,batch:989]:acc: 0.906913,loss:0.222012\n",
      "[epoch:11] :acc: 0.906958,loss:0.221902,lr:0.000010,patience:0\n",
      "[epoch:11]: val_loss:0.355611,val_acc:0.862084,\n",
      "save new model loss,now loss is  0.35561108589172363\n",
      "save new model acc,now acc is  tensor(0.8621, device='cuda:0')\n",
      "Epoch 12/59\n",
      "----------\n",
      "lr now is 0.000010\n",
      "now patience is 0 \n",
      "[epoch:12,batch:29]:acc: 0.914583,loss:0.211286\n",
      "[epoch:12,batch:59]:acc: 0.917188,loss:0.206885\n",
      "[epoch:12,batch:89]:acc: 0.919792,loss:0.205652\n",
      "[epoch:12,batch:119]:acc: 0.916927,loss:0.202124\n",
      "[epoch:12,batch:149]:acc: 0.917708,loss:0.200154\n",
      "[epoch:12,batch:179]:acc: 0.918924,loss:0.196809\n",
      "[epoch:12,batch:209]:acc: 0.921429,loss:0.194205\n",
      "[epoch:12,batch:239]:acc: 0.920312,loss:0.196451\n",
      "[epoch:12,batch:269]:acc: 0.919444,loss:0.198593\n",
      "[epoch:12,batch:299]:acc: 0.918542,loss:0.200773\n",
      "[epoch:12,batch:299]: val_loss:0.352837,val_acc:0.864067,val_total:4539\n",
      "[epoch:12,batch:329]:acc: 0.917708,loss:0.201020\n",
      "[epoch:12,batch:359]:acc: 0.917535,loss:0.201781\n",
      "[epoch:12,batch:389]:acc: 0.916907,loss:0.202279\n",
      "[epoch:12,batch:419]:acc: 0.917188,loss:0.202116\n",
      "[epoch:12,batch:449]:acc: 0.917292,loss:0.202327\n",
      "[epoch:12,batch:479]:acc: 0.916667,loss:0.202745\n",
      "[epoch:12,batch:509]:acc: 0.916299,loss:0.203740\n",
      "[epoch:12,batch:539]:acc: 0.916667,loss:0.203548\n",
      "[epoch:12,batch:569]:acc: 0.916831,loss:0.203781\n",
      "[epoch:12,batch:599]:acc: 0.916979,loss:0.203959\n",
      "[epoch:12,batch:599]: val_loss:0.353203,val_acc:0.861643,val_total:4539\n",
      "[epoch:12,batch:629]:acc: 0.916567,loss:0.204980\n",
      "[epoch:12,batch:659]:acc: 0.916383,loss:0.205727\n",
      "[epoch:12,batch:689]:acc: 0.915625,loss:0.206440\n",
      "[epoch:12,batch:719]:acc: 0.915191,loss:0.206738\n",
      "[epoch:12,batch:749]:acc: 0.915208,loss:0.206565\n",
      "[epoch:12,batch:779]:acc: 0.915345,loss:0.206812\n",
      "[epoch:12,batch:809]:acc: 0.915278,loss:0.206769\n",
      "[epoch:12,batch:839]:acc: 0.915216,loss:0.207176\n",
      "[epoch:12,batch:869]:acc: 0.914978,loss:0.207431\n",
      "[epoch:12,batch:899]:acc: 0.914896,loss:0.207910\n",
      "[epoch:12,batch:899]: val_loss:0.351819,val_acc:0.863406,val_total:4539\n",
      "[epoch:12,batch:929]:acc: 0.914987,loss:0.207721\n",
      "[epoch:12,batch:959]:acc: 0.914909,loss:0.207389\n",
      "[epoch:12,batch:989]:acc: 0.914710,loss:0.207596\n",
      "[epoch:12] :acc: 0.914683,loss:0.207820,lr:0.000010,patience:0\n",
      "[epoch:12]: val_loss:0.355013,val_acc:0.862745,\n",
      "save new model loss,now loss is  0.3550127148628235\n",
      "save new model acc,now acc is  tensor(0.8627, device='cuda:0')\n",
      "Epoch 13/59\n",
      "----------\n",
      "lr now is 0.000010\n",
      "now patience is 0 \n",
      "[epoch:13,batch:29]:acc: 0.897917,loss:0.222881\n",
      "[epoch:13,batch:59]:acc: 0.910417,loss:0.206968\n",
      "[epoch:13,batch:89]:acc: 0.916667,loss:0.195566\n",
      "[epoch:13,batch:119]:acc: 0.913542,loss:0.199730\n",
      "[epoch:13,batch:149]:acc: 0.914167,loss:0.199040\n",
      "[epoch:13,batch:179]:acc: 0.911111,loss:0.203459\n",
      "[epoch:13,batch:209]:acc: 0.910268,loss:0.206867\n",
      "[epoch:13,batch:239]:acc: 0.910286,loss:0.205650\n",
      "[epoch:13,batch:269]:acc: 0.908912,loss:0.207100\n",
      "[epoch:13,batch:299]:acc: 0.909687,loss:0.207344\n",
      "[epoch:13,batch:299]: val_loss:0.355329,val_acc:0.862304,val_total:4539\n",
      "[epoch:13,batch:329]:acc: 0.910795,loss:0.205897\n",
      "[epoch:13,batch:359]:acc: 0.912153,loss:0.203924\n",
      "[epoch:13,batch:389]:acc: 0.912260,loss:0.205184\n",
      "[epoch:13,batch:419]:acc: 0.912649,loss:0.203848\n",
      "[epoch:13,batch:449]:acc: 0.912778,loss:0.204686\n",
      "[epoch:13,batch:479]:acc: 0.912760,loss:0.203816\n",
      "[epoch:13,batch:509]:acc: 0.912194,loss:0.204603\n",
      "[epoch:13,batch:539]:acc: 0.912326,loss:0.204510\n",
      "[epoch:13,batch:569]:acc: 0.913048,loss:0.203800\n",
      "[epoch:13,batch:599]:acc: 0.913125,loss:0.204166\n",
      "[epoch:13,batch:599]: val_loss:0.354217,val_acc:0.862745,val_total:4539\n",
      "[epoch:13,batch:629]:acc: 0.913641,loss:0.203068\n",
      "[epoch:13,batch:659]:acc: 0.913826,loss:0.202848\n",
      "[epoch:13,batch:689]:acc: 0.914583,loss:0.202027\n",
      "[epoch:13,batch:719]:acc: 0.914800,loss:0.201456\n",
      "[epoch:13,batch:749]:acc: 0.914333,loss:0.202331\n",
      "[epoch:13,batch:779]:acc: 0.914503,loss:0.201959\n",
      "[epoch:13,batch:809]:acc: 0.914082,loss:0.202374\n",
      "[epoch:13,batch:839]:acc: 0.914062,loss:0.202665\n",
      "[epoch:13,batch:869]:acc: 0.914511,loss:0.201925\n",
      "[epoch:13,batch:899]:acc: 0.914375,loss:0.202117\n",
      "[epoch:13,batch:899]: val_loss:0.356722,val_acc:0.862965,val_total:4539\n",
      "[epoch:13,batch:929]:acc: 0.913978,loss:0.202744\n",
      "[epoch:13,batch:959]:acc: 0.914095,loss:0.202635\n",
      "[epoch:13,batch:989]:acc: 0.914394,loss:0.202033\n",
      "[epoch:13] :acc: 0.914494,loss:0.201918,lr:0.000010,patience:0\n",
      "[epoch:13]: val_loss:0.357086,val_acc:0.861423,\n",
      "Epoch 14/59\n",
      "----------\n",
      "lr now is 0.000010\n",
      "now patience is 1 \n",
      "[epoch:14,batch:29]:acc: 0.919792,loss:0.188454\n",
      "[epoch:14,batch:59]:acc: 0.917708,loss:0.192382\n",
      "[epoch:14,batch:89]:acc: 0.920139,loss:0.193267\n",
      "[epoch:14,batch:119]:acc: 0.923177,loss:0.189836\n",
      "[epoch:14,batch:149]:acc: 0.923125,loss:0.191205\n",
      "[epoch:14,batch:179]:acc: 0.923611,loss:0.188321\n",
      "[epoch:14,batch:209]:acc: 0.922024,loss:0.189994\n",
      "[epoch:14,batch:239]:acc: 0.920964,loss:0.191182\n",
      "[epoch:14,batch:269]:acc: 0.920023,loss:0.192319\n",
      "[epoch:14,batch:299]:acc: 0.920625,loss:0.191133\n",
      "[epoch:14,batch:299]: val_loss:0.363468,val_acc:0.859220,val_total:4539\n",
      "[epoch:14,batch:329]:acc: 0.919508,loss:0.192561\n",
      "[epoch:14,batch:359]:acc: 0.920139,loss:0.191768\n",
      "[epoch:14,batch:389]:acc: 0.920353,loss:0.191785\n",
      "[epoch:14,batch:419]:acc: 0.920164,loss:0.192941\n",
      "[epoch:14,batch:449]:acc: 0.920694,loss:0.192184\n",
      "[epoch:14,batch:479]:acc: 0.920768,loss:0.191778\n",
      "[epoch:14,batch:509]:acc: 0.920527,loss:0.191863\n",
      "[epoch:14,batch:539]:acc: 0.921065,loss:0.191387\n",
      "[epoch:14,batch:569]:acc: 0.921436,loss:0.191132\n",
      "[epoch:14,batch:599]:acc: 0.922188,loss:0.190507\n",
      "[epoch:14,batch:599]: val_loss:0.357535,val_acc:0.862525,val_total:4539\n",
      "[epoch:14,batch:629]:acc: 0.921776,loss:0.191987\n",
      "[epoch:14,batch:659]:acc: 0.921638,loss:0.192499\n",
      "[epoch:14,batch:689]:acc: 0.921241,loss:0.193186\n",
      "[epoch:14,batch:719]:acc: 0.921484,loss:0.192482\n",
      "[epoch:14,batch:749]:acc: 0.921583,loss:0.192282\n",
      "[epoch:14,batch:779]:acc: 0.921875,loss:0.192017\n",
      "[epoch:14,batch:809]:acc: 0.921489,loss:0.192041\n",
      "[epoch:14,batch:839]:acc: 0.921652,loss:0.191761\n",
      "[epoch:14,batch:869]:acc: 0.921552,loss:0.191943\n",
      "[epoch:14,batch:899]:acc: 0.921493,loss:0.191876\n",
      "[epoch:14,batch:899]: val_loss:0.361014,val_acc:0.859000,val_total:4539\n",
      "[epoch:14,batch:929]:acc: 0.921573,loss:0.191999\n",
      "[epoch:14,batch:959]:acc: 0.921159,loss:0.192701\n",
      "[epoch:14,batch:989]:acc: 0.920896,loss:0.192816\n",
      "[epoch:14] :acc: 0.920926,loss:0.192714,lr:0.000010,patience:1\n",
      "[epoch:14]: val_loss:0.357364,val_acc:0.864067,\n",
      "save new model acc,now acc is  tensor(0.8641, device='cuda:0')\n",
      "Epoch 15/59\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr now is 0.000001\n",
      "now patience is 0 \n",
      "[epoch:15,batch:29]:acc: 0.913542,loss:0.198884\n",
      "[epoch:15,batch:59]:acc: 0.913542,loss:0.201259\n",
      "[epoch:15,batch:89]:acc: 0.917014,loss:0.200795\n",
      "[epoch:15,batch:119]:acc: 0.920312,loss:0.200175\n",
      "[epoch:15,batch:149]:acc: 0.918958,loss:0.198547\n",
      "[epoch:15,batch:179]:acc: 0.916667,loss:0.203799\n",
      "[epoch:15,batch:209]:acc: 0.916220,loss:0.203786\n",
      "[epoch:15,batch:239]:acc: 0.916016,loss:0.203271\n",
      "[epoch:15,batch:269]:acc: 0.917130,loss:0.201940\n",
      "[epoch:15,batch:299]:acc: 0.919792,loss:0.199075\n",
      "[epoch:15,batch:299]: val_loss:0.351311,val_acc:0.862745,val_total:4539\n",
      "[epoch:15,batch:329]:acc: 0.920360,loss:0.198618\n",
      "[epoch:15,batch:359]:acc: 0.919965,loss:0.198359\n",
      "[epoch:15,batch:389]:acc: 0.920112,loss:0.198221\n",
      "[epoch:15,batch:419]:acc: 0.920238,loss:0.198653\n",
      "[epoch:15,batch:449]:acc: 0.919514,loss:0.199560\n",
      "[epoch:15,batch:479]:acc: 0.919466,loss:0.199674\n",
      "[epoch:15,batch:509]:acc: 0.919975,loss:0.199075\n",
      "[epoch:15,batch:539]:acc: 0.920775,loss:0.198056\n",
      "[epoch:15,batch:569]:acc: 0.920724,loss:0.198174\n",
      "[epoch:15,batch:599]:acc: 0.920312,loss:0.198825\n",
      "[epoch:15,batch:599]: val_loss:0.353050,val_acc:0.863626,val_total:4539\n",
      "[epoch:15,batch:629]:acc: 0.920883,loss:0.198087\n",
      "[epoch:15,batch:659]:acc: 0.921165,loss:0.197220\n",
      "[epoch:15,batch:689]:acc: 0.920516,loss:0.197960\n",
      "[epoch:15,batch:719]:acc: 0.920356,loss:0.198010\n",
      "[epoch:15,batch:749]:acc: 0.920292,loss:0.198189\n",
      "[epoch:15,batch:779]:acc: 0.919712,loss:0.199075\n",
      "[epoch:15,batch:809]:acc: 0.919792,loss:0.198674\n",
      "[epoch:15,batch:839]:acc: 0.920089,loss:0.198161\n",
      "[epoch:15,batch:869]:acc: 0.920007,loss:0.198277\n",
      "[epoch:15,batch:899]:acc: 0.919687,loss:0.198767\n",
      "[epoch:15,batch:899]: val_loss:0.352926,val_acc:0.864948,val_total:4539\n",
      "[epoch:15,batch:929]:acc: 0.919220,loss:0.199110\n",
      "[epoch:15,batch:959]:acc: 0.919889,loss:0.198444\n",
      "[epoch:15,batch:989]:acc: 0.920265,loss:0.197765\n",
      "[epoch:15] :acc: 0.920295,loss:0.197745,lr:0.000001,patience:0\n",
      "[epoch:15]: val_loss:0.355813,val_acc:0.864067,\n",
      "Epoch 16/59\n",
      "----------\n",
      "lr now is 0.000001\n",
      "now patience is 1 \n",
      "[epoch:16,batch:29]:acc: 0.914583,loss:0.207197\n",
      "[epoch:16,batch:59]:acc: 0.924479,loss:0.187977\n",
      "[epoch:16,batch:89]:acc: 0.922569,loss:0.186156\n",
      "[epoch:16,batch:119]:acc: 0.923177,loss:0.189418\n",
      "[epoch:16,batch:149]:acc: 0.922917,loss:0.189657\n",
      "[epoch:16,batch:179]:acc: 0.925694,loss:0.186605\n",
      "[epoch:16,batch:209]:acc: 0.925893,loss:0.185241\n",
      "[epoch:16,batch:239]:acc: 0.926172,loss:0.185142\n",
      "[epoch:16,batch:269]:acc: 0.925579,loss:0.185163\n",
      "[epoch:16,batch:299]:acc: 0.924896,loss:0.187047\n"
     ]
    }
   ],
   "source": [
    "reuseTrain('../model/DesNet121/2018-11-04_acc_best.pth',60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainWithRawData(path,epochNum):\n",
    "    writer=SummaryWriter('log/'+date+'/DesNet121/') # 创建 /log/日期/InceptionResnet的组织形式\n",
    "    train_dataset=MyDataSet(json_Description=ANNOTATION_TRAIN,transform=preprocess(normalize_torch,IMAGE_SIZE),path_pre=IMAGE_TRAIN_PRE)\n",
    "    val_dataset=MyDataSet(json_Description=ANNOTATION_VAL,transform=preprocess(normalize_torch,IMAGE_SIZE),path_pre=IMAGE_VAL_PRE)\n",
    "    train_dataLoader=DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE,num_workers=16,shuffle=True)\n",
    "    val_dataLoader=DataLoader(dataset=val_dataset,batch_size=BATCH_SIZE,num_workers=1,shuffle=False)\n",
    "    model=getmodel()\n",
    "    criterion=nn.CrossEntropyLoss().cuda()\n",
    "    modelParams=torch.load(path)\n",
    "    model.load_state_dict(modelParams['state_dict'])\n",
    "    min_loss=modelParams['val_loss']\n",
    "    print('min_loss is :%f'%(min_loss))\n",
    "    print('val_correct is %f'%(modelParams['val_correct']))\n",
    "    min_acc=max(modelParams['val_correct'],0.81)\n",
    "    optinizerSave=modelParams['optimizer']\n",
    "    patience=0\n",
    "    lr=1e-4\n",
    "    momentum=0.9\n",
    "    beginepoch=modelParams['epoch']\n",
    "    for epoch in range(beginepoch,epochNum):\n",
    "        print('Epoch {}/{}'.format(epoch, epochNum - 1))\n",
    "        print('-' * 10)\n",
    "        if patience==3:\n",
    "            patience=0\n",
    "            model.load_state_dict(torch.load('../model/DesNet121/'+date+'_loss_best.pth')['state_dict'])\n",
    "            lr=lr/5\n",
    "            print('lr desencd')\n",
    "        if epoch==beginepoch:\n",
    "            optimizer=torch.optim.SGD(params=model.parameters(),lr=lr,momentum=momentum)\n",
    "#             optimizer.load_state_dict(optinizerSave)\n",
    "#             lr=optimizer['lr']\n",
    "#             momentum=optimizer['momentum']\n",
    "            print('begin lr is ',lr)\n",
    "            \n",
    "        else:\n",
    "            optimizer=torch.optim.SGD(params=model.parameters(),lr=lr,momentum=momentum)\n",
    "                   \n",
    "        running_loss = RunningMean()\n",
    "        running_corrects = RunningMean()\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_dataLoader):\n",
    "            model.train(True)\n",
    "            n_batchsize=inputs.size(0)\n",
    "            inputs = Variable(inputs).cuda()\n",
    "            labels = Variable(labels).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            if isinstance(outputs,tuple):\n",
    "                loss=sum((criterion(o,labels)) for o in outputs)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "            running_loss.update(loss.item(),1)\n",
    "            running_corrects.update(torch.sum(preds == labels.data).data,n_batchsize)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx%30==29:\n",
    "                print('[epoch:%d,batch:%d]:acc: %f,loss:%f'%(epoch,batch_idx,running_corrects.value,running_loss.value))\n",
    "                if batch_idx%300==299: \n",
    "                    niter = epoch * len(train_dataset)/BATCH_SIZE + batch_idx\n",
    "                    writer.add_scalar('Train/Acc',running_corrects.value,niter)\n",
    "                    writer.add_scalar('Train/Loss',running_loss.value,niter)\n",
    "                    lx,px=utils.predict(model,val_dataLoader)\n",
    "                    log_loss = criterion(Variable(px), Variable(lx))\n",
    "                    log_loss = log_loss.item()\n",
    "                    _, preds = torch.max(px, dim=1)\n",
    "                    accuracy = torch.mean((preds == lx).float())\n",
    "                    writer.add_scalar('Val/Acc',accuracy,niter)\n",
    "                    writer.add_scalar('Val/Loss',log_loss,niter)\n",
    "                    print('[epoch:%d,batch:%d]: val_loss:%f,val_acc:%f,val_total:%d'%(epoch,batch_idx,log_loss,accuracy,len(val_dataset)))\n",
    "                    if  log_loss < min_loss:\n",
    "                        utils.snapshot('../model/', 'DesNet121', {\n",
    "                               'epoch': epoch + 1,\n",
    "                               'state_dict': model.state_dict(),\n",
    "                               'optimizer': optimizer.state_dict(),\n",
    "                               'val_loss': log_loss,\n",
    "                               'val_correct':accuracy })          \n",
    "\n",
    "                        min_loss=log_loss\n",
    "                        print('save new model loss,now loss is ',min_loss)\n",
    "\n",
    "                    if accuracy>min_acc:\n",
    "                        utils.snapshot('../model/', 'DesNet121', {\n",
    "                               'epoch': epoch + 1,\n",
    "                               'state_dict': model.state_dict(),\n",
    "                               'optimizer': optimizer.state_dict(),\n",
    "                               'val_loss': log_loss,\n",
    "                               'val_correct':accuracy },key='acc') \n",
    "                        min_acc=accuracy\n",
    "                        print('save new model acc,now acc is ',min_acc)\n",
    "        print('[epoch:%d] :acc: %f,loss:%f,lr:%f,patience:%d'%(epoch,running_corrects.value,running_loss.value,lr,patience))         \n",
    "        lx,px=utils.predict(model,val_dataLoader)\n",
    "        log_loss = criterion(Variable(px), Variable(lx))\n",
    "        log_loss = log_loss.item()\n",
    "        _, preds = torch.max(px, dim=1)\n",
    "        accuracy = torch.mean((preds == lx).float())\n",
    "        writer.add_scalar('Val/Acc',accuracy,(epoch+1) * len(train_dataset)/BATCH_SIZE)\n",
    "        writer.add_scalar('Val/Loss',log_loss,(epoch+1) * len(train_dataset)/BATCH_SIZE)\n",
    "        print('[epoch:%d]: val_loss:%f,val_acc:%f,'%(epoch,log_loss,accuracy))\n",
    "        if  log_loss < min_loss:\n",
    "            utils.snapshot('../model/', 'DesNet121', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy })          \n",
    "            patience = 0\n",
    "            min_loss=log_loss\n",
    "            print('save new model loss,now loss is ',min_loss)\n",
    "        else:\n",
    "            patience += 1\n",
    "        if accuracy>min_acc:\n",
    "            utils.snapshot('../model/', 'DesNet121', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy },key='acc') \n",
    "            min_acc=accuracy\n",
    "            print('save new model acc,now acc is ',min_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] loading model... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lujunfeng/anaconda3/envs/Conda_Env_Pytorch/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "min_loss is :0.357364\n",
      "val_correct is 0.864067\n",
      "Epoch 15/59\n",
      "----------\n",
      "begin lr is  0.0001\n",
      "[epoch:15,batch:29]:acc: 0.921875,loss:0.188780\n",
      "[epoch:15,batch:59]:acc: 0.923958,loss:0.178278\n",
      "[epoch:15,batch:89]:acc: 0.929861,loss:0.174021\n",
      "[epoch:15,batch:119]:acc: 0.930208,loss:0.176278\n",
      "[epoch:15,batch:149]:acc: 0.929167,loss:0.178120\n",
      "[epoch:15,batch:179]:acc: 0.928125,loss:0.179004\n",
      "[epoch:15,batch:209]:acc: 0.926637,loss:0.179931\n",
      "[epoch:15,batch:239]:acc: 0.928255,loss:0.176995\n",
      "[epoch:15,batch:269]:acc: 0.929861,loss:0.174198\n",
      "[epoch:15,batch:299]:acc: 0.930417,loss:0.172389\n",
      "[epoch:15,batch:299]: val_loss:0.348654,val_acc:0.867151,val_total:4539\n",
      "save new model loss,now loss is  0.3486535847187042\n",
      "save new model acc,now acc is  tensor(0.8672, device='cuda:0')\n",
      "[epoch:15,batch:329]:acc: 0.930303,loss:0.173446\n",
      "[epoch:15,batch:359]:acc: 0.929427,loss:0.173616\n",
      "[epoch:15,batch:389]:acc: 0.929808,loss:0.172369\n",
      "[epoch:15,batch:419]:acc: 0.929241,loss:0.173667\n",
      "[epoch:15,batch:449]:acc: 0.929722,loss:0.172851\n",
      "[epoch:15,batch:479]:acc: 0.928971,loss:0.173430\n",
      "[epoch:15,batch:509]:acc: 0.928370,loss:0.173765\n",
      "[epoch:15,batch:539]:acc: 0.928819,loss:0.173699\n",
      "[epoch:15,batch:569]:acc: 0.929057,loss:0.173833\n",
      "[epoch:15,batch:599]:acc: 0.930104,loss:0.172596\n",
      "[epoch:15,batch:599]: val_loss:0.350191,val_acc:0.870015,val_total:4539\n",
      "save new model acc,now acc is  tensor(0.8700, device='cuda:0')\n",
      "[epoch:15,batch:629]:acc: 0.930903,loss:0.171559\n",
      "[epoch:15,batch:659]:acc: 0.931392,loss:0.170692\n",
      "[epoch:15,batch:689]:acc: 0.930480,loss:0.172130\n",
      "[epoch:15,batch:719]:acc: 0.930599,loss:0.171582\n",
      "[epoch:15,batch:749]:acc: 0.930833,loss:0.171140\n",
      "[epoch:15,batch:779]:acc: 0.930769,loss:0.171231\n",
      "[epoch:15,batch:809]:acc: 0.930903,loss:0.171108\n",
      "[epoch:15,batch:839]:acc: 0.931064,loss:0.170815\n",
      "[epoch:15,batch:869]:acc: 0.931070,loss:0.170941\n",
      "[epoch:15,batch:899]:acc: 0.930868,loss:0.171445\n",
      "[epoch:15,batch:899]: val_loss:0.347417,val_acc:0.869795,val_total:4539\n",
      "save new model loss,now loss is  0.34741705656051636\n",
      "[epoch:15,batch:929]:acc: 0.930679,loss:0.171671\n",
      "[epoch:15,batch:959]:acc: 0.930501,loss:0.171527\n",
      "[epoch:15,batch:989]:acc: 0.930524,loss:0.171730\n",
      "[epoch:15] :acc: 0.930574,loss:0.171819,lr:0.000100,patience:0\n",
      "[epoch:15]: val_loss:0.346940,val_acc:0.870015,\n",
      "save new model loss,now loss is  0.34693995118141174\n",
      "Epoch 16/59\n",
      "----------\n",
      "[epoch:16,batch:29]:acc: 0.934375,loss:0.160274\n",
      "[epoch:16,batch:59]:acc: 0.931250,loss:0.166955\n",
      "[epoch:16,batch:89]:acc: 0.932292,loss:0.168601\n",
      "[epoch:16,batch:119]:acc: 0.932552,loss:0.167498\n",
      "[epoch:16,batch:149]:acc: 0.929583,loss:0.169730\n",
      "[epoch:16,batch:179]:acc: 0.931597,loss:0.166043\n",
      "[epoch:16,batch:209]:acc: 0.933333,loss:0.163740\n",
      "[epoch:16,batch:239]:acc: 0.934766,loss:0.162610\n",
      "[epoch:16,batch:269]:acc: 0.934722,loss:0.163832\n",
      "[epoch:16,batch:299]:acc: 0.935521,loss:0.163418\n",
      "[epoch:16,batch:299]: val_loss:0.346956,val_acc:0.868694,val_total:4539\n",
      "[epoch:16,batch:329]:acc: 0.935133,loss:0.164079\n",
      "[epoch:16,batch:359]:acc: 0.934896,loss:0.163948\n",
      "[epoch:16,batch:389]:acc: 0.935256,loss:0.163652\n",
      "[epoch:16,batch:419]:acc: 0.934524,loss:0.164749\n",
      "[epoch:16,batch:449]:acc: 0.934444,loss:0.164633\n",
      "[epoch:16,batch:479]:acc: 0.934570,loss:0.164490\n",
      "[epoch:16,batch:509]:acc: 0.935172,loss:0.163190\n",
      "[epoch:16,batch:539]:acc: 0.934954,loss:0.163095\n",
      "[epoch:16,batch:569]:acc: 0.935088,loss:0.162878\n",
      "[epoch:16,batch:599]:acc: 0.934948,loss:0.162758\n",
      "[epoch:16,batch:599]: val_loss:0.347614,val_acc:0.870676,val_total:4539\n",
      "save new model acc,now acc is  tensor(0.8707, device='cuda:0')\n",
      "[epoch:16,batch:629]:acc: 0.934425,loss:0.163208\n",
      "[epoch:16,batch:659]:acc: 0.934044,loss:0.163624\n",
      "[epoch:16,batch:689]:acc: 0.934420,loss:0.163900\n",
      "[epoch:16,batch:719]:acc: 0.934201,loss:0.163945\n",
      "[epoch:16,batch:749]:acc: 0.934417,loss:0.163958\n",
      "[epoch:16,batch:779]:acc: 0.934455,loss:0.163870\n",
      "[epoch:16,batch:809]:acc: 0.934336,loss:0.163307\n",
      "[epoch:16,batch:839]:acc: 0.934189,loss:0.163920\n",
      "[epoch:16,batch:869]:acc: 0.934555,loss:0.163452\n",
      "[epoch:16,batch:899]:acc: 0.934410,loss:0.163530\n",
      "[epoch:16,batch:899]: val_loss:0.344878,val_acc:0.872879,val_total:4539\n",
      "save new model loss,now loss is  0.34487831592559814\n",
      "save new model acc,now acc is  tensor(0.8729, device='cuda:0')\n",
      "[epoch:16,batch:929]:acc: 0.934308,loss:0.164061\n",
      "[epoch:16,batch:959]:acc: 0.934375,loss:0.163913\n",
      "[epoch:16,batch:989]:acc: 0.934280,loss:0.163830\n",
      "[epoch:16] :acc: 0.934294,loss:0.163965,lr:0.000100,patience:0\n",
      "[epoch:16]: val_loss:0.346956,val_acc:0.871558,\n",
      "Epoch 17/59\n",
      "----------\n",
      "[epoch:17,batch:29]:acc: 0.937500,loss:0.164769\n",
      "[epoch:17,batch:59]:acc: 0.936458,loss:0.162727\n",
      "[epoch:17,batch:89]:acc: 0.938889,loss:0.156957\n",
      "[epoch:17,batch:119]:acc: 0.942448,loss:0.151583\n",
      "[epoch:17,batch:149]:acc: 0.942708,loss:0.151922\n",
      "[epoch:17,batch:179]:acc: 0.943576,loss:0.152306\n",
      "[epoch:17,batch:209]:acc: 0.942113,loss:0.153882\n",
      "[epoch:17,batch:239]:acc: 0.940365,loss:0.155535\n",
      "[epoch:17,batch:269]:acc: 0.939120,loss:0.157575\n",
      "[epoch:17,batch:299]:acc: 0.938854,loss:0.159005\n",
      "[epoch:17,batch:299]: val_loss:0.349266,val_acc:0.867372,val_total:4539\n",
      "[epoch:17,batch:329]:acc: 0.937689,loss:0.159666\n",
      "[epoch:17,batch:359]:acc: 0.937674,loss:0.160025\n",
      "[epoch:17,batch:389]:acc: 0.938462,loss:0.158652\n",
      "[epoch:17,batch:419]:acc: 0.939063,loss:0.158250\n",
      "[epoch:17,batch:449]:acc: 0.938611,loss:0.158291\n",
      "[epoch:17,batch:479]:acc: 0.938216,loss:0.158124\n",
      "[epoch:17,batch:509]:acc: 0.937377,loss:0.159121\n",
      "[epoch:17,batch:539]:acc: 0.937269,loss:0.158814\n",
      "[epoch:17,batch:569]:acc: 0.936513,loss:0.160033\n",
      "[epoch:17,batch:599]:acc: 0.936458,loss:0.160359\n",
      "[epoch:17,batch:599]: val_loss:0.348335,val_acc:0.869134,val_total:4539\n",
      "[epoch:17,batch:629]:acc: 0.936458,loss:0.160156\n",
      "[epoch:17,batch:659]:acc: 0.936600,loss:0.159899\n",
      "[epoch:17,batch:689]:acc: 0.936187,loss:0.160666\n",
      "[epoch:17,batch:719]:acc: 0.936068,loss:0.160628\n",
      "[epoch:17,batch:749]:acc: 0.936542,loss:0.160215\n",
      "[epoch:17,batch:779]:acc: 0.936939,loss:0.159853\n",
      "[epoch:17,batch:809]:acc: 0.936960,loss:0.159678\n",
      "[epoch:17,batch:839]:acc: 0.937388,loss:0.158757\n",
      "[epoch:17,batch:869]:acc: 0.937320,loss:0.158610\n",
      "[epoch:17,batch:899]:acc: 0.937569,loss:0.158397\n",
      "[epoch:17,batch:899]: val_loss:0.349272,val_acc:0.869795,val_total:4539\n",
      "[epoch:17,batch:929]:acc: 0.937500,loss:0.158637\n",
      "[epoch:17,batch:959]:acc: 0.937695,loss:0.158276\n",
      "[epoch:17,batch:989]:acc: 0.937279,loss:0.158708\n",
      "[epoch:17] :acc: 0.937321,loss:0.158752,lr:0.000100,patience:1\n",
      "[epoch:17]: val_loss:0.348510,val_acc:0.871337,\n",
      "Epoch 18/59\n",
      "----------\n",
      "[epoch:18,batch:29]:acc: 0.940625,loss:0.155096\n",
      "[epoch:18,batch:59]:acc: 0.944271,loss:0.149222\n",
      "[epoch:18,batch:89]:acc: 0.945139,loss:0.148775\n",
      "[epoch:18,batch:119]:acc: 0.942969,loss:0.149122\n",
      "[epoch:18,batch:149]:acc: 0.940417,loss:0.154516\n",
      "[epoch:18,batch:179]:acc: 0.939757,loss:0.155390\n",
      "[epoch:18,batch:209]:acc: 0.939286,loss:0.156887\n",
      "[epoch:18,batch:239]:acc: 0.939583,loss:0.156526\n",
      "[epoch:18,batch:269]:acc: 0.940278,loss:0.155946\n",
      "[epoch:18,batch:299]:acc: 0.939583,loss:0.156232\n",
      "[epoch:18,batch:299]: val_loss:0.347447,val_acc:0.870236,val_total:4539\n",
      "[epoch:18,batch:329]:acc: 0.940814,loss:0.154239\n",
      "[epoch:18,batch:359]:acc: 0.941146,loss:0.154017\n",
      "[epoch:18,batch:389]:acc: 0.940625,loss:0.155625\n",
      "[epoch:18,batch:419]:acc: 0.941071,loss:0.155412\n",
      "[epoch:18,batch:449]:acc: 0.941389,loss:0.154578\n",
      "[epoch:18,batch:479]:acc: 0.941146,loss:0.154850\n",
      "[epoch:18,batch:509]:acc: 0.940993,loss:0.155100\n",
      "[epoch:18,batch:539]:acc: 0.940451,loss:0.155690\n",
      "[epoch:18,batch:569]:acc: 0.940186,loss:0.155140\n",
      "[epoch:18,batch:599]:acc: 0.940052,loss:0.154791\n",
      "[epoch:18,batch:599]: val_loss:0.349115,val_acc:0.870676,val_total:4539\n",
      "[epoch:18,batch:629]:acc: 0.940030,loss:0.155521\n",
      "[epoch:18,batch:659]:acc: 0.940294,loss:0.155485\n",
      "[epoch:18,batch:689]:acc: 0.939946,loss:0.155700\n",
      "[epoch:18,batch:719]:acc: 0.940321,loss:0.154855\n",
      "[epoch:18,batch:749]:acc: 0.940167,loss:0.155195\n",
      "[epoch:18,batch:779]:acc: 0.940625,loss:0.154773\n",
      "[epoch:18,batch:809]:acc: 0.940895,loss:0.154486\n",
      "[epoch:18,batch:839]:acc: 0.940588,loss:0.154769\n",
      "[epoch:18,batch:869]:acc: 0.940589,loss:0.154307\n",
      "[epoch:18,batch:899]:acc: 0.940486,loss:0.154362\n",
      "[epoch:18,batch:899]: val_loss:0.349508,val_acc:0.873320,val_total:4539\n",
      "save new model acc,now acc is  tensor(0.8733, device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:18,batch:929]:acc: 0.940020,loss:0.154580\n",
      "[epoch:18,batch:959]:acc: 0.939779,loss:0.154669\n",
      "[epoch:18,batch:989]:acc: 0.939615,loss:0.154977\n",
      "[epoch:18] :acc: 0.939622,loss:0.155680,lr:0.000100,patience:2\n",
      "[epoch:18]: val_loss:0.350762,val_acc:0.870015,\n",
      "Epoch 19/59\n",
      "----------\n",
      "lr desencd\n",
      "[epoch:19,batch:29]:acc: 0.937500,loss:0.151659\n",
      "[epoch:19,batch:59]:acc: 0.938542,loss:0.160642\n",
      "[epoch:19,batch:89]:acc: 0.939931,loss:0.156982\n",
      "[epoch:19,batch:119]:acc: 0.938281,loss:0.158123\n",
      "[epoch:19,batch:149]:acc: 0.937500,loss:0.158173\n",
      "[epoch:19,batch:179]:acc: 0.938715,loss:0.155664\n",
      "[epoch:19,batch:209]:acc: 0.939137,loss:0.155704\n",
      "[epoch:19,batch:239]:acc: 0.941536,loss:0.153435\n",
      "[epoch:19,batch:269]:acc: 0.941088,loss:0.154223\n",
      "[epoch:19,batch:299]:acc: 0.940104,loss:0.155253\n",
      "[epoch:19,batch:299]: val_loss:0.347381,val_acc:0.869354,val_total:4539\n",
      "[epoch:19,batch:329]:acc: 0.939110,loss:0.155896\n",
      "[epoch:19,batch:359]:acc: 0.938108,loss:0.157214\n",
      "[epoch:19,batch:389]:acc: 0.938462,loss:0.155516\n",
      "[epoch:19,batch:419]:acc: 0.937872,loss:0.157279\n",
      "[epoch:19,batch:449]:acc: 0.937847,loss:0.158092\n",
      "[epoch:19,batch:479]:acc: 0.938672,loss:0.157014\n",
      "[epoch:19,batch:509]:acc: 0.938848,loss:0.156670\n",
      "[epoch:19,batch:539]:acc: 0.938600,loss:0.156867\n",
      "[epoch:19,batch:569]:acc: 0.938432,loss:0.157679\n",
      "[epoch:19,batch:599]:acc: 0.938698,loss:0.157296\n",
      "[epoch:19,batch:599]: val_loss:0.347420,val_acc:0.869354,val_total:4539\n",
      "[epoch:19,batch:629]:acc: 0.939038,loss:0.156626\n",
      "[epoch:19,batch:659]:acc: 0.938920,loss:0.157108\n",
      "[epoch:19,batch:689]:acc: 0.938406,loss:0.157979\n",
      "[epoch:19,batch:719]:acc: 0.938542,loss:0.157480\n",
      "[epoch:19,batch:749]:acc: 0.938542,loss:0.157935\n",
      "[epoch:19,batch:779]:acc: 0.938221,loss:0.157920\n",
      "[epoch:19,batch:809]:acc: 0.938349,loss:0.157788\n",
      "[epoch:19,batch:839]:acc: 0.938058,loss:0.158322\n",
      "[epoch:19,batch:869]:acc: 0.938039,loss:0.158056\n",
      "[epoch:19,batch:899]:acc: 0.937951,loss:0.158515\n",
      "[epoch:19,batch:899]: val_loss:0.346159,val_acc:0.869575,val_total:4539\n",
      "[epoch:19,batch:929]:acc: 0.938038,loss:0.158633\n",
      "[epoch:19,batch:959]:acc: 0.938086,loss:0.158682\n",
      "[epoch:19,batch:989]:acc: 0.937721,loss:0.158868\n",
      "[epoch:19] :acc: 0.937731,loss:0.158859,lr:0.000020,patience:0\n",
      "[epoch:19]: val_loss:0.348725,val_acc:0.870456,\n",
      "Epoch 20/59\n",
      "----------\n",
      "[epoch:20,batch:29]:acc: 0.938542,loss:0.159585\n",
      "[epoch:20,batch:59]:acc: 0.944792,loss:0.151283\n",
      "[epoch:20,batch:89]:acc: 0.940972,loss:0.154527\n",
      "[epoch:20,batch:119]:acc: 0.939844,loss:0.156967\n",
      "[epoch:20,batch:149]:acc: 0.939792,loss:0.156379\n",
      "[epoch:20,batch:179]:acc: 0.941493,loss:0.153973\n",
      "[epoch:20,batch:209]:acc: 0.940030,loss:0.154156\n",
      "[epoch:20,batch:239]:acc: 0.939974,loss:0.154042\n",
      "[epoch:20,batch:269]:acc: 0.938889,loss:0.155561\n",
      "[epoch:20,batch:299]:acc: 0.938854,loss:0.156457\n",
      "[epoch:20,batch:299]: val_loss:0.346720,val_acc:0.871337,val_total:4539\n",
      "[epoch:20,batch:329]:acc: 0.940057,loss:0.155418\n",
      "[epoch:20,batch:359]:acc: 0.941233,loss:0.154545\n",
      "[epoch:20,batch:389]:acc: 0.941106,loss:0.154451\n",
      "[epoch:20,batch:419]:acc: 0.940848,loss:0.154748\n",
      "[epoch:20,batch:449]:acc: 0.941042,loss:0.153941\n",
      "[epoch:20,batch:479]:acc: 0.940820,loss:0.153685\n",
      "[epoch:20,batch:509]:acc: 0.940564,loss:0.154173\n",
      "[epoch:20,batch:539]:acc: 0.940220,loss:0.154918\n",
      "[epoch:20,batch:569]:acc: 0.939967,loss:0.155102\n",
      "[epoch:20,batch:599]:acc: 0.939375,loss:0.156022\n",
      "[epoch:20,batch:599]: val_loss:0.346149,val_acc:0.870676,val_total:4539\n",
      "[epoch:20,batch:629]:acc: 0.939881,loss:0.155490\n",
      "[epoch:20,batch:659]:acc: 0.939725,loss:0.155835\n",
      "[epoch:20,batch:689]:acc: 0.939266,loss:0.156199\n",
      "[epoch:20,batch:719]:acc: 0.939583,loss:0.155717\n",
      "[epoch:20,batch:749]:acc: 0.939292,loss:0.156038\n",
      "[epoch:20,batch:779]:acc: 0.939063,loss:0.156547\n",
      "[epoch:20,batch:809]:acc: 0.938657,loss:0.156896\n",
      "[epoch:20,batch:839]:acc: 0.938616,loss:0.157415\n",
      "[epoch:20,batch:869]:acc: 0.938218,loss:0.158065\n",
      "[epoch:20,batch:899]:acc: 0.938264,loss:0.157885\n",
      "[epoch:20,batch:899]: val_loss:0.346879,val_acc:0.871117,val_total:4539\n",
      "[epoch:20,batch:929]:acc: 0.937970,loss:0.158176\n",
      "[epoch:20,batch:959]:acc: 0.938118,loss:0.157936\n",
      "[epoch:20,batch:989]:acc: 0.938605,loss:0.157283\n",
      "[epoch:20] :acc: 0.938582,loss:0.157211,lr:0.000020,patience:1\n",
      "[epoch:20]: val_loss:0.348904,val_acc:0.870676,\n",
      "Epoch 21/59\n",
      "----------\n",
      "[epoch:21,batch:29]:acc: 0.950000,loss:0.141469\n",
      "[epoch:21,batch:59]:acc: 0.944271,loss:0.144505\n",
      "[epoch:21,batch:89]:acc: 0.944444,loss:0.144989\n",
      "[epoch:21,batch:119]:acc: 0.939323,loss:0.153541\n",
      "[epoch:21,batch:149]:acc: 0.937917,loss:0.157100\n",
      "[epoch:21,batch:179]:acc: 0.937153,loss:0.159778\n",
      "[epoch:21,batch:209]:acc: 0.938988,loss:0.157464\n",
      "[epoch:21,batch:239]:acc: 0.939453,loss:0.157296\n",
      "[epoch:21,batch:269]:acc: 0.938426,loss:0.157729\n",
      "[epoch:21,batch:299]:acc: 0.939375,loss:0.158083\n",
      "[epoch:21,batch:299]: val_loss:0.346959,val_acc:0.869354,val_total:4539\n",
      "[epoch:21,batch:329]:acc: 0.938731,loss:0.158353\n",
      "[epoch:21,batch:359]:acc: 0.939497,loss:0.156406\n",
      "[epoch:21,batch:389]:acc: 0.939423,loss:0.156451\n",
      "[epoch:21,batch:419]:acc: 0.939583,loss:0.155824\n",
      "[epoch:21,batch:449]:acc: 0.938958,loss:0.155666\n",
      "[epoch:21,batch:479]:acc: 0.939063,loss:0.155388\n",
      "[epoch:21,batch:509]:acc: 0.938971,loss:0.155960\n",
      "[epoch:21,batch:539]:acc: 0.939236,loss:0.155455\n",
      "[epoch:21,batch:569]:acc: 0.939090,loss:0.155786\n",
      "[epoch:21,batch:599]:acc: 0.938802,loss:0.156048\n",
      "[epoch:21,batch:599]: val_loss:0.347772,val_acc:0.870676,val_total:4539\n",
      "[epoch:21,batch:629]:acc: 0.938988,loss:0.155674\n",
      "[epoch:21,batch:659]:acc: 0.939063,loss:0.155973\n",
      "[epoch:21,batch:689]:acc: 0.939402,loss:0.155695\n",
      "[epoch:21,batch:719]:acc: 0.939497,loss:0.155920\n",
      "[epoch:21,batch:749]:acc: 0.939292,loss:0.156278\n",
      "[epoch:21,batch:779]:acc: 0.939263,loss:0.156349\n",
      "[epoch:21,batch:809]:acc: 0.939005,loss:0.156761\n",
      "[epoch:21,batch:839]:acc: 0.938988,loss:0.156939\n",
      "[epoch:21,batch:869]:acc: 0.938937,loss:0.156658\n",
      "[epoch:21,batch:899]:acc: 0.938958,loss:0.156546\n",
      "[epoch:21,batch:899]: val_loss:0.345673,val_acc:0.871558,val_total:4539\n",
      "[epoch:21,batch:929]:acc: 0.938911,loss:0.156665\n",
      "[epoch:21,batch:959]:acc: 0.938542,loss:0.157105\n",
      "[epoch:21,batch:989]:acc: 0.938289,loss:0.157452\n",
      "[epoch:21] :acc: 0.938267,loss:0.157458,lr:0.000020,patience:2\n",
      "[epoch:21]: val_loss:0.349827,val_acc:0.871778,\n",
      "Epoch 22/59\n",
      "----------\n",
      "lr desencd\n",
      "[epoch:22,batch:29]:acc: 0.934375,loss:0.164674\n",
      "[epoch:22,batch:59]:acc: 0.933854,loss:0.169497\n",
      "[epoch:22,batch:89]:acc: 0.934722,loss:0.168391\n",
      "[epoch:22,batch:119]:acc: 0.935417,loss:0.164390\n",
      "[epoch:22,batch:149]:acc: 0.935833,loss:0.163310\n",
      "[epoch:22,batch:179]:acc: 0.937153,loss:0.161393\n",
      "[epoch:22,batch:209]:acc: 0.937202,loss:0.160961\n",
      "[epoch:22,batch:239]:acc: 0.938151,loss:0.159779\n",
      "[epoch:22,batch:269]:acc: 0.937963,loss:0.161117\n",
      "[epoch:22,batch:299]:acc: 0.936875,loss:0.162611\n",
      "[epoch:22,batch:299]: val_loss:0.344919,val_acc:0.870676,val_total:4539\n",
      "[epoch:22,batch:329]:acc: 0.935890,loss:0.162962\n",
      "[epoch:22,batch:359]:acc: 0.936632,loss:0.162043\n",
      "[epoch:22,batch:389]:acc: 0.935817,loss:0.162249\n",
      "[epoch:22,batch:419]:acc: 0.936086,loss:0.161584\n",
      "[epoch:22,batch:449]:acc: 0.935625,loss:0.162481\n",
      "[epoch:22,batch:479]:acc: 0.935352,loss:0.162970\n",
      "[epoch:22,batch:509]:acc: 0.935662,loss:0.162422\n",
      "[epoch:22,batch:539]:acc: 0.935995,loss:0.161851\n",
      "[epoch:22,batch:569]:acc: 0.936404,loss:0.160725\n",
      "[epoch:22,batch:599]:acc: 0.936250,loss:0.160493\n",
      "[epoch:22,batch:599]: val_loss:0.345838,val_acc:0.871337,val_total:4539\n",
      "[epoch:22,batch:629]:acc: 0.935813,loss:0.160935\n",
      "[epoch:22,batch:659]:acc: 0.935795,loss:0.160838\n",
      "[epoch:22,batch:689]:acc: 0.935824,loss:0.160885\n",
      "[epoch:22,batch:719]:acc: 0.936458,loss:0.160055\n",
      "[epoch:22,batch:749]:acc: 0.937042,loss:0.159210\n",
      "[epoch:22,batch:779]:acc: 0.936779,loss:0.159537\n",
      "[epoch:22,batch:809]:acc: 0.937037,loss:0.159034\n",
      "[epoch:22,batch:839]:acc: 0.936942,loss:0.159336\n",
      "[epoch:22,batch:869]:acc: 0.937069,loss:0.159340\n",
      "[epoch:22,batch:899]:acc: 0.937396,loss:0.159078\n",
      "[epoch:22,batch:899]: val_loss:0.347290,val_acc:0.870236,val_total:4539\n",
      "[epoch:22,batch:929]:acc: 0.937534,loss:0.158814\n",
      "[epoch:22,batch:959]:acc: 0.937174,loss:0.159086\n",
      "[epoch:22,batch:989]:acc: 0.937405,loss:0.158993\n",
      "[epoch:22] :acc: 0.937415,loss:0.159405,lr:0.000004,patience:0\n",
      "[epoch:22]: val_loss:0.349031,val_acc:0.871117,\n",
      "Epoch 23/59\n",
      "----------\n",
      "[epoch:23,batch:29]:acc: 0.926042,loss:0.171800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:23,batch:59]:acc: 0.926562,loss:0.172204\n",
      "[epoch:23,batch:89]:acc: 0.930556,loss:0.165305\n",
      "[epoch:23,batch:119]:acc: 0.935937,loss:0.161015\n",
      "[epoch:23,batch:149]:acc: 0.936458,loss:0.159638\n",
      "[epoch:23,batch:179]:acc: 0.936285,loss:0.158988\n",
      "[epoch:23,batch:209]:acc: 0.937202,loss:0.159062\n",
      "[epoch:23,batch:239]:acc: 0.937760,loss:0.157852\n",
      "[epoch:23,batch:269]:acc: 0.937269,loss:0.158313\n",
      "[epoch:23,batch:299]:acc: 0.937187,loss:0.158488\n",
      "[epoch:23,batch:299]: val_loss:0.345140,val_acc:0.870015,val_total:4539\n",
      "[epoch:23,batch:329]:acc: 0.937784,loss:0.156945\n",
      "[epoch:23,batch:359]:acc: 0.936979,loss:0.157560\n",
      "[epoch:23,batch:389]:acc: 0.937019,loss:0.157505\n",
      "[epoch:23,batch:419]:acc: 0.937351,loss:0.156847\n",
      "[epoch:23,batch:449]:acc: 0.937778,loss:0.157508\n",
      "[epoch:23,batch:479]:acc: 0.937370,loss:0.157793\n",
      "[epoch:23,batch:509]:acc: 0.937561,loss:0.157687\n",
      "[epoch:23,batch:539]:acc: 0.937442,loss:0.158394\n",
      "[epoch:23,batch:569]:acc: 0.937390,loss:0.158650\n",
      "[epoch:23,batch:599]:acc: 0.937865,loss:0.158186\n",
      "[epoch:23,batch:599]: val_loss:0.346913,val_acc:0.870015,val_total:4539\n",
      "[epoch:23,batch:629]:acc: 0.938641,loss:0.157564\n",
      "[epoch:23,batch:659]:acc: 0.938305,loss:0.158476\n",
      "[epoch:23,batch:689]:acc: 0.937817,loss:0.158523\n",
      "[epoch:23,batch:719]:acc: 0.938064,loss:0.157790\n",
      "[epoch:23,batch:749]:acc: 0.937833,loss:0.157956\n",
      "[epoch:23,batch:779]:acc: 0.937740,loss:0.158140\n",
      "[epoch:23,batch:809]:acc: 0.938002,loss:0.157813\n",
      "[epoch:23,batch:839]:acc: 0.937835,loss:0.157989\n",
      "[epoch:23,batch:869]:acc: 0.938362,loss:0.157697\n",
      "[epoch:23,batch:899]:acc: 0.938021,loss:0.158424\n",
      "[epoch:23,batch:899]: val_loss:0.346792,val_acc:0.868253,val_total:4539\n",
      "[epoch:23,batch:929]:acc: 0.937970,loss:0.158818\n",
      "[epoch:23,batch:959]:acc: 0.937988,loss:0.159054\n",
      "[epoch:23,batch:989]:acc: 0.937879,loss:0.159567\n",
      "[epoch:23] :acc: 0.937888,loss:0.159439,lr:0.000004,patience:1\n",
      "[epoch:23]: val_loss:0.347197,val_acc:0.871117,\n",
      "Epoch 24/59\n",
      "----------\n",
      "[epoch:24,batch:29]:acc: 0.940625,loss:0.177827\n",
      "[epoch:24,batch:59]:acc: 0.936979,loss:0.170313\n",
      "[epoch:24,batch:89]:acc: 0.935764,loss:0.165759\n",
      "[epoch:24,batch:119]:acc: 0.935417,loss:0.163497\n",
      "[epoch:24,batch:149]:acc: 0.934375,loss:0.165349\n",
      "[epoch:24,batch:179]:acc: 0.935590,loss:0.163228\n",
      "[epoch:24,batch:209]:acc: 0.936012,loss:0.161340\n",
      "[epoch:24,batch:239]:acc: 0.935807,loss:0.162244\n",
      "[epoch:24,batch:269]:acc: 0.935880,loss:0.161426\n",
      "[epoch:24,batch:299]:acc: 0.935312,loss:0.162154\n",
      "[epoch:24,batch:299]: val_loss:0.345422,val_acc:0.870236,val_total:4539\n",
      "[epoch:24,batch:329]:acc: 0.936174,loss:0.160585\n",
      "[epoch:24,batch:359]:acc: 0.935937,loss:0.161438\n",
      "[epoch:24,batch:389]:acc: 0.935417,loss:0.162167\n",
      "[epoch:24,batch:419]:acc: 0.935417,loss:0.162619\n",
      "[epoch:24,batch:449]:acc: 0.935556,loss:0.161836\n",
      "[epoch:24,batch:479]:acc: 0.936003,loss:0.161019\n",
      "[epoch:24,batch:509]:acc: 0.936336,loss:0.160330\n",
      "[epoch:24,batch:539]:acc: 0.936169,loss:0.160881\n",
      "[epoch:24,batch:569]:acc: 0.935800,loss:0.161739\n",
      "[epoch:24,batch:599]:acc: 0.935937,loss:0.161554\n",
      "[epoch:24,batch:599]: val_loss:0.346139,val_acc:0.870236,val_total:4539\n",
      "[epoch:24,batch:629]:acc: 0.936607,loss:0.160768\n",
      "[epoch:24,batch:659]:acc: 0.936316,loss:0.160973\n",
      "[epoch:24,batch:689]:acc: 0.936413,loss:0.160547\n",
      "[epoch:24,batch:719]:acc: 0.936675,loss:0.160216\n",
      "[epoch:24,batch:749]:acc: 0.936708,loss:0.160193\n",
      "[epoch:24,batch:779]:acc: 0.936819,loss:0.160559\n",
      "[epoch:24,batch:809]:acc: 0.937423,loss:0.159639\n",
      "[epoch:24,batch:839]:acc: 0.937388,loss:0.159669\n",
      "[epoch:24,batch:869]:acc: 0.937572,loss:0.159297\n",
      "[epoch:24,batch:899]:acc: 0.937257,loss:0.159236\n",
      "[epoch:24,batch:899]: val_loss:0.345341,val_acc:0.869795,val_total:4539\n",
      "[epoch:24,batch:929]:acc: 0.937198,loss:0.158777\n",
      "[epoch:24,batch:959]:acc: 0.937565,loss:0.158297\n",
      "[epoch:24,batch:989]:acc: 0.937879,loss:0.157750\n",
      "[epoch:24] :acc: 0.937825,loss:0.157911,lr:0.000004,patience:2\n",
      "[epoch:24]: val_loss:0.349072,val_acc:0.872219,\n",
      "Epoch 25/59\n",
      "----------\n",
      "lr desencd\n",
      "[epoch:25,batch:29]:acc: 0.926042,loss:0.168723\n",
      "[epoch:25,batch:59]:acc: 0.925521,loss:0.176069\n",
      "[epoch:25,batch:89]:acc: 0.933333,loss:0.170115\n",
      "[epoch:25,batch:119]:acc: 0.936719,loss:0.163933\n",
      "[epoch:25,batch:149]:acc: 0.936667,loss:0.162586\n",
      "[epoch:25,batch:179]:acc: 0.935764,loss:0.165958\n",
      "[epoch:25,batch:209]:acc: 0.934226,loss:0.165456\n",
      "[epoch:25,batch:239]:acc: 0.935286,loss:0.162542\n",
      "[epoch:25,batch:269]:acc: 0.934722,loss:0.162680\n",
      "[epoch:25,batch:299]:acc: 0.935521,loss:0.161988\n",
      "[epoch:25,batch:299]: val_loss:0.346951,val_acc:0.869134,val_total:4539\n",
      "[epoch:25,batch:329]:acc: 0.936648,loss:0.160742\n",
      "[epoch:25,batch:359]:acc: 0.936111,loss:0.161325\n",
      "[epoch:25,batch:389]:acc: 0.935256,loss:0.162453\n",
      "[epoch:25,batch:419]:acc: 0.935268,loss:0.162274\n",
      "[epoch:25,batch:449]:acc: 0.935972,loss:0.160862\n",
      "[epoch:25,batch:479]:acc: 0.937174,loss:0.159042\n",
      "[epoch:25,batch:509]:acc: 0.936949,loss:0.158785\n",
      "[epoch:25,batch:539]:acc: 0.937269,loss:0.158457\n",
      "[epoch:25,batch:569]:acc: 0.938048,loss:0.157467\n",
      "[epoch:25,batch:599]:acc: 0.937969,loss:0.157827\n",
      "[epoch:25,batch:599]: val_loss:0.347738,val_acc:0.871778,val_total:4539\n",
      "[epoch:25,batch:629]:acc: 0.937252,loss:0.158724\n",
      "[epoch:25,batch:659]:acc: 0.937311,loss:0.158999\n",
      "[epoch:25,batch:689]:acc: 0.937319,loss:0.158749\n",
      "[epoch:25,batch:719]:acc: 0.937283,loss:0.158961\n",
      "[epoch:25,batch:749]:acc: 0.937500,loss:0.158709\n",
      "[epoch:25,batch:779]:acc: 0.937260,loss:0.159081\n",
      "[epoch:25,batch:809]:acc: 0.937191,loss:0.159498\n",
      "[epoch:25,batch:839]:acc: 0.936830,loss:0.159656\n",
      "[epoch:25,batch:869]:acc: 0.936853,loss:0.159628\n",
      "[epoch:25,batch:899]:acc: 0.936979,loss:0.159201\n",
      "[epoch:25,batch:899]: val_loss:0.347393,val_acc:0.870015,val_total:4539\n",
      "[epoch:25,batch:929]:acc: 0.936996,loss:0.159175\n",
      "[epoch:25,batch:959]:acc: 0.937109,loss:0.159210\n",
      "[epoch:25,batch:989]:acc: 0.937153,loss:0.159062\n",
      "[epoch:25] :acc: 0.937132,loss:0.159397,lr:0.000001,patience:0\n",
      "[epoch:25]: val_loss:0.348089,val_acc:0.870897,\n",
      "Epoch 26/59\n",
      "----------\n",
      "[epoch:26,batch:29]:acc: 0.938542,loss:0.160064\n",
      "[epoch:26,batch:59]:acc: 0.940104,loss:0.161238\n",
      "[epoch:26,batch:89]:acc: 0.940625,loss:0.163480\n",
      "[epoch:26,batch:119]:acc: 0.938281,loss:0.162376\n",
      "[epoch:26,batch:149]:acc: 0.938750,loss:0.162220\n",
      "[epoch:26,batch:179]:acc: 0.937500,loss:0.162827\n",
      "[epoch:26,batch:209]:acc: 0.937054,loss:0.163827\n",
      "[epoch:26,batch:239]:acc: 0.938542,loss:0.162803\n",
      "[epoch:26,batch:269]:acc: 0.937500,loss:0.163154\n",
      "[epoch:26,batch:299]:acc: 0.937083,loss:0.162946\n",
      "[epoch:26,batch:299]: val_loss:0.346011,val_acc:0.869575,val_total:4539\n",
      "[epoch:26,batch:329]:acc: 0.937121,loss:0.162957\n",
      "[epoch:26,batch:359]:acc: 0.937760,loss:0.162107\n",
      "[epoch:26,batch:389]:acc: 0.938381,loss:0.160815\n",
      "[epoch:26,batch:419]:acc: 0.938988,loss:0.160691\n",
      "[epoch:26,batch:449]:acc: 0.939444,loss:0.159241\n",
      "[epoch:26,batch:479]:acc: 0.939583,loss:0.159050\n",
      "[epoch:26,batch:509]:acc: 0.938787,loss:0.159719\n",
      "[epoch:26,batch:539]:acc: 0.939294,loss:0.159106\n",
      "[epoch:26,batch:569]:acc: 0.939035,loss:0.159033\n",
      "[epoch:26,batch:599]:acc: 0.938854,loss:0.158620\n",
      "[epoch:26,batch:599]: val_loss:0.346227,val_acc:0.870897,val_total:4539\n",
      "[epoch:26,batch:629]:acc: 0.938492,loss:0.158706\n",
      "[epoch:26,batch:659]:acc: 0.937737,loss:0.159223\n",
      "[epoch:26,batch:689]:acc: 0.938315,loss:0.158526\n",
      "[epoch:26,batch:719]:acc: 0.938628,loss:0.157970\n",
      "[epoch:26,batch:749]:acc: 0.938250,loss:0.157864\n",
      "[epoch:26,batch:779]:acc: 0.937981,loss:0.158090\n",
      "[epoch:26,batch:809]:acc: 0.937963,loss:0.158710\n",
      "[epoch:26,batch:839]:acc: 0.937500,loss:0.159238\n",
      "[epoch:26,batch:869]:acc: 0.937751,loss:0.159034\n",
      "[epoch:26,batch:899]:acc: 0.937535,loss:0.159108\n",
      "[epoch:26,batch:899]: val_loss:0.346194,val_acc:0.871778,val_total:4539\n",
      "[epoch:26,batch:929]:acc: 0.937164,loss:0.159199\n",
      "[epoch:26,batch:959]:acc: 0.937598,loss:0.158711\n",
      "[epoch:26,batch:989]:acc: 0.937658,loss:0.158729\n",
      "[epoch:26] :acc: 0.937731,loss:0.158687,lr:0.000001,patience:1\n",
      "[epoch:26]: val_loss:0.348185,val_acc:0.869575,\n",
      "Epoch 27/59\n",
      "----------\n",
      "[epoch:27,batch:29]:acc: 0.930208,loss:0.176853\n",
      "[epoch:27,batch:59]:acc: 0.940104,loss:0.160993\n",
      "[epoch:27,batch:89]:acc: 0.938889,loss:0.165889\n",
      "[epoch:27,batch:119]:acc: 0.938802,loss:0.162980\n",
      "[epoch:27,batch:149]:acc: 0.940208,loss:0.160818\n",
      "[epoch:27,batch:179]:acc: 0.938715,loss:0.158489\n",
      "[epoch:27,batch:209]:acc: 0.936756,loss:0.159993\n",
      "[epoch:27,batch:239]:acc: 0.936458,loss:0.160750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:27,batch:269]:acc: 0.936921,loss:0.159929\n",
      "[epoch:27,batch:299]:acc: 0.936979,loss:0.159796\n",
      "[epoch:27,batch:299]: val_loss:0.346204,val_acc:0.870015,val_total:4539\n",
      "[epoch:27,batch:329]:acc: 0.936553,loss:0.161581\n",
      "[epoch:27,batch:359]:acc: 0.936285,loss:0.160448\n",
      "[epoch:27,batch:389]:acc: 0.935897,loss:0.160609\n",
      "[epoch:27,batch:419]:acc: 0.936235,loss:0.160322\n",
      "[epoch:27,batch:449]:acc: 0.936319,loss:0.160710\n",
      "[epoch:27,batch:479]:acc: 0.936784,loss:0.159600\n",
      "[epoch:27,batch:509]:acc: 0.936029,loss:0.161208\n",
      "[epoch:27,batch:539]:acc: 0.936227,loss:0.160773\n",
      "[epoch:27,batch:569]:acc: 0.937061,loss:0.160089\n",
      "[epoch:27,batch:599]:acc: 0.936667,loss:0.160275\n",
      "[epoch:27,batch:599]: val_loss:0.345521,val_acc:0.871558,val_total:4539\n",
      "[epoch:27,batch:629]:acc: 0.937004,loss:0.159420\n",
      "[epoch:27,batch:659]:acc: 0.937074,loss:0.158914\n",
      "[epoch:27,batch:689]:acc: 0.936911,loss:0.158928\n",
      "[epoch:27,batch:719]:acc: 0.936675,loss:0.159022\n",
      "[epoch:27,batch:749]:acc: 0.936542,loss:0.158716\n",
      "[epoch:27,batch:779]:acc: 0.936699,loss:0.158250\n",
      "[epoch:27,batch:809]:acc: 0.935957,loss:0.158815\n",
      "[epoch:27,batch:839]:acc: 0.935975,loss:0.159188\n",
      "[epoch:27,batch:869]:acc: 0.936063,loss:0.159128\n",
      "[epoch:27,batch:899]:acc: 0.936354,loss:0.158687\n",
      "[epoch:27,batch:899]: val_loss:0.346253,val_acc:0.870897,val_total:4539\n",
      "[epoch:27,batch:929]:acc: 0.936626,loss:0.158212\n",
      "[epoch:27,batch:959]:acc: 0.936686,loss:0.158287\n",
      "[epoch:27,batch:989]:acc: 0.936995,loss:0.158335\n",
      "[epoch:27] :acc: 0.936911,loss:0.158531,lr:0.000001,patience:2\n",
      "[epoch:27]: val_loss:0.349586,val_acc:0.870236,\n",
      "Epoch 28/59\n",
      "----------\n",
      "lr desencd\n",
      "[epoch:28,batch:29]:acc: 0.948958,loss:0.148359\n",
      "[epoch:28,batch:59]:acc: 0.943750,loss:0.153827\n",
      "[epoch:28,batch:89]:acc: 0.941667,loss:0.155372\n",
      "[epoch:28,batch:119]:acc: 0.941927,loss:0.153248\n",
      "[epoch:28,batch:149]:acc: 0.939792,loss:0.157299\n",
      "[epoch:28,batch:179]:acc: 0.937500,loss:0.160573\n",
      "[epoch:28,batch:209]:acc: 0.938839,loss:0.159570\n",
      "[epoch:28,batch:239]:acc: 0.939323,loss:0.159003\n",
      "[epoch:28,batch:269]:acc: 0.939815,loss:0.158143\n",
      "[epoch:28,batch:299]:acc: 0.939271,loss:0.157900\n",
      "[epoch:28,batch:299]: val_loss:0.347243,val_acc:0.870015,val_total:4539\n",
      "[epoch:28,batch:329]:acc: 0.938826,loss:0.157701\n",
      "[epoch:28,batch:359]:acc: 0.937934,loss:0.158533\n",
      "[epoch:28,batch:389]:acc: 0.938061,loss:0.158620\n",
      "[epoch:28,batch:419]:acc: 0.937798,loss:0.159029\n",
      "[epoch:28,batch:449]:acc: 0.937778,loss:0.159324\n",
      "[epoch:28,batch:479]:acc: 0.938477,loss:0.158314\n",
      "[epoch:28,batch:509]:acc: 0.938051,loss:0.158543\n",
      "[epoch:28,batch:539]:acc: 0.937558,loss:0.159375\n",
      "[epoch:28,batch:569]:acc: 0.937719,loss:0.159835\n",
      "[epoch:28,batch:599]:acc: 0.937708,loss:0.159618\n",
      "[epoch:28,batch:599]: val_loss:0.347657,val_acc:0.870015,val_total:4539\n",
      "[epoch:28,batch:629]:acc: 0.937401,loss:0.159677\n",
      "[epoch:28,batch:659]:acc: 0.937263,loss:0.159540\n",
      "[epoch:28,batch:689]:acc: 0.937772,loss:0.159127\n",
      "[epoch:28,batch:719]:acc: 0.937457,loss:0.159836\n",
      "[epoch:28,batch:749]:acc: 0.937375,loss:0.159954\n",
      "[epoch:28,batch:779]:acc: 0.937059,loss:0.160598\n",
      "[epoch:28,batch:809]:acc: 0.936728,loss:0.161573\n",
      "[epoch:28,batch:839]:acc: 0.936756,loss:0.161204\n",
      "[epoch:28,batch:869]:acc: 0.936782,loss:0.161141\n",
      "[epoch:28,batch:899]:acc: 0.936979,loss:0.160992\n",
      "[epoch:28,batch:899]: val_loss:0.345873,val_acc:0.870897,val_total:4539\n",
      "[epoch:28,batch:929]:acc: 0.937399,loss:0.160409\n",
      "[epoch:28,batch:959]:acc: 0.937435,loss:0.160602\n",
      "[epoch:28,batch:989]:acc: 0.937626,loss:0.160023\n",
      "[epoch:28] :acc: 0.937573,loss:0.160018,lr:0.000000,patience:0\n",
      "[epoch:28]: val_loss:0.346547,val_acc:0.870456,\n",
      "Epoch 29/59\n",
      "----------\n",
      "[epoch:29,batch:29]:acc: 0.937500,loss:0.162747\n",
      "[epoch:29,batch:59]:acc: 0.941146,loss:0.156684\n",
      "[epoch:29,batch:89]:acc: 0.938889,loss:0.159908\n",
      "[epoch:29,batch:119]:acc: 0.939844,loss:0.157689\n",
      "[epoch:29,batch:149]:acc: 0.941667,loss:0.155205\n",
      "[epoch:29,batch:179]:acc: 0.939410,loss:0.156883\n",
      "[epoch:29,batch:209]:acc: 0.938839,loss:0.158088\n",
      "[epoch:29,batch:239]:acc: 0.938411,loss:0.157905\n",
      "[epoch:29,batch:269]:acc: 0.939005,loss:0.156644\n",
      "[epoch:29,batch:299]:acc: 0.938958,loss:0.156702\n",
      "[epoch:29,batch:299]: val_loss:0.346805,val_acc:0.869575,val_total:4539\n",
      "[epoch:29,batch:329]:acc: 0.939394,loss:0.155060\n",
      "[epoch:29,batch:359]:acc: 0.938715,loss:0.155754\n",
      "[epoch:29,batch:389]:acc: 0.938301,loss:0.156308\n",
      "[epoch:29,batch:419]:acc: 0.936533,loss:0.158652\n",
      "[epoch:29,batch:449]:acc: 0.936875,loss:0.158177\n",
      "[epoch:29,batch:479]:acc: 0.936719,loss:0.158736\n",
      "[epoch:29,batch:509]:acc: 0.937194,loss:0.158057\n",
      "[epoch:29,batch:539]:acc: 0.937326,loss:0.157852\n",
      "[epoch:29,batch:569]:acc: 0.937336,loss:0.158279\n",
      "[epoch:29,batch:599]:acc: 0.936562,loss:0.159296\n",
      "[epoch:29,batch:599]: val_loss:0.346308,val_acc:0.871778,val_total:4539\n",
      "[epoch:29,batch:629]:acc: 0.936508,loss:0.159276\n",
      "[epoch:29,batch:659]:acc: 0.936458,loss:0.158578\n",
      "[epoch:29,batch:689]:acc: 0.936232,loss:0.158993\n",
      "[epoch:29,batch:719]:acc: 0.936502,loss:0.159350\n",
      "[epoch:29,batch:749]:acc: 0.936625,loss:0.159338\n",
      "[epoch:29,batch:779]:acc: 0.936779,loss:0.158783\n",
      "[epoch:29,batch:809]:acc: 0.936767,loss:0.159116\n",
      "[epoch:29,batch:839]:acc: 0.936496,loss:0.159172\n",
      "[epoch:29,batch:869]:acc: 0.936422,loss:0.159446\n",
      "[epoch:29,batch:899]:acc: 0.936562,loss:0.159624\n",
      "[epoch:29,batch:899]: val_loss:0.345416,val_acc:0.871558,val_total:4539\n",
      "[epoch:29,batch:929]:acc: 0.936761,loss:0.159401\n",
      "[epoch:29,batch:959]:acc: 0.936947,loss:0.159428\n",
      "[epoch:29,batch:989]:acc: 0.937027,loss:0.159276\n",
      "[epoch:29] :acc: 0.937037,loss:0.159522,lr:0.000000,patience:1\n",
      "[epoch:29]: val_loss:0.347657,val_acc:0.868914,\n",
      "Epoch 30/59\n",
      "----------\n",
      "[epoch:30,batch:29]:acc: 0.929167,loss:0.162814\n",
      "[epoch:30,batch:59]:acc: 0.937500,loss:0.155860\n",
      "[epoch:30,batch:89]:acc: 0.937847,loss:0.154214\n",
      "[epoch:30,batch:119]:acc: 0.934375,loss:0.158187\n",
      "[epoch:30,batch:149]:acc: 0.934583,loss:0.157869\n",
      "[epoch:30,batch:179]:acc: 0.934549,loss:0.158378\n",
      "[epoch:30,batch:209]:acc: 0.932292,loss:0.159625\n",
      "[epoch:30,batch:239]:acc: 0.932682,loss:0.161231\n",
      "[epoch:30,batch:269]:acc: 0.932870,loss:0.161872\n",
      "[epoch:30,batch:299]:acc: 0.932708,loss:0.162690\n",
      "[epoch:30,batch:299]: val_loss:0.345833,val_acc:0.870015,val_total:4539\n",
      "[epoch:30,batch:329]:acc: 0.934375,loss:0.161379\n",
      "[epoch:30,batch:359]:acc: 0.934809,loss:0.161782\n",
      "[epoch:30,batch:389]:acc: 0.933894,loss:0.161876\n",
      "[epoch:30,batch:419]:acc: 0.933408,loss:0.162274\n",
      "[epoch:30,batch:449]:acc: 0.933958,loss:0.161875\n",
      "[epoch:30,batch:479]:acc: 0.934570,loss:0.161053\n",
      "[epoch:30,batch:509]:acc: 0.933701,loss:0.162343\n",
      "[epoch:30,batch:539]:acc: 0.933623,loss:0.162159\n",
      "[epoch:30,batch:569]:acc: 0.933936,loss:0.161768\n",
      "[epoch:30,batch:599]:acc: 0.934167,loss:0.161497\n",
      "[epoch:30,batch:599]: val_loss:0.346090,val_acc:0.871337,val_total:4539\n",
      "[epoch:30,batch:629]:acc: 0.934375,loss:0.160947\n",
      "[epoch:30,batch:659]:acc: 0.934943,loss:0.159988\n",
      "[epoch:30,batch:689]:acc: 0.935417,loss:0.159721\n",
      "[epoch:30,batch:719]:acc: 0.935981,loss:0.159536\n",
      "[epoch:30,batch:749]:acc: 0.936333,loss:0.159309\n",
      "[epoch:30,batch:779]:acc: 0.937019,loss:0.158800\n",
      "[epoch:30,batch:809]:acc: 0.936883,loss:0.159091\n",
      "[epoch:30,batch:839]:acc: 0.937165,loss:0.159055\n",
      "[epoch:30,batch:869]:acc: 0.937572,loss:0.158594\n",
      "[epoch:30,batch:899]:acc: 0.938264,loss:0.157815\n",
      "[epoch:30,batch:899]: val_loss:0.346191,val_acc:0.870015,val_total:4539\n",
      "[epoch:30,batch:929]:acc: 0.937870,loss:0.158255\n",
      "[epoch:30,batch:959]:acc: 0.937695,loss:0.158458\n",
      "[epoch:30,batch:989]:acc: 0.937721,loss:0.158625\n",
      "[epoch:30] :acc: 0.937731,loss:0.158470,lr:0.000000,patience:2\n",
      "[epoch:30]: val_loss:0.348255,val_acc:0.870897,\n",
      "Epoch 31/59\n",
      "----------\n",
      "lr desencd\n",
      "[epoch:31,batch:29]:acc: 0.945833,loss:0.151197\n",
      "[epoch:31,batch:59]:acc: 0.940625,loss:0.154531\n",
      "[epoch:31,batch:89]:acc: 0.939931,loss:0.156025\n",
      "[epoch:31,batch:119]:acc: 0.937500,loss:0.159720\n",
      "[epoch:31,batch:149]:acc: 0.938333,loss:0.160808\n",
      "[epoch:31,batch:179]:acc: 0.936458,loss:0.159567\n",
      "[epoch:31,batch:209]:acc: 0.937946,loss:0.156651\n",
      "[epoch:31,batch:239]:acc: 0.937630,loss:0.157723\n",
      "[epoch:31,batch:269]:acc: 0.937616,loss:0.158263\n",
      "[epoch:31,batch:299]:acc: 0.935833,loss:0.160403\n",
      "[epoch:31,batch:299]: val_loss:0.346278,val_acc:0.870236,val_total:4539\n",
      "[epoch:31,batch:329]:acc: 0.936269,loss:0.159355\n",
      "[epoch:31,batch:359]:acc: 0.936632,loss:0.159620\n",
      "[epoch:31,batch:389]:acc: 0.936939,loss:0.159422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:31,batch:419]:acc: 0.936310,loss:0.159756\n",
      "[epoch:31,batch:449]:acc: 0.936111,loss:0.159617\n",
      "[epoch:31,batch:479]:acc: 0.936133,loss:0.159533\n",
      "[epoch:31,batch:509]:acc: 0.936029,loss:0.159436\n",
      "[epoch:31,batch:539]:acc: 0.936227,loss:0.159332\n",
      "[epoch:31,batch:569]:acc: 0.936623,loss:0.158796\n",
      "[epoch:31,batch:599]:acc: 0.936719,loss:0.158891\n",
      "[epoch:31,batch:599]: val_loss:0.345388,val_acc:0.869795,val_total:4539\n",
      "[epoch:31,batch:629]:acc: 0.937004,loss:0.158687\n",
      "[epoch:31,batch:659]:acc: 0.937169,loss:0.158134\n",
      "[epoch:31,batch:689]:acc: 0.937092,loss:0.157861\n",
      "[epoch:31,batch:719]:acc: 0.937066,loss:0.158191\n",
      "[epoch:31,batch:749]:acc: 0.936833,loss:0.159278\n",
      "[epoch:31,batch:779]:acc: 0.936258,loss:0.159696\n",
      "[epoch:31,batch:809]:acc: 0.936343,loss:0.159700\n",
      "[epoch:31,batch:839]:acc: 0.936310,loss:0.159862\n",
      "[epoch:31,batch:869]:acc: 0.936566,loss:0.159494\n",
      "[epoch:31,batch:899]:acc: 0.936562,loss:0.159571\n",
      "[epoch:31,batch:899]: val_loss:0.346752,val_acc:0.870456,val_total:4539\n",
      "[epoch:31,batch:929]:acc: 0.936727,loss:0.159272\n",
      "[epoch:31,batch:959]:acc: 0.936882,loss:0.158725\n",
      "[epoch:31,batch:989]:acc: 0.936963,loss:0.159080\n",
      "[epoch:31] :acc: 0.936942,loss:0.159257,lr:0.000000,patience:0\n",
      "[epoch:31]: val_loss:0.350598,val_acc:0.869354,\n",
      "Epoch 32/59\n",
      "----------\n",
      "[epoch:32,batch:29]:acc: 0.940625,loss:0.145811\n",
      "[epoch:32,batch:59]:acc: 0.941667,loss:0.147762\n",
      "[epoch:32,batch:89]:acc: 0.940278,loss:0.153674\n",
      "[epoch:32,batch:119]:acc: 0.940625,loss:0.155717\n",
      "[epoch:32,batch:149]:acc: 0.941042,loss:0.157987\n",
      "[epoch:32,batch:179]:acc: 0.940451,loss:0.158676\n",
      "[epoch:32,batch:209]:acc: 0.938988,loss:0.161161\n",
      "[epoch:32,batch:239]:acc: 0.939063,loss:0.161113\n",
      "[epoch:32,batch:269]:acc: 0.938889,loss:0.160588\n",
      "[epoch:32,batch:299]:acc: 0.938229,loss:0.161414\n",
      "[epoch:32,batch:299]: val_loss:0.345438,val_acc:0.870015,val_total:4539\n",
      "[epoch:32,batch:329]:acc: 0.937973,loss:0.161795\n",
      "[epoch:32,batch:359]:acc: 0.937066,loss:0.162335\n",
      "[epoch:32,batch:389]:acc: 0.936538,loss:0.162671\n",
      "[epoch:32,batch:419]:acc: 0.935863,loss:0.163000\n",
      "[epoch:32,batch:449]:acc: 0.936319,loss:0.162318\n",
      "[epoch:32,batch:479]:acc: 0.936328,loss:0.162151\n",
      "[epoch:32,batch:509]:acc: 0.935846,loss:0.162761\n",
      "[epoch:32,batch:539]:acc: 0.935995,loss:0.163508\n",
      "[epoch:32,batch:569]:acc: 0.936349,loss:0.162656\n",
      "[epoch:32,batch:599]:acc: 0.936875,loss:0.162312\n",
      "[epoch:32,batch:599]: val_loss:0.346219,val_acc:0.870456,val_total:4539\n",
      "[epoch:32,batch:629]:acc: 0.937054,loss:0.161836\n",
      "[epoch:32,batch:659]:acc: 0.937169,loss:0.161486\n",
      "[epoch:32,batch:689]:acc: 0.937183,loss:0.161248\n",
      "[epoch:32,batch:719]:acc: 0.936849,loss:0.161426\n",
      "[epoch:32,batch:749]:acc: 0.936250,loss:0.161838\n",
      "[epoch:32,batch:779]:acc: 0.936378,loss:0.161593\n",
      "[epoch:32,batch:809]:acc: 0.936728,loss:0.160899\n",
      "[epoch:32,batch:839]:acc: 0.936682,loss:0.160760\n",
      "[epoch:32,batch:869]:acc: 0.937033,loss:0.160419\n",
      "[epoch:32,batch:899]:acc: 0.937361,loss:0.159815\n",
      "[epoch:32,batch:899]: val_loss:0.348424,val_acc:0.871778,val_total:4539\n",
      "[epoch:32,batch:929]:acc: 0.937970,loss:0.159120\n",
      "[epoch:32,batch:959]:acc: 0.938216,loss:0.158293\n",
      "[epoch:32,batch:989]:acc: 0.938037,loss:0.158620\n",
      "[epoch:32] :acc: 0.937951,loss:0.160317,lr:0.000000,patience:1\n",
      "[epoch:32]: val_loss:0.348524,val_acc:0.868033,\n",
      "Epoch 33/59\n",
      "----------\n",
      "[epoch:33,batch:29]:acc: 0.934375,loss:0.170219\n",
      "[epoch:33,batch:59]:acc: 0.936979,loss:0.162937\n",
      "[epoch:33,batch:89]:acc: 0.931944,loss:0.167384\n",
      "[epoch:33,batch:119]:acc: 0.934635,loss:0.162377\n",
      "[epoch:33,batch:149]:acc: 0.935208,loss:0.160432\n",
      "[epoch:33,batch:179]:acc: 0.936806,loss:0.159179\n",
      "[epoch:33,batch:209]:acc: 0.938095,loss:0.156955\n",
      "[epoch:33,batch:239]:acc: 0.938021,loss:0.156910\n",
      "[epoch:33,batch:269]:acc: 0.938426,loss:0.156122\n",
      "[epoch:33,batch:299]:acc: 0.938333,loss:0.155915\n",
      "[epoch:33,batch:299]: val_loss:0.348115,val_acc:0.871998,val_total:4539\n",
      "[epoch:33,batch:329]:acc: 0.937595,loss:0.156673\n",
      "[epoch:33,batch:359]:acc: 0.936632,loss:0.157831\n",
      "[epoch:33,batch:389]:acc: 0.936699,loss:0.158536\n",
      "[epoch:33,batch:419]:acc: 0.936235,loss:0.159080\n",
      "[epoch:33,batch:449]:acc: 0.936528,loss:0.158560\n",
      "[epoch:33,batch:479]:acc: 0.936263,loss:0.158768\n",
      "[epoch:33,batch:509]:acc: 0.937071,loss:0.158149\n",
      "[epoch:33,batch:539]:acc: 0.935937,loss:0.159556\n",
      "[epoch:33,batch:569]:acc: 0.935691,loss:0.159613\n",
      "[epoch:33,batch:599]:acc: 0.936667,loss:0.158623\n",
      "[epoch:33,batch:599]: val_loss:0.345589,val_acc:0.871337,val_total:4539\n",
      "[epoch:33,batch:629]:acc: 0.937302,loss:0.157823\n",
      "[epoch:33,batch:659]:acc: 0.937311,loss:0.157857\n",
      "[epoch:33,batch:689]:acc: 0.937862,loss:0.157592\n",
      "[epoch:33,batch:719]:acc: 0.937717,loss:0.158553\n",
      "[epoch:33,batch:749]:acc: 0.938000,loss:0.158775\n",
      "[epoch:33,batch:779]:acc: 0.937861,loss:0.158687\n",
      "[epoch:33,batch:809]:acc: 0.937461,loss:0.159005\n",
      "[epoch:33,batch:839]:acc: 0.937016,loss:0.159315\n",
      "[epoch:33,batch:869]:acc: 0.937428,loss:0.158728\n",
      "[epoch:33,batch:899]:acc: 0.937569,loss:0.158439\n",
      "[epoch:33,batch:899]: val_loss:0.346489,val_acc:0.871558,val_total:4539\n",
      "[epoch:33,batch:929]:acc: 0.937668,loss:0.158479\n",
      "[epoch:33,batch:959]:acc: 0.937728,loss:0.158094\n",
      "[epoch:33,batch:989]:acc: 0.938068,loss:0.157454\n",
      "[epoch:33] :acc: 0.938046,loss:0.157370,lr:0.000000,patience:2\n",
      "[epoch:33]: val_loss:0.347234,val_acc:0.870676,\n",
      "Epoch 34/59\n",
      "----------\n",
      "lr desencd\n",
      "[epoch:34,batch:29]:acc: 0.933333,loss:0.162300\n",
      "[epoch:34,batch:59]:acc: 0.935937,loss:0.158246\n",
      "[epoch:34,batch:89]:acc: 0.937500,loss:0.154901\n",
      "[epoch:34,batch:119]:acc: 0.934115,loss:0.159371\n",
      "[epoch:34,batch:149]:acc: 0.933958,loss:0.160296\n",
      "[epoch:34,batch:179]:acc: 0.936632,loss:0.156460\n",
      "[epoch:34,batch:209]:acc: 0.936161,loss:0.156939\n",
      "[epoch:34,batch:239]:acc: 0.937240,loss:0.156340\n",
      "[epoch:34,batch:269]:acc: 0.937269,loss:0.155705\n",
      "[epoch:34,batch:299]:acc: 0.936979,loss:0.157481\n",
      "[epoch:34,batch:299]: val_loss:0.345041,val_acc:0.869354,val_total:4539\n",
      "[epoch:34,batch:329]:acc: 0.937311,loss:0.157031\n",
      "[epoch:34,batch:359]:acc: 0.936198,loss:0.157895\n",
      "[epoch:34,batch:389]:acc: 0.935897,loss:0.158880\n",
      "[epoch:34,batch:419]:acc: 0.935937,loss:0.158891\n",
      "[epoch:34,batch:449]:acc: 0.936181,loss:0.159188\n",
      "[epoch:34,batch:479]:acc: 0.936393,loss:0.158620\n",
      "[epoch:34,batch:509]:acc: 0.936765,loss:0.158710\n",
      "[epoch:34,batch:539]:acc: 0.936921,loss:0.158699\n",
      "[epoch:34,batch:569]:acc: 0.936568,loss:0.159323\n",
      "[epoch:34,batch:599]:acc: 0.936510,loss:0.159722\n",
      "[epoch:34,batch:599]: val_loss:0.345162,val_acc:0.870456,val_total:4539\n",
      "[epoch:34,batch:629]:acc: 0.936558,loss:0.159495\n",
      "[epoch:34,batch:659]:acc: 0.936742,loss:0.159059\n",
      "[epoch:34,batch:689]:acc: 0.936957,loss:0.159492\n",
      "[epoch:34,batch:719]:acc: 0.937240,loss:0.159075\n",
      "[epoch:34,batch:749]:acc: 0.937417,loss:0.158843\n",
      "[epoch:34,batch:779]:acc: 0.937139,loss:0.159080\n",
      "[epoch:34,batch:809]:acc: 0.937191,loss:0.159480\n",
      "[epoch:34,batch:839]:acc: 0.937835,loss:0.158836\n",
      "[epoch:34,batch:869]:acc: 0.937967,loss:0.158979\n",
      "[epoch:34,batch:899]:acc: 0.938229,loss:0.158479\n",
      "[epoch:34,batch:899]: val_loss:0.345655,val_acc:0.870676,val_total:4539\n",
      "[epoch:34,batch:929]:acc: 0.938239,loss:0.158344\n",
      "[epoch:34,batch:959]:acc: 0.938184,loss:0.158474\n",
      "[epoch:34,batch:989]:acc: 0.938226,loss:0.158534\n",
      "[epoch:34] :acc: 0.938267,loss:0.158349,lr:0.000000,patience:0\n",
      "[epoch:34]: val_loss:0.347473,val_acc:0.872439,\n",
      "Epoch 35/59\n",
      "----------\n",
      "[epoch:35,batch:29]:acc: 0.937500,loss:0.159667\n",
      "[epoch:35,batch:59]:acc: 0.932292,loss:0.167267\n",
      "[epoch:35,batch:89]:acc: 0.935417,loss:0.162270\n",
      "[epoch:35,batch:119]:acc: 0.936458,loss:0.159587\n",
      "[epoch:35,batch:149]:acc: 0.936667,loss:0.158566\n",
      "[epoch:35,batch:179]:acc: 0.936979,loss:0.160126\n",
      "[epoch:35,batch:209]:acc: 0.936756,loss:0.159954\n",
      "[epoch:35,batch:239]:acc: 0.937109,loss:0.158597\n",
      "[epoch:35,batch:269]:acc: 0.937731,loss:0.158021\n",
      "[epoch:35,batch:299]:acc: 0.937708,loss:0.157618\n",
      "[epoch:35,batch:299]: val_loss:0.347142,val_acc:0.870236,val_total:4539\n",
      "[epoch:35,batch:329]:acc: 0.938258,loss:0.156731\n",
      "[epoch:35,batch:359]:acc: 0.937760,loss:0.157099\n",
      "[epoch:35,batch:389]:acc: 0.936699,loss:0.158556\n",
      "[epoch:35,batch:419]:acc: 0.937277,loss:0.157210\n",
      "[epoch:35,batch:449]:acc: 0.937361,loss:0.157704\n",
      "[epoch:35,batch:479]:acc: 0.936458,loss:0.158261\n",
      "[epoch:35,batch:509]:acc: 0.936826,loss:0.157332\n",
      "[epoch:35,batch:539]:acc: 0.936979,loss:0.157296\n",
      "[epoch:35,batch:569]:acc: 0.937281,loss:0.156670\n",
      "[epoch:35,batch:599]:acc: 0.937708,loss:0.155963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:35,batch:599]: val_loss:0.346072,val_acc:0.871117,val_total:4539\n",
      "[epoch:35,batch:629]:acc: 0.938095,loss:0.155981\n",
      "[epoch:35,batch:659]:acc: 0.938305,loss:0.155822\n",
      "[epoch:35,batch:689]:acc: 0.938225,loss:0.156043\n",
      "[epoch:35,batch:719]:acc: 0.937934,loss:0.156640\n",
      "[epoch:35,batch:749]:acc: 0.937542,loss:0.157430\n",
      "[epoch:35,batch:779]:acc: 0.937460,loss:0.157707\n",
      "[epoch:35,batch:809]:acc: 0.937423,loss:0.157208\n",
      "[epoch:35,batch:839]:acc: 0.937054,loss:0.157744\n",
      "[epoch:35,batch:869]:acc: 0.937464,loss:0.157458\n",
      "[epoch:35,batch:899]:acc: 0.937465,loss:0.157634\n",
      "[epoch:35,batch:899]: val_loss:0.347609,val_acc:0.870897,val_total:4539\n",
      "[epoch:35,batch:929]:acc: 0.936929,loss:0.158119\n",
      "[epoch:35,batch:959]:acc: 0.936979,loss:0.158631\n",
      "[epoch:35,batch:989]:acc: 0.937468,loss:0.157802\n",
      "[epoch:35] :acc: 0.937478,loss:0.157896,lr:0.000000,patience:1\n",
      "[epoch:35]: val_loss:0.348681,val_acc:0.870015,\n",
      "Epoch 36/59\n",
      "----------\n",
      "[epoch:36,batch:29]:acc: 0.942708,loss:0.156154\n",
      "[epoch:36,batch:59]:acc: 0.938021,loss:0.167334\n",
      "[epoch:36,batch:89]:acc: 0.935417,loss:0.167478\n",
      "[epoch:36,batch:119]:acc: 0.935677,loss:0.166800\n",
      "[epoch:36,batch:149]:acc: 0.936667,loss:0.166043\n",
      "[epoch:36,batch:179]:acc: 0.937847,loss:0.162214\n",
      "[epoch:36,batch:209]:acc: 0.938690,loss:0.162714\n",
      "[epoch:36,batch:239]:acc: 0.938932,loss:0.162839\n",
      "[epoch:36,batch:269]:acc: 0.939931,loss:0.162202\n",
      "[epoch:36,batch:299]:acc: 0.940312,loss:0.161203\n",
      "[epoch:36,batch:299]: val_loss:0.346706,val_acc:0.871337,val_total:4539\n",
      "[epoch:36,batch:329]:acc: 0.940341,loss:0.160953\n",
      "[epoch:36,batch:359]:acc: 0.940538,loss:0.160080\n",
      "[epoch:36,batch:389]:acc: 0.941026,loss:0.159733\n",
      "[epoch:36,batch:419]:acc: 0.940625,loss:0.159900\n",
      "[epoch:36,batch:449]:acc: 0.941458,loss:0.158332\n",
      "[epoch:36,batch:479]:acc: 0.940625,loss:0.159011\n",
      "[epoch:36,batch:509]:acc: 0.940319,loss:0.159381\n",
      "[epoch:36,batch:539]:acc: 0.940451,loss:0.158541\n",
      "[epoch:36,batch:569]:acc: 0.940351,loss:0.158212\n",
      "[epoch:36,batch:599]:acc: 0.940521,loss:0.157832\n",
      "[epoch:36,batch:599]: val_loss:0.347498,val_acc:0.871337,val_total:4539\n",
      "[epoch:36,batch:629]:acc: 0.940228,loss:0.157746\n",
      "[epoch:36,batch:659]:acc: 0.939583,loss:0.158121\n",
      "[epoch:36,batch:689]:acc: 0.939583,loss:0.157913\n",
      "[epoch:36,batch:719]:acc: 0.939670,loss:0.157721\n",
      "[epoch:36,batch:749]:acc: 0.939292,loss:0.157844\n",
      "[epoch:36,batch:779]:acc: 0.939864,loss:0.157534\n",
      "[epoch:36,batch:809]:acc: 0.940123,loss:0.157465\n",
      "[epoch:36,batch:839]:acc: 0.939769,loss:0.157884\n",
      "[epoch:36,batch:869]:acc: 0.939655,loss:0.158058\n",
      "[epoch:36,batch:899]:acc: 0.939826,loss:0.157879\n",
      "[epoch:36,batch:899]: val_loss:0.347297,val_acc:0.870676,val_total:4539\n",
      "[epoch:36,batch:929]:acc: 0.939516,loss:0.158071\n",
      "[epoch:36,batch:959]:acc: 0.939258,loss:0.158350\n",
      "[epoch:36,batch:989]:acc: 0.939426,loss:0.158182\n",
      "[epoch:36] :acc: 0.939433,loss:0.158221,lr:0.000000,patience:2\n"
     ]
    }
   ],
   "source": [
    "TrainWithRawData('../model/DesNet121/2018-11-04_acc_best.pth',60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Conda_Env_Pytorch]",
   "language": "python",
   "name": "conda-env-Conda_Env_Pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
