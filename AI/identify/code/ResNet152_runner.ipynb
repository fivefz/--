{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CropModels\n",
    "from CropDataset import MyDataSet,normalize_torch,normalize_05,normalize_dataset,preprocess,preprocess_hflip,preprocess_with_augmentation\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from tensorboardX import SummaryWriter\n",
    "import datetime\n",
    "import os\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from utils import RunningMean\n",
    "import utils\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_CLASS=59\n",
    "SEED=888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "IMAGE_SIZE=224    # 不同模型修改不同的Size\n",
    "IMAGE_TRAIN_PRE='../data/AgriculturalDisease_trainingset/images/'\n",
    "ANNOTATION_TRAIN='../data/AgriculturalDisease_trainingset/AgriculturalDisease_train_annotations_deleteNoise.json' #是否需要剔除两类异常类\n",
    "IMAGE_VAL_PRE='../data/AgriculturalDisease_validationset/images/'\n",
    "ANNOTATION_VAL='../data/AgriculturalDisease_validationset/AgriculturalDisease_validation_annotations_deleteNoise.json' #是否需要剔除两类异常类\n",
    "date=str(datetime.date.today())\n",
    "with open(ANNOTATION_TRAIN) as datafile1:\n",
    "    trainDataFram=pd.read_json(datafile1,orient='records')\n",
    "with open(ANNOTATION_VAL) as datafile2: #first check if it's a valid json file or not\n",
    "    validateDataFram =pd.read_json(datafile2,orient='records')    \n",
    "def getmodel():\n",
    "    print('[+] loading model... ', end='', flush=True)\n",
    "    model=CropModels.resnet152_finetune(NB_CLASS)\n",
    "    model.cuda()\n",
    "    print('Done')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochNum):\n",
    "    writer=SummaryWriter('log/'+date+'/ResNet/') # 创建 /log/日期/InceptionResnet的组织形式  不同模型需要修改不同名称\n",
    "    train_dataset=MyDataSet(json_Description=ANNOTATION_TRAIN,transform=preprocess_with_augmentation(normalize_torch,IMAGE_SIZE),path_pre=IMAGE_TRAIN_PRE)\n",
    "    val_dataset=MyDataSet(json_Description=ANNOTATION_VAL,transform=preprocess(normalize_torch,IMAGE_SIZE),path_pre=IMAGE_VAL_PRE)\n",
    "    train_dataLoader=DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE,num_workers=16,shuffle=True)\n",
    "    val_dataLoader=DataLoader(dataset=val_dataset,batch_size=BATCH_SIZE,num_workers=1,shuffle=False)\n",
    "    model=getmodel()\n",
    "    weight=torch.Tensor([1,3,3,3,3,4,2,3,3,3,3,3,3,3,3,3,2,3,3,3,2,3,4,2,3,1,1,3,2,2,1,3,3,1,3,2,3,3,3,3,2,1,3,2,3,3,3,1,3,3,4,4,3,2,2,3,1,1,3]).cuda()\n",
    "    criterion=nn.CrossEntropyLoss(weight=weight).cuda()\n",
    "#     lx, px = utils.predict(model,val_dataLoader)\n",
    "#     min_loss = criterion(Variable(px), Variable(lx)).item()\n",
    "    min_loss=4.1\n",
    "    print('min_loss is :%f'%(min_loss))\n",
    "    min_acc=0.80\n",
    "    patience=0\n",
    "    lr=0.0\n",
    "    momentum=0.0\n",
    "    for epoch in range(epochNum):\n",
    "        print('Epoch {}/{}'.format(epoch, epochNum - 1))\n",
    "        print('-' * 10)\n",
    "        if epoch==3:\n",
    "            lr=1e-4\n",
    "            momentum=0.9\n",
    "            print('set lr=:%f,momentum=%f'%(lr,momentum))\n",
    "        if patience==2:\n",
    "            patience=0\n",
    "            model.load_state_dict(torch.load('../model/ResNet/'+date+'_loss_best.pth')['state_dict'])\n",
    "            lr=lr/10\n",
    "            print('loss has increased lr divide 10 lr now is :%f'%(lr))\n",
    "        if epoch==0 or epoch==1 or epoch==2: #第一轮首先训练全连接层\n",
    "            lr=1e-3\n",
    "#             optimizer=torch.optim.SGD(params=model.fresh_params(),lr=lr,momentum=0.9)\n",
    "            optimizer = torch.optim.Adam(model.fresh_params(),lr = lr,amsgrad=True,weight_decay=1e-4)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(model.parameters(),lr = lr,amsgrad=True,weight_decay=1e-4)\n",
    "#             optimizer=torch.optim.SGD(params=model.parameters(),lr=lr,momentum=momentum)\n",
    "        running_loss = RunningMean()\n",
    "        running_corrects = RunningMean()\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_dataLoader):\n",
    "            model.train(True)\n",
    "            n_batchsize=inputs.size(0)\n",
    "            inputs = Variable(inputs).cuda()\n",
    "            labels = Variable(labels).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            if isinstance(outputs,tuple):\n",
    "                loss=sum((criterion(o,labels)) for o in outputs)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "            running_loss.update(loss.item(),1)\n",
    "            running_corrects.update(torch.sum(preds == labels.data).data,n_batchsize)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx%30==29:\n",
    "                print('[epoch:%d,batch:%d]:acc: %f,loss:%f'%(epoch,batch_idx,running_corrects.value,running_loss.value))\n",
    "                if batch_idx%300==299: \n",
    "                    niter = epoch * len(train_dataset)/BATCH_SIZE + batch_idx\n",
    "                    writer.add_scalar('Train/Acc',running_corrects.value,niter)\n",
    "                    writer.add_scalar('Train/Loss',running_loss.value,niter)\n",
    "                    lx,px=utils.predict(model,val_dataLoader)\n",
    "                    log_loss = criterion(Variable(px), Variable(lx))\n",
    "                    log_loss = log_loss.item()\n",
    "                    _, preds = torch.max(px, dim=1)\n",
    "                    accuracy = torch.mean((preds == lx).float())\n",
    "                    writer.add_scalar('Val/Acc',accuracy,niter)\n",
    "                    writer.add_scalar('Val/Loss',log_loss,niter)\n",
    "                    print('[epoch:%d,batch:%d]: val_loss:%f,val_acc:%f,val_total:%d'%(epoch,batch_idx,log_loss,accuracy,len(val_dataset)))\n",
    "        print('[epoch:%d] :acc: %f,loss:%f,lr:%f,patience:%d'%(epoch,running_corrects.value,running_loss.value,lr,patience))       \n",
    "        lx,px=utils.predict(model,val_dataLoader)\n",
    "        log_loss = criterion(Variable(px), Variable(lx))\n",
    "        log_loss = log_loss.item()\n",
    "        _, preds = torch.max(px, dim=1)\n",
    "        accuracy = torch.mean((preds == lx).float())\n",
    "        writer.add_scalar('Val/Acc',accuracy,(epoch+1) * len(train_dataset)/BATCH_SIZE)\n",
    "        writer.add_scalar('Val/Loss',log_loss,(epoch+1) * len(train_dataset)/BATCH_SIZE)\n",
    "        print('[epoch:%d]: val_loss:%f,val_acc:%f,'%(epoch,log_loss,accuracy))\n",
    "        if  log_loss < min_loss:\n",
    "            utils.snapshot('../model/', 'ResNet', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy })          \n",
    "            patience = 0\n",
    "            min_loss=log_loss\n",
    "            print('save new model loss,now loss is ',min_loss)\n",
    "        else:\n",
    "            patience += 1\n",
    "        if accuracy>min_acc:\n",
    "            utils.snapshot('../model/', 'ResNet', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy },key='acc') \n",
    "            min_acc=accuracy\n",
    "            print('save new model acc,now acc is ',min_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] loading model... Done\n",
      "min_loss is :4.100000\n",
      "Epoch 0/59\n",
      "----------\n",
      "[epoch:0,batch:29]:acc: 0.182292,loss:3.888799\n",
      "[epoch:0,batch:59]:acc: 0.280208,loss:3.396592\n",
      "[epoch:0,batch:89]:acc: 0.347569,loss:3.006880\n",
      "[epoch:0,batch:119]:acc: 0.396354,loss:2.756406\n",
      "[epoch:0,batch:149]:acc: 0.432292,loss:2.553408\n",
      "[epoch:0,batch:179]:acc: 0.461458,loss:2.415579\n",
      "[epoch:0,batch:209]:acc: 0.480357,loss:2.301396\n",
      "[epoch:0,batch:239]:acc: 0.499349,loss:2.202862\n",
      "[epoch:0,batch:269]:acc: 0.515625,loss:2.112860\n",
      "[epoch:0,batch:299]:acc: 0.524896,loss:2.043763\n",
      "[epoch:0,batch:299]: val_loss:1.086678,val_acc:0.705442,val_total:4539\n",
      "[epoch:0,batch:329]:acc: 0.537405,loss:1.969479\n",
      "[epoch:0,batch:359]:acc: 0.544792,loss:1.912062\n",
      "[epoch:0,batch:389]:acc: 0.552885,loss:1.863799\n",
      "[epoch:0,batch:419]:acc: 0.560491,loss:1.813773\n",
      "[epoch:0,batch:449]:acc: 0.567431,loss:1.768813\n",
      "[epoch:0,batch:479]:acc: 0.572461,loss:1.732220\n",
      "[epoch:0,batch:509]:acc: 0.578370,loss:1.695121\n",
      "[epoch:0,batch:539]:acc: 0.584838,loss:1.658454\n",
      "[epoch:0,batch:569]:acc: 0.589857,loss:1.627693\n",
      "[epoch:0,batch:599]:acc: 0.592969,loss:1.600765\n",
      "[epoch:0,batch:599]: val_loss:0.902752,val_acc:0.714254,val_total:4539\n",
      "[epoch:0,batch:629]:acc: 0.595982,loss:1.578522\n",
      "[epoch:0,batch:659]:acc: 0.600189,loss:1.556411\n",
      "[epoch:0,batch:689]:acc: 0.604393,loss:1.532661\n",
      "[epoch:0,batch:719]:acc: 0.608854,loss:1.508616\n",
      "[epoch:0,batch:749]:acc: 0.611875,loss:1.489266\n",
      "[epoch:0,batch:779]:acc: 0.614704,loss:1.472123\n",
      "[epoch:0,batch:809]:acc: 0.616860,loss:1.454917\n",
      "[epoch:0,batch:839]:acc: 0.619010,loss:1.438107\n",
      "[epoch:0,batch:869]:acc: 0.620941,loss:1.424038\n",
      "[epoch:0,batch:899]:acc: 0.622778,loss:1.411004\n",
      "[epoch:0,batch:899]: val_loss:0.820189,val_acc:0.726371,val_total:4539\n",
      "[epoch:0,batch:929]:acc: 0.624530,loss:1.399561\n",
      "[epoch:0,batch:959]:acc: 0.626530,loss:1.385969\n",
      "[epoch:0,batch:989]:acc: 0.628093,loss:1.376166\n",
      "[epoch:0] :acc: 0.628307,loss:1.375010,lr:0.001000,patience:0\n",
      "[epoch:0]: val_loss:0.878268,val_acc:0.718000,\n",
      "save new model loss,now loss is  0.8782682418823242\n",
      "Epoch 1/59\n",
      "----------\n",
      "[epoch:1,batch:29]:acc: 0.681250,loss:1.114409\n",
      "[epoch:1,batch:59]:acc: 0.695833,loss:1.033106\n",
      "[epoch:1,batch:89]:acc: 0.700347,loss:0.996027\n",
      "[epoch:1,batch:119]:acc: 0.702865,loss:0.976184\n",
      "[epoch:1,batch:149]:acc: 0.702500,loss:0.967711\n",
      "[epoch:1,batch:179]:acc: 0.701910,loss:0.969500\n",
      "[epoch:1,batch:209]:acc: 0.700893,loss:0.967059\n",
      "[epoch:1,batch:239]:acc: 0.698307,loss:0.972369\n",
      "[epoch:1,batch:269]:acc: 0.696759,loss:0.970140\n",
      "[epoch:1,batch:299]:acc: 0.694583,loss:0.974307\n",
      "[epoch:1,batch:299]: val_loss:0.816805,val_acc:0.727032,val_total:4539\n",
      "[epoch:1,batch:329]:acc: 0.695928,loss:0.964213\n",
      "[epoch:1,batch:359]:acc: 0.697569,loss:0.954343\n",
      "[epoch:1,batch:389]:acc: 0.699038,loss:0.947221\n",
      "[epoch:1,batch:419]:acc: 0.697917,loss:0.945772\n",
      "[epoch:1,batch:449]:acc: 0.697917,loss:0.946288\n",
      "[epoch:1,batch:479]:acc: 0.698633,loss:0.946317\n",
      "[epoch:1,batch:509]:acc: 0.699816,loss:0.945376\n",
      "[epoch:1,batch:539]:acc: 0.701042,loss:0.942103\n",
      "[epoch:1,batch:569]:acc: 0.701480,loss:0.943326\n",
      "[epoch:1,batch:599]:acc: 0.701927,loss:0.937982\n",
      "[epoch:1,batch:599]: val_loss:0.784056,val_acc:0.758317,val_total:4539\n",
      "[epoch:1,batch:629]:acc: 0.703819,loss:0.933402\n",
      "[epoch:1,batch:659]:acc: 0.704688,loss:0.931032\n",
      "[epoch:1,batch:689]:acc: 0.704348,loss:0.932023\n",
      "[epoch:1,batch:719]:acc: 0.703906,loss:0.935094\n",
      "[epoch:1,batch:749]:acc: 0.704125,loss:0.932498\n",
      "[epoch:1,batch:779]:acc: 0.704688,loss:0.930854\n",
      "[epoch:1,batch:809]:acc: 0.704051,loss:0.931449\n",
      "[epoch:1,batch:839]:acc: 0.704055,loss:0.929709\n",
      "[epoch:1,batch:869]:acc: 0.704346,loss:0.928536\n",
      "[epoch:1,batch:899]:acc: 0.705556,loss:0.925599\n",
      "[epoch:1,batch:899]: val_loss:0.753821,val_acc:0.753911,val_total:4539\n",
      "[epoch:1,batch:929]:acc: 0.705981,loss:0.923893\n",
      "[epoch:1,batch:959]:acc: 0.705859,loss:0.925546\n",
      "[epoch:1,batch:989]:acc: 0.706755,loss:0.922319\n",
      "[epoch:1] :acc: 0.706845,loss:0.921688,lr:0.001000,patience:0\n",
      "[epoch:1]: val_loss:0.800358,val_acc:0.755012,\n",
      "save new model loss,now loss is  0.8003577589988708\n",
      "Epoch 2/59\n",
      "----------\n",
      "[epoch:2,batch:29]:acc: 0.702083,loss:0.993965\n",
      "[epoch:2,batch:59]:acc: 0.710938,loss:0.916891\n",
      "[epoch:2,batch:89]:acc: 0.720833,loss:0.895411\n",
      "[epoch:2,batch:119]:acc: 0.724219,loss:0.888069\n",
      "[epoch:2,batch:149]:acc: 0.728125,loss:0.868632\n",
      "[epoch:2,batch:179]:acc: 0.729340,loss:0.865164\n",
      "[epoch:2,batch:209]:acc: 0.728720,loss:0.868334\n",
      "[epoch:2,batch:239]:acc: 0.726693,loss:0.865078\n",
      "[epoch:2,batch:269]:acc: 0.728472,loss:0.860022\n",
      "[epoch:2,batch:299]:acc: 0.729688,loss:0.860677\n",
      "[epoch:2,batch:299]: val_loss:0.691600,val_acc:0.776823,val_total:4539\n",
      "[epoch:2,batch:329]:acc: 0.732292,loss:0.852871\n",
      "[epoch:2,batch:359]:acc: 0.732465,loss:0.851735\n",
      "[epoch:2,batch:389]:acc: 0.730929,loss:0.854701\n",
      "[epoch:2,batch:419]:acc: 0.730506,loss:0.854172\n",
      "[epoch:2,batch:449]:acc: 0.729306,loss:0.855834\n",
      "[epoch:2,batch:479]:acc: 0.730013,loss:0.852754\n",
      "[epoch:2,batch:509]:acc: 0.730270,loss:0.849985\n",
      "[epoch:2,batch:539]:acc: 0.728993,loss:0.852181\n",
      "[epoch:2,batch:569]:acc: 0.728783,loss:0.849031\n",
      "[epoch:2,batch:599]:acc: 0.728646,loss:0.849159\n",
      "[epoch:2,batch:599]: val_loss:0.743505,val_acc:0.752809,val_total:4539\n",
      "[epoch:2,batch:629]:acc: 0.726736,loss:0.853569\n",
      "[epoch:2,batch:659]:acc: 0.726894,loss:0.855812\n",
      "[epoch:2,batch:689]:acc: 0.726676,loss:0.853226\n",
      "[epoch:2,batch:719]:acc: 0.725781,loss:0.852286\n",
      "[epoch:2,batch:749]:acc: 0.725792,loss:0.852269\n",
      "[epoch:2,batch:779]:acc: 0.725280,loss:0.852989\n",
      "[epoch:2,batch:809]:acc: 0.724807,loss:0.855189\n",
      "[epoch:2,batch:839]:acc: 0.724591,loss:0.855430\n",
      "[epoch:2,batch:869]:acc: 0.724138,loss:0.857149\n",
      "[epoch:2,batch:899]:acc: 0.724132,loss:0.857270\n",
      "[epoch:2,batch:899]: val_loss:0.728888,val_acc:0.762062,val_total:4539\n",
      "[epoch:2,batch:929]:acc: 0.723488,loss:0.859503\n",
      "[epoch:2,batch:959]:acc: 0.723242,loss:0.857782\n",
      "[epoch:2,batch:989]:acc: 0.723138,loss:0.858093\n",
      "[epoch:2] :acc: 0.723145,loss:0.858095,lr:0.001000,patience:0\n",
      "[epoch:2]: val_loss:0.730540,val_acc:0.770214,\n",
      "save new model loss,now loss is  0.7305402159690857\n",
      "Epoch 3/59\n",
      "----------\n",
      "set lr=:0.000100,momentum=0.900000\n",
      "[epoch:3,batch:29]:acc: 0.709375,loss:0.964990\n",
      "[epoch:3,batch:59]:acc: 0.725521,loss:0.881985\n",
      "[epoch:3,batch:89]:acc: 0.735069,loss:0.830280\n",
      "[epoch:3,batch:119]:acc: 0.738542,loss:0.799642\n",
      "[epoch:3,batch:149]:acc: 0.740208,loss:0.791428\n",
      "[epoch:3,batch:179]:acc: 0.737847,loss:0.791976\n",
      "[epoch:3,batch:209]:acc: 0.740774,loss:0.775804\n",
      "[epoch:3,batch:239]:acc: 0.745313,loss:0.761501\n",
      "[epoch:3,batch:269]:acc: 0.749769,loss:0.751040\n",
      "[epoch:3,batch:299]:acc: 0.754792,loss:0.733143\n",
      "[epoch:3,batch:299]: val_loss:0.541207,val_acc:0.800617,val_total:4539\n",
      "[epoch:3,batch:329]:acc: 0.756913,loss:0.721572\n",
      "[epoch:3,batch:359]:acc: 0.759115,loss:0.711212\n",
      "[epoch:3,batch:389]:acc: 0.758574,loss:0.703165\n",
      "[epoch:3,batch:419]:acc: 0.760565,loss:0.692960\n",
      "[epoch:3,batch:449]:acc: 0.762153,loss:0.687984\n",
      "[epoch:3,batch:479]:acc: 0.762695,loss:0.684316\n",
      "[epoch:3,batch:509]:acc: 0.762929,loss:0.677766\n",
      "[epoch:3,batch:539]:acc: 0.765162,loss:0.669571\n",
      "[epoch:3,batch:569]:acc: 0.765241,loss:0.664315\n",
      "[epoch:3,batch:599]:acc: 0.767396,loss:0.656383\n",
      "[epoch:3,batch:599]: val_loss:0.522471,val_acc:0.808107,val_total:4539\n",
      "[epoch:3,batch:629]:acc: 0.768601,loss:0.650105\n",
      "[epoch:3,batch:659]:acc: 0.770218,loss:0.645233\n",
      "[epoch:3,batch:689]:acc: 0.771014,loss:0.642518\n",
      "[epoch:3,batch:719]:acc: 0.771788,loss:0.637241\n",
      "[epoch:3,batch:749]:acc: 0.773375,loss:0.631199\n",
      "[epoch:3,batch:779]:acc: 0.774639,loss:0.625221\n",
      "[epoch:3,batch:809]:acc: 0.775772,loss:0.619795\n",
      "[epoch:3,batch:839]:acc: 0.777083,loss:0.616451\n",
      "[epoch:3,batch:869]:acc: 0.777874,loss:0.613682\n",
      "[epoch:3,batch:899]:acc: 0.778646,loss:0.609673\n",
      "[epoch:3,batch:899]: val_loss:0.468780,val_acc:0.814497,val_total:4539\n",
      "[epoch:3,batch:929]:acc: 0.778831,loss:0.607716\n",
      "[epoch:3,batch:959]:acc: 0.779492,loss:0.605204\n",
      "[epoch:3,batch:989]:acc: 0.780903,loss:0.600258\n",
      "[epoch:3] :acc: 0.780969,loss:0.600146,lr:0.000100,patience:0\n",
      "[epoch:3]: val_loss:0.452060,val_acc:0.830139,\n",
      "save new model loss,now loss is  0.4520597457885742\n",
      "save new model acc,now acc is  tensor(0.8301, device='cuda:0')\n",
      "Epoch 4/59\n",
      "----------\n",
      "[epoch:4,batch:29]:acc: 0.818750,loss:0.478409\n",
      "[epoch:4,batch:59]:acc: 0.811458,loss:0.528665\n",
      "[epoch:4,batch:89]:acc: 0.809028,loss:0.527571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:4,batch:119]:acc: 0.811198,loss:0.514957\n",
      "[epoch:4,batch:149]:acc: 0.808125,loss:0.511416\n",
      "[epoch:4,batch:179]:acc: 0.807465,loss:0.515490\n",
      "[epoch:4,batch:209]:acc: 0.808185,loss:0.521796\n",
      "[epoch:4,batch:239]:acc: 0.808464,loss:0.524954\n",
      "[epoch:4,batch:269]:acc: 0.809722,loss:0.518436\n",
      "[epoch:4,batch:299]:acc: 0.810312,loss:0.517052\n",
      "[epoch:4,batch:299]: val_loss:0.552869,val_acc:0.806565,val_total:4539\n",
      "[epoch:4,batch:329]:acc: 0.808712,loss:0.518569\n",
      "[epoch:4,batch:359]:acc: 0.807986,loss:0.518524\n",
      "[epoch:4,batch:389]:acc: 0.807532,loss:0.515180\n",
      "[epoch:4,batch:419]:acc: 0.809152,loss:0.510038\n",
      "[epoch:4,batch:449]:acc: 0.809236,loss:0.507527\n",
      "[epoch:4,batch:479]:acc: 0.809245,loss:0.508282\n",
      "[epoch:4,batch:509]:acc: 0.808517,loss:0.509624\n",
      "[epoch:4,batch:539]:acc: 0.808623,loss:0.507373\n",
      "[epoch:4,batch:569]:acc: 0.808498,loss:0.507132\n",
      "[epoch:4,batch:599]:acc: 0.808646,loss:0.506530\n",
      "[epoch:4,batch:599]: val_loss:0.497001,val_acc:0.799295,val_total:4539\n",
      "[epoch:4,batch:629]:acc: 0.808581,loss:0.506385\n",
      "[epoch:4,batch:659]:acc: 0.808712,loss:0.506203\n",
      "[epoch:4,batch:689]:acc: 0.808424,loss:0.506293\n",
      "[epoch:4,batch:719]:acc: 0.809115,loss:0.504178\n",
      "[epoch:4,batch:749]:acc: 0.809500,loss:0.502387\n",
      "[epoch:4,batch:779]:acc: 0.809856,loss:0.500660\n",
      "[epoch:4,batch:809]:acc: 0.809799,loss:0.500063\n",
      "[epoch:4,batch:839]:acc: 0.810045,loss:0.499094\n",
      "[epoch:4,batch:869]:acc: 0.810201,loss:0.498304\n",
      "[epoch:4,batch:899]:acc: 0.810799,loss:0.496539\n",
      "[epoch:4,batch:899]: val_loss:0.470513,val_acc:0.815818,val_total:4539\n",
      "[epoch:4,batch:929]:acc: 0.810786,loss:0.496968\n",
      "[epoch:4,batch:959]:acc: 0.810579,loss:0.497330\n",
      "[epoch:4,batch:989]:acc: 0.810638,loss:0.496005\n",
      "[epoch:4] :acc: 0.810575,loss:0.497996,lr:0.000100,patience:0\n",
      "[epoch:4]: val_loss:0.492713,val_acc:0.804362,\n",
      "Epoch 5/59\n",
      "----------\n",
      "[epoch:5,batch:29]:acc: 0.820833,loss:0.473618\n",
      "[epoch:5,batch:59]:acc: 0.820833,loss:0.476097\n",
      "[epoch:5,batch:89]:acc: 0.809375,loss:0.498132\n",
      "[epoch:5,batch:119]:acc: 0.817187,loss:0.477753\n",
      "[epoch:5,batch:149]:acc: 0.818542,loss:0.472841\n",
      "[epoch:5,batch:179]:acc: 0.818056,loss:0.469464\n",
      "[epoch:5,batch:209]:acc: 0.819643,loss:0.461318\n",
      "[epoch:5,batch:239]:acc: 0.820443,loss:0.454595\n",
      "[epoch:5,batch:269]:acc: 0.819213,loss:0.455710\n",
      "[epoch:5,batch:299]:acc: 0.819479,loss:0.454531\n",
      "[epoch:5,batch:299]: val_loss:0.447973,val_acc:0.818682,val_total:4539\n",
      "[epoch:5,batch:329]:acc: 0.821780,loss:0.451508\n",
      "[epoch:5,batch:359]:acc: 0.823524,loss:0.449449\n",
      "[epoch:5,batch:389]:acc: 0.822596,loss:0.451539\n",
      "[epoch:5,batch:419]:acc: 0.822247,loss:0.454463\n",
      "[epoch:5,batch:449]:acc: 0.822153,loss:0.453496\n",
      "[epoch:5,batch:479]:acc: 0.822982,loss:0.452837\n",
      "[epoch:5,batch:509]:acc: 0.823039,loss:0.450536\n",
      "[epoch:5,batch:539]:acc: 0.824479,loss:0.448894\n",
      "[epoch:5,batch:569]:acc: 0.824890,loss:0.447084\n",
      "[epoch:5,batch:599]:acc: 0.824427,loss:0.446377\n",
      "[epoch:5,batch:599]: val_loss:0.455746,val_acc:0.823529,val_total:4539\n",
      "[epoch:5,batch:629]:acc: 0.823958,loss:0.445863\n",
      "[epoch:5,batch:659]:acc: 0.824148,loss:0.446636\n",
      "[epoch:5,batch:689]:acc: 0.824592,loss:0.446068\n",
      "[epoch:5,batch:719]:acc: 0.824740,loss:0.445212\n",
      "[epoch:5,batch:749]:acc: 0.825083,loss:0.445014\n",
      "[epoch:5,batch:779]:acc: 0.825321,loss:0.444402\n",
      "[epoch:5,batch:809]:acc: 0.823341,loss:0.447403\n",
      "[epoch:5,batch:839]:acc: 0.823400,loss:0.447748\n",
      "[epoch:5,batch:869]:acc: 0.823922,loss:0.447400\n",
      "[epoch:5,batch:899]:acc: 0.824097,loss:0.447057\n",
      "[epoch:5,batch:899]: val_loss:0.465307,val_acc:0.809650,val_total:4539\n",
      "[epoch:5,batch:929]:acc: 0.823757,loss:0.448128\n",
      "[epoch:5,batch:959]:acc: 0.823340,loss:0.447893\n",
      "[epoch:5,batch:989]:acc: 0.823453,loss:0.447850\n",
      "[epoch:5] :acc: 0.823281,loss:0.448454,lr:0.000100,patience:1\n",
      "[epoch:5]: val_loss:0.464867,val_acc:0.825733,\n",
      "Epoch 6/59\n",
      "----------\n",
      "loss has increased lr divide 10 lr now is :0.000010\n",
      "[epoch:6,batch:29]:acc: 0.834375,loss:0.389753\n",
      "[epoch:6,batch:59]:acc: 0.842187,loss:0.390880\n",
      "[epoch:6,batch:89]:acc: 0.838542,loss:0.391746\n",
      "[epoch:6,batch:119]:acc: 0.841146,loss:0.390896\n",
      "[epoch:6,batch:149]:acc: 0.842708,loss:0.389496\n",
      "[epoch:6,batch:179]:acc: 0.846354,loss:0.380246\n",
      "[epoch:6,batch:209]:acc: 0.846726,loss:0.378424\n",
      "[epoch:6,batch:239]:acc: 0.846745,loss:0.375566\n",
      "[epoch:6,batch:269]:acc: 0.849421,loss:0.372460\n",
      "[epoch:6,batch:299]:acc: 0.849375,loss:0.372659\n",
      "[epoch:6,batch:299]: val_loss:0.397130,val_acc:0.849747,val_total:4539\n",
      "[epoch:6,batch:329]:acc: 0.849337,loss:0.369506\n",
      "[epoch:6,batch:359]:acc: 0.850347,loss:0.369704\n",
      "[epoch:6,batch:389]:acc: 0.850721,loss:0.366481\n",
      "[epoch:6,batch:419]:acc: 0.851786,loss:0.362923\n",
      "[epoch:6,batch:449]:acc: 0.853056,loss:0.363516\n",
      "[epoch:6,batch:479]:acc: 0.853646,loss:0.361741\n",
      "[epoch:6,batch:509]:acc: 0.853125,loss:0.364097\n",
      "[epoch:6,batch:539]:acc: 0.853356,loss:0.363274\n",
      "[epoch:6,batch:569]:acc: 0.853125,loss:0.364181\n",
      "[epoch:6,batch:599]:acc: 0.854635,loss:0.363321\n",
      "[epoch:6,batch:599]: val_loss:0.402843,val_acc:0.835647,val_total:4539\n",
      "[epoch:6,batch:629]:acc: 0.854911,loss:0.362054\n",
      "[epoch:6,batch:659]:acc: 0.854877,loss:0.362328\n",
      "[epoch:6,batch:689]:acc: 0.855797,loss:0.361324\n",
      "[epoch:6,batch:719]:acc: 0.856120,loss:0.360910\n",
      "[epoch:6,batch:749]:acc: 0.856125,loss:0.360819\n",
      "[epoch:6,batch:779]:acc: 0.855769,loss:0.360359\n",
      "[epoch:6,batch:809]:acc: 0.856173,loss:0.359906\n",
      "[epoch:6,batch:839]:acc: 0.856324,loss:0.359298\n",
      "[epoch:6,batch:869]:acc: 0.856430,loss:0.358763\n",
      "[epoch:6,batch:899]:acc: 0.856354,loss:0.359138\n",
      "[epoch:6,batch:899]: val_loss:0.388582,val_acc:0.852170,val_total:4539\n",
      "[epoch:6,batch:929]:acc: 0.856116,loss:0.358827\n",
      "[epoch:6,batch:959]:acc: 0.856087,loss:0.359022\n",
      "[epoch:6,batch:989]:acc: 0.856376,loss:0.358474\n",
      "[epoch:6] :acc: 0.856355,loss:0.358215,lr:0.000010,patience:0\n",
      "[epoch:6]: val_loss:0.383077,val_acc:0.852831,\n",
      "save new model loss,now loss is  0.383076548576355\n",
      "save new model acc,now acc is  tensor(0.8528, device='cuda:0')\n",
      "Epoch 7/59\n",
      "----------\n",
      "[epoch:7,batch:29]:acc: 0.858333,loss:0.333533\n",
      "[epoch:7,batch:59]:acc: 0.860938,loss:0.327190\n",
      "[epoch:7,batch:89]:acc: 0.862153,loss:0.327647\n",
      "[epoch:7,batch:119]:acc: 0.860938,loss:0.328148\n",
      "[epoch:7,batch:149]:acc: 0.865417,loss:0.327228\n",
      "[epoch:7,batch:179]:acc: 0.869097,loss:0.319521\n",
      "[epoch:7,batch:209]:acc: 0.869048,loss:0.319542\n",
      "[epoch:7,batch:239]:acc: 0.869922,loss:0.318147\n",
      "[epoch:7,batch:269]:acc: 0.870602,loss:0.312033\n",
      "[epoch:7,batch:299]:acc: 0.870521,loss:0.313670\n",
      "[epoch:7,batch:299]: val_loss:0.383875,val_acc:0.857017,val_total:4539\n",
      "[epoch:7,batch:329]:acc: 0.872159,loss:0.310887\n",
      "[epoch:7,batch:359]:acc: 0.872396,loss:0.307944\n",
      "[epoch:7,batch:389]:acc: 0.870513,loss:0.310355\n",
      "[epoch:7,batch:419]:acc: 0.870387,loss:0.309343\n",
      "[epoch:7,batch:449]:acc: 0.870972,loss:0.308940\n",
      "[epoch:7,batch:479]:acc: 0.870247,loss:0.313065\n",
      "[epoch:7,batch:509]:acc: 0.870282,loss:0.313828\n",
      "[epoch:7,batch:539]:acc: 0.870081,loss:0.313823\n",
      "[epoch:7,batch:569]:acc: 0.870066,loss:0.313297\n",
      "[epoch:7,batch:599]:acc: 0.870260,loss:0.313377\n",
      "[epoch:7,batch:599]: val_loss:0.389296,val_acc:0.849967,val_total:4539\n",
      "[epoch:7,batch:629]:acc: 0.870437,loss:0.313639\n",
      "[epoch:7,batch:659]:acc: 0.869839,loss:0.315105\n",
      "[epoch:7,batch:689]:acc: 0.870788,loss:0.313489\n",
      "[epoch:7,batch:719]:acc: 0.870964,loss:0.313942\n",
      "[epoch:7,batch:749]:acc: 0.870917,loss:0.313991\n",
      "[epoch:7,batch:779]:acc: 0.870553,loss:0.315715\n",
      "[epoch:7,batch:809]:acc: 0.869792,loss:0.316885\n",
      "[epoch:7,batch:839]:acc: 0.869382,loss:0.317092\n",
      "[epoch:7,batch:869]:acc: 0.868930,loss:0.317080\n",
      "[epoch:7,batch:899]:acc: 0.868854,loss:0.318595\n",
      "[epoch:7,batch:899]: val_loss:0.388883,val_acc:0.847323,val_total:4539\n",
      "[epoch:7,batch:929]:acc: 0.868851,loss:0.318957\n",
      "[epoch:7,batch:959]:acc: 0.868490,loss:0.320493\n",
      "[epoch:7,batch:989]:acc: 0.868561,loss:0.319905\n",
      "[epoch:7] :acc: 0.868525,loss:0.320407,lr:0.000010,patience:0\n",
      "[epoch:7]: val_loss:0.387675,val_acc:0.851289,\n",
      "Epoch 8/59\n",
      "----------\n",
      "[epoch:8,batch:29]:acc: 0.871875,loss:0.296068\n",
      "[epoch:8,batch:59]:acc: 0.873437,loss:0.292077\n",
      "[epoch:8,batch:89]:acc: 0.875000,loss:0.293843\n",
      "[epoch:8,batch:119]:acc: 0.874219,loss:0.292885\n",
      "[epoch:8,batch:149]:acc: 0.880000,loss:0.286207\n",
      "[epoch:8,batch:179]:acc: 0.877431,loss:0.287634\n",
      "[epoch:8,batch:209]:acc: 0.876786,loss:0.288916\n",
      "[epoch:8,batch:239]:acc: 0.875911,loss:0.286619\n",
      "[epoch:8,batch:269]:acc: 0.875579,loss:0.287658\n",
      "[epoch:8,batch:299]:acc: 0.875521,loss:0.285640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:8,batch:299]: val_loss:0.384681,val_acc:0.857458,val_total:4539\n",
      "[epoch:8,batch:329]:acc: 0.874621,loss:0.287535\n",
      "[epoch:8,batch:359]:acc: 0.877517,loss:0.284146\n",
      "[epoch:8,batch:389]:acc: 0.876683,loss:0.286941\n",
      "[epoch:8,batch:419]:acc: 0.877753,loss:0.287166\n",
      "[epoch:8,batch:449]:acc: 0.877569,loss:0.287028\n",
      "[epoch:8,batch:479]:acc: 0.878060,loss:0.286750\n",
      "[epoch:8,batch:509]:acc: 0.878738,loss:0.285839\n",
      "[epoch:8,batch:539]:acc: 0.879514,loss:0.285790\n",
      "[epoch:8,batch:569]:acc: 0.879112,loss:0.286083\n",
      "[epoch:8,batch:599]:acc: 0.879844,loss:0.285001\n",
      "[epoch:8,batch:599]: val_loss:0.401345,val_acc:0.849747,val_total:4539\n",
      "[epoch:8,batch:629]:acc: 0.878869,loss:0.287472\n",
      "[epoch:8,batch:659]:acc: 0.877936,loss:0.289071\n",
      "[epoch:8,batch:689]:acc: 0.877672,loss:0.289436\n",
      "[epoch:8,batch:719]:acc: 0.877778,loss:0.288239\n",
      "[epoch:8,batch:749]:acc: 0.877125,loss:0.287742\n",
      "[epoch:8,batch:779]:acc: 0.876963,loss:0.288942\n",
      "[epoch:8,batch:809]:acc: 0.876427,loss:0.290495\n",
      "[epoch:8,batch:839]:acc: 0.876600,loss:0.290846\n",
      "[epoch:8,batch:869]:acc: 0.877047,loss:0.290486\n",
      "[epoch:8,batch:899]:acc: 0.877153,loss:0.290204\n",
      "[epoch:8,batch:899]: val_loss:0.387168,val_acc:0.853051,val_total:4539\n",
      "[epoch:8,batch:929]:acc: 0.877722,loss:0.289501\n",
      "[epoch:8,batch:959]:acc: 0.877441,loss:0.290448\n",
      "[epoch:8,batch:989]:acc: 0.876515,loss:0.291593\n",
      "[epoch:8] :acc: 0.876565,loss:0.291491,lr:0.000010,patience:1\n",
      "[epoch:8]: val_loss:0.388324,val_acc:0.851950,\n",
      "Epoch 9/59\n",
      "----------\n",
      "loss has increased lr divide 10 lr now is :0.000001\n",
      "[epoch:9,batch:29]:acc: 0.880208,loss:0.304623\n",
      "[epoch:9,batch:59]:acc: 0.882292,loss:0.309505\n",
      "[epoch:9,batch:89]:acc: 0.877431,loss:0.310427\n",
      "[epoch:9,batch:119]:acc: 0.871875,loss:0.315547\n",
      "[epoch:9,batch:149]:acc: 0.871458,loss:0.316347\n",
      "[epoch:9,batch:179]:acc: 0.873437,loss:0.314663\n",
      "[epoch:9,batch:209]:acc: 0.872321,loss:0.315148\n",
      "[epoch:9,batch:239]:acc: 0.872526,loss:0.314632\n",
      "[epoch:9,batch:269]:acc: 0.871065,loss:0.316430\n",
      "[epoch:9,batch:299]:acc: 0.870000,loss:0.315536\n",
      "[epoch:9,batch:299]: val_loss:0.377691,val_acc:0.855475,val_total:4539\n",
      "[epoch:9,batch:329]:acc: 0.869981,loss:0.315382\n",
      "[epoch:9,batch:359]:acc: 0.871007,loss:0.314752\n",
      "[epoch:9,batch:389]:acc: 0.871234,loss:0.314450\n",
      "[epoch:9,batch:419]:acc: 0.870461,loss:0.314571\n",
      "[epoch:9,batch:449]:acc: 0.870764,loss:0.314094\n",
      "[epoch:9,batch:479]:acc: 0.871029,loss:0.313546\n",
      "[epoch:9,batch:509]:acc: 0.871324,loss:0.313174\n",
      "[epoch:9,batch:539]:acc: 0.870775,loss:0.312676\n",
      "[epoch:9,batch:569]:acc: 0.870340,loss:0.312781\n",
      "[epoch:9,batch:599]:acc: 0.871042,loss:0.310935\n",
      "[epoch:9,batch:599]: val_loss:0.379906,val_acc:0.853933,val_total:4539\n",
      "[epoch:9,batch:629]:acc: 0.870685,loss:0.311490\n",
      "[epoch:9,batch:659]:acc: 0.870170,loss:0.312610\n",
      "[epoch:9,batch:689]:acc: 0.870562,loss:0.312014\n",
      "[epoch:9,batch:719]:acc: 0.870920,loss:0.312144\n",
      "[epoch:9,batch:749]:acc: 0.871250,loss:0.311849\n",
      "[epoch:9,batch:779]:acc: 0.871915,loss:0.310853\n",
      "[epoch:9,batch:809]:acc: 0.871373,loss:0.311495\n",
      "[epoch:9,batch:839]:acc: 0.871057,loss:0.311605\n",
      "[epoch:9,batch:869]:acc: 0.871336,loss:0.311389\n",
      "[epoch:9,batch:899]:acc: 0.870729,loss:0.311982\n",
      "[epoch:9,batch:899]: val_loss:0.376528,val_acc:0.858119,val_total:4539\n",
      "[epoch:9,batch:929]:acc: 0.870934,loss:0.313165\n",
      "[epoch:9,batch:959]:acc: 0.870638,loss:0.314325\n",
      "[epoch:9,batch:989]:acc: 0.871023,loss:0.313799\n",
      "[epoch:9] :acc: 0.870984,loss:0.313757,lr:0.000001,patience:0\n",
      "[epoch:9]: val_loss:0.377839,val_acc:0.855254,\n",
      "save new model loss,now loss is  0.37783902883529663\n",
      "save new model acc,now acc is  tensor(0.8553, device='cuda:0')\n",
      "Epoch 10/59\n",
      "----------\n",
      "[epoch:10,batch:29]:acc: 0.864583,loss:0.343243\n",
      "[epoch:10,batch:59]:acc: 0.860938,loss:0.333533\n",
      "[epoch:10,batch:89]:acc: 0.862500,loss:0.318396\n",
      "[epoch:10,batch:119]:acc: 0.869792,loss:0.311667\n",
      "[epoch:10,batch:149]:acc: 0.865417,loss:0.321159\n",
      "[epoch:10,batch:179]:acc: 0.869444,loss:0.315200\n",
      "[epoch:10,batch:209]:acc: 0.870536,loss:0.310257\n",
      "[epoch:10,batch:239]:acc: 0.869661,loss:0.312614\n",
      "[epoch:10,batch:269]:acc: 0.871759,loss:0.310190\n",
      "[epoch:10,batch:299]:acc: 0.872083,loss:0.309780\n",
      "[epoch:10,batch:299]: val_loss:0.374879,val_acc:0.855254,val_total:4539\n",
      "[epoch:10,batch:329]:acc: 0.871496,loss:0.308638\n",
      "[epoch:10,batch:359]:acc: 0.871441,loss:0.308410\n",
      "[epoch:10,batch:389]:acc: 0.871795,loss:0.307018\n",
      "[epoch:10,batch:419]:acc: 0.872693,loss:0.305577\n",
      "[epoch:10,batch:449]:acc: 0.872500,loss:0.306841\n",
      "[epoch:10,batch:479]:acc: 0.872591,loss:0.307335\n",
      "[epoch:10,batch:509]:acc: 0.873284,loss:0.306072\n",
      "[epoch:10,batch:539]:acc: 0.873553,loss:0.305976\n",
      "[epoch:10,batch:569]:acc: 0.872368,loss:0.306859\n",
      "[epoch:10,batch:599]:acc: 0.873281,loss:0.305827\n",
      "[epoch:10,batch:599]: val_loss:0.375482,val_acc:0.858119,val_total:4539\n",
      "[epoch:10,batch:629]:acc: 0.873115,loss:0.305885\n",
      "[epoch:10,batch:659]:acc: 0.873295,loss:0.304710\n",
      "[epoch:10,batch:689]:acc: 0.872283,loss:0.305351\n",
      "[epoch:10,batch:719]:acc: 0.872092,loss:0.305251\n",
      "[epoch:10,batch:749]:acc: 0.872542,loss:0.304206\n",
      "[epoch:10,batch:779]:acc: 0.872796,loss:0.303391\n",
      "[epoch:10,batch:809]:acc: 0.872608,loss:0.303105\n",
      "[epoch:10,batch:839]:acc: 0.873177,loss:0.302830\n",
      "[epoch:10,batch:869]:acc: 0.873204,loss:0.302624\n",
      "[epoch:10,batch:899]:acc: 0.873507,loss:0.302521\n",
      "[epoch:10,batch:899]: val_loss:0.375208,val_acc:0.857017,val_total:4539\n",
      "[epoch:10,batch:929]:acc: 0.873253,loss:0.302253\n",
      "[epoch:10,batch:959]:acc: 0.873210,loss:0.302449\n",
      "[epoch:10,batch:989]:acc: 0.873548,loss:0.302834\n",
      "[epoch:10] :acc: 0.873443,loss:0.302907,lr:0.000001,patience:0\n",
      "[epoch:10]: val_loss:0.378634,val_acc:0.857678,\n",
      "save new model acc,now acc is  tensor(0.8577, device='cuda:0')\n",
      "Epoch 11/59\n",
      "----------\n",
      "[epoch:11,batch:29]:acc: 0.878125,loss:0.332551\n",
      "[epoch:11,batch:59]:acc: 0.877083,loss:0.314009\n",
      "[epoch:11,batch:89]:acc: 0.874306,loss:0.310512\n",
      "[epoch:11,batch:119]:acc: 0.876302,loss:0.301446\n",
      "[epoch:11,batch:149]:acc: 0.877917,loss:0.301411\n",
      "[epoch:11,batch:179]:acc: 0.878993,loss:0.298584\n",
      "[epoch:11,batch:209]:acc: 0.878125,loss:0.304880\n",
      "[epoch:11,batch:239]:acc: 0.877344,loss:0.308014\n",
      "[epoch:11,batch:269]:acc: 0.878472,loss:0.304047\n",
      "[epoch:11,batch:299]:acc: 0.879896,loss:0.299768\n",
      "[epoch:11,batch:299]: val_loss:0.377141,val_acc:0.857237,val_total:4539\n",
      "[epoch:11,batch:329]:acc: 0.879640,loss:0.298619\n",
      "[epoch:11,batch:359]:acc: 0.880382,loss:0.297139\n",
      "[epoch:11,batch:389]:acc: 0.879647,loss:0.297902\n",
      "[epoch:11,batch:419]:acc: 0.879018,loss:0.297611\n",
      "[epoch:11,batch:449]:acc: 0.879792,loss:0.296151\n",
      "[epoch:11,batch:479]:acc: 0.880143,loss:0.295762\n",
      "[epoch:11,batch:509]:acc: 0.879350,loss:0.296051\n",
      "[epoch:11,batch:539]:acc: 0.878877,loss:0.296230\n",
      "[epoch:11,batch:569]:acc: 0.879057,loss:0.295627\n",
      "[epoch:11,batch:599]:acc: 0.879375,loss:0.294876\n",
      "[epoch:11,batch:599]: val_loss:0.374775,val_acc:0.856797,val_total:4539\n",
      "[epoch:11,batch:629]:acc: 0.878472,loss:0.295386\n",
      "[epoch:11,batch:659]:acc: 0.878078,loss:0.295383\n",
      "[epoch:11,batch:689]:acc: 0.877491,loss:0.294720\n",
      "[epoch:11,batch:719]:acc: 0.877734,loss:0.294921\n",
      "[epoch:11,batch:749]:acc: 0.878125,loss:0.294945\n",
      "[epoch:11,batch:779]:acc: 0.877163,loss:0.295676\n",
      "[epoch:11,batch:809]:acc: 0.876620,loss:0.296278\n",
      "[epoch:11,batch:839]:acc: 0.876302,loss:0.296576\n",
      "[epoch:11,batch:869]:acc: 0.876616,loss:0.296099\n",
      "[epoch:11,batch:899]:acc: 0.876771,loss:0.295875\n",
      "[epoch:11,batch:899]: val_loss:0.374249,val_acc:0.861423,val_total:4539\n",
      "[epoch:11,batch:929]:acc: 0.876310,loss:0.295647\n",
      "[epoch:11,batch:959]:acc: 0.876888,loss:0.295354\n",
      "[epoch:11,batch:989]:acc: 0.876547,loss:0.296520\n",
      "[epoch:11] :acc: 0.876659,loss:0.296324,lr:0.000001,patience:1\n",
      "[epoch:11]: val_loss:0.387043,val_acc:0.852170,\n",
      "Epoch 12/59\n",
      "----------\n",
      "loss has increased lr divide 10 lr now is :0.000000\n",
      "[epoch:12,batch:29]:acc: 0.865625,loss:0.308557\n",
      "[epoch:12,batch:59]:acc: 0.874479,loss:0.303127\n",
      "[epoch:12,batch:89]:acc: 0.874653,loss:0.296318\n",
      "[epoch:12,batch:119]:acc: 0.873698,loss:0.295865\n",
      "[epoch:12,batch:149]:acc: 0.878750,loss:0.298505\n",
      "[epoch:12,batch:179]:acc: 0.878819,loss:0.304046\n",
      "[epoch:12,batch:209]:acc: 0.880060,loss:0.300355\n",
      "[epoch:12,batch:239]:acc: 0.883333,loss:0.293037\n",
      "[epoch:12,batch:269]:acc: 0.883449,loss:0.289656\n",
      "[epoch:12,batch:299]:acc: 0.882396,loss:0.290985\n",
      "[epoch:12,batch:299]: val_loss:0.373256,val_acc:0.857458,val_total:4539\n",
      "[epoch:12,batch:329]:acc: 0.882197,loss:0.293215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:12,batch:359]:acc: 0.880295,loss:0.295763\n",
      "[epoch:12,batch:389]:acc: 0.879968,loss:0.296310\n",
      "[epoch:12,batch:419]:acc: 0.879539,loss:0.296985\n",
      "[epoch:12,batch:449]:acc: 0.877847,loss:0.299536\n",
      "[epoch:12,batch:479]:acc: 0.876953,loss:0.302383\n",
      "[epoch:12,batch:509]:acc: 0.876961,loss:0.301628\n",
      "[epoch:12,batch:539]:acc: 0.876505,loss:0.301968\n",
      "[epoch:12,batch:569]:acc: 0.875439,loss:0.304161\n",
      "[epoch:12,batch:599]:acc: 0.876042,loss:0.303205\n",
      "[epoch:12,batch:599]: val_loss:0.375603,val_acc:0.859220,val_total:4539\n",
      "[epoch:12,batch:629]:acc: 0.876538,loss:0.302276\n",
      "[epoch:12,batch:659]:acc: 0.876610,loss:0.302057\n",
      "[epoch:12,batch:689]:acc: 0.877174,loss:0.302394\n",
      "[epoch:12,batch:719]:acc: 0.876953,loss:0.304374\n",
      "[epoch:12,batch:749]:acc: 0.876833,loss:0.304188\n",
      "[epoch:12,batch:779]:acc: 0.876442,loss:0.304025\n",
      "[epoch:12,batch:809]:acc: 0.876427,loss:0.303933\n",
      "[epoch:12,batch:839]:acc: 0.876600,loss:0.304259\n",
      "[epoch:12,batch:869]:acc: 0.877011,loss:0.303216\n",
      "[epoch:12,batch:899]:acc: 0.876806,loss:0.302519\n",
      "[epoch:12,batch:899]: val_loss:0.372976,val_acc:0.857237,val_total:4539\n",
      "[epoch:12,batch:929]:acc: 0.876243,loss:0.303173\n",
      "[epoch:12,batch:959]:acc: 0.876563,loss:0.302438\n",
      "[epoch:12,batch:989]:acc: 0.876736,loss:0.302014\n",
      "[epoch:12] :acc: 0.876691,loss:0.303066,lr:0.000000,patience:0\n",
      "[epoch:12]: val_loss:0.373333,val_acc:0.857458,\n",
      "save new model loss,now loss is  0.37333282828330994\n",
      "Epoch 13/59\n",
      "----------\n",
      "[epoch:13,batch:29]:acc: 0.872917,loss:0.303540\n",
      "[epoch:13,batch:59]:acc: 0.877604,loss:0.293987\n",
      "[epoch:13,batch:89]:acc: 0.882292,loss:0.282218\n",
      "[epoch:13,batch:119]:acc: 0.882031,loss:0.281312\n",
      "[epoch:13,batch:149]:acc: 0.880625,loss:0.286121\n",
      "[epoch:13,batch:179]:acc: 0.877431,loss:0.291186\n",
      "[epoch:13,batch:209]:acc: 0.876339,loss:0.298769\n",
      "[epoch:13,batch:239]:acc: 0.876563,loss:0.299596\n",
      "[epoch:13,batch:269]:acc: 0.875694,loss:0.298695\n",
      "[epoch:13,batch:299]:acc: 0.876250,loss:0.298783\n",
      "[epoch:13,batch:299]: val_loss:0.374783,val_acc:0.859000,val_total:4539\n",
      "[epoch:13,batch:329]:acc: 0.876231,loss:0.300300\n",
      "[epoch:13,batch:359]:acc: 0.878212,loss:0.296171\n",
      "[epoch:13,batch:389]:acc: 0.877885,loss:0.296909\n",
      "[epoch:13,batch:419]:acc: 0.878199,loss:0.295547\n",
      "[epoch:13,batch:449]:acc: 0.877153,loss:0.296991\n",
      "[epoch:13,batch:479]:acc: 0.875065,loss:0.301398\n",
      "[epoch:13,batch:509]:acc: 0.875858,loss:0.300293\n",
      "[epoch:13,batch:539]:acc: 0.875521,loss:0.301783\n",
      "[epoch:13,batch:569]:acc: 0.874561,loss:0.302235\n",
      "[epoch:13,batch:599]:acc: 0.873906,loss:0.302607\n",
      "[epoch:13,batch:599]: val_loss:0.376823,val_acc:0.857017,val_total:4539\n",
      "[epoch:13,batch:629]:acc: 0.873859,loss:0.302672\n",
      "[epoch:13,batch:659]:acc: 0.874053,loss:0.302953\n",
      "[epoch:13,batch:689]:acc: 0.874411,loss:0.303925\n",
      "[epoch:13,batch:719]:acc: 0.874826,loss:0.304367\n",
      "[epoch:13,batch:749]:acc: 0.874917,loss:0.303717\n",
      "[epoch:13,batch:779]:acc: 0.874760,loss:0.303901\n",
      "[epoch:13,batch:809]:acc: 0.874537,loss:0.303824\n",
      "[epoch:13,batch:839]:acc: 0.874851,loss:0.302441\n",
      "[epoch:13,batch:869]:acc: 0.875144,loss:0.302344\n",
      "[epoch:13,batch:899]:acc: 0.875035,loss:0.302103\n",
      "[epoch:13,batch:899]: val_loss:0.376497,val_acc:0.858559,val_total:4539\n",
      "[epoch:13,batch:929]:acc: 0.874866,loss:0.302260\n",
      "[epoch:13,batch:959]:acc: 0.874772,loss:0.303042\n",
      "[epoch:13,batch:989]:acc: 0.875253,loss:0.302144\n",
      "[epoch:13] :acc: 0.875209,loss:0.302161,lr:0.000000,patience:0\n",
      "[epoch:13]: val_loss:0.377701,val_acc:0.857458,\n",
      "Epoch 14/59\n",
      "----------\n",
      "[epoch:14,batch:29]:acc: 0.882292,loss:0.268613\n",
      "[epoch:14,batch:59]:acc: 0.877083,loss:0.274548\n",
      "[epoch:14,batch:89]:acc: 0.876389,loss:0.280562\n",
      "[epoch:14,batch:119]:acc: 0.873698,loss:0.289596\n",
      "[epoch:14,batch:149]:acc: 0.872083,loss:0.290003\n",
      "[epoch:14,batch:179]:acc: 0.872569,loss:0.291761\n",
      "[epoch:14,batch:209]:acc: 0.872173,loss:0.296041\n",
      "[epoch:14,batch:239]:acc: 0.873177,loss:0.292594\n",
      "[epoch:14,batch:269]:acc: 0.873958,loss:0.292772\n",
      "[epoch:14,batch:299]:acc: 0.874896,loss:0.295988\n",
      "[epoch:14,batch:299]: val_loss:0.373833,val_acc:0.859440,val_total:4539\n",
      "[epoch:14,batch:329]:acc: 0.874148,loss:0.298831\n",
      "[epoch:14,batch:359]:acc: 0.874826,loss:0.299516\n",
      "[epoch:14,batch:389]:acc: 0.875160,loss:0.299578\n",
      "[epoch:14,batch:419]:acc: 0.875893,loss:0.297373\n",
      "[epoch:14,batch:449]:acc: 0.875833,loss:0.297028\n",
      "[epoch:14,batch:479]:acc: 0.874674,loss:0.299260\n",
      "[epoch:14,batch:509]:acc: 0.875551,loss:0.299621\n",
      "[epoch:14,batch:539]:acc: 0.875463,loss:0.300748\n",
      "[epoch:14,batch:569]:acc: 0.875658,loss:0.300657\n",
      "[epoch:14,batch:599]:acc: 0.875833,loss:0.300078\n",
      "[epoch:14,batch:599]: val_loss:0.374001,val_acc:0.856356,val_total:4539\n",
      "[epoch:14,batch:629]:acc: 0.874653,loss:0.302124\n",
      "[epoch:14,batch:659]:acc: 0.874621,loss:0.303217\n",
      "[epoch:14,batch:689]:acc: 0.874547,loss:0.303179\n",
      "[epoch:14,batch:719]:acc: 0.875174,loss:0.303512\n",
      "[epoch:14,batch:749]:acc: 0.875792,loss:0.302701\n",
      "[epoch:14,batch:779]:acc: 0.875641,loss:0.303198\n",
      "[epoch:14,batch:809]:acc: 0.875502,loss:0.303117\n",
      "[epoch:14,batch:839]:acc: 0.875298,loss:0.302907\n",
      "[epoch:14,batch:869]:acc: 0.874928,loss:0.302328\n",
      "[epoch:14,batch:899]:acc: 0.874722,loss:0.302783\n",
      "[epoch:14,batch:899]: val_loss:0.374695,val_acc:0.858779,val_total:4539\n",
      "[epoch:14,batch:929]:acc: 0.874664,loss:0.302465\n",
      "[epoch:14,batch:959]:acc: 0.874837,loss:0.301447\n",
      "[epoch:14,batch:989]:acc: 0.874968,loss:0.300892\n",
      "[epoch:14] :acc: 0.874862,loss:0.301289,lr:0.000000,patience:1\n",
      "[epoch:14]: val_loss:0.375886,val_acc:0.858779,\n",
      "save new model acc,now acc is  tensor(0.8588, device='cuda:0')\n",
      "Epoch 15/59\n",
      "----------\n",
      "loss has increased lr divide 10 lr now is :0.000000\n",
      "[epoch:15,batch:29]:acc: 0.853125,loss:0.336323\n",
      "[epoch:15,batch:59]:acc: 0.867188,loss:0.319406\n",
      "[epoch:15,batch:89]:acc: 0.871875,loss:0.306597\n",
      "[epoch:15,batch:119]:acc: 0.873177,loss:0.299817\n",
      "[epoch:15,batch:149]:acc: 0.873958,loss:0.300721\n",
      "[epoch:15,batch:179]:acc: 0.874479,loss:0.299974\n",
      "[epoch:15,batch:209]:acc: 0.873810,loss:0.301718\n",
      "[epoch:15,batch:239]:acc: 0.873177,loss:0.302252\n",
      "[epoch:15,batch:269]:acc: 0.871181,loss:0.303601\n",
      "[epoch:15,batch:299]:acc: 0.871042,loss:0.304598\n",
      "[epoch:15,batch:299]: val_loss:0.376636,val_acc:0.857458,val_total:4539\n",
      "[epoch:15,batch:329]:acc: 0.871117,loss:0.304035\n",
      "[epoch:15,batch:359]:acc: 0.871875,loss:0.304128\n",
      "[epoch:15,batch:389]:acc: 0.872276,loss:0.303799\n",
      "[epoch:15,batch:419]:acc: 0.873065,loss:0.304114\n",
      "[epoch:15,batch:449]:acc: 0.872986,loss:0.304843\n",
      "[epoch:15,batch:479]:acc: 0.873372,loss:0.306021\n",
      "[epoch:15,batch:509]:acc: 0.872917,loss:0.304813\n",
      "[epoch:15,batch:539]:acc: 0.873495,loss:0.303496\n",
      "[epoch:15,batch:569]:acc: 0.874232,loss:0.302460\n",
      "[epoch:15,batch:599]:acc: 0.873854,loss:0.303759\n",
      "[epoch:15,batch:599]: val_loss:0.373858,val_acc:0.856576,val_total:4539\n",
      "[epoch:15,batch:629]:acc: 0.874256,loss:0.302622\n",
      "[epoch:15,batch:659]:acc: 0.875189,loss:0.301097\n",
      "[epoch:15,batch:689]:acc: 0.874683,loss:0.302278\n",
      "[epoch:15,batch:719]:acc: 0.875347,loss:0.301085\n",
      "[epoch:15,batch:749]:acc: 0.875042,loss:0.300786\n",
      "[epoch:15,batch:779]:acc: 0.874960,loss:0.300467\n",
      "[epoch:15,batch:809]:acc: 0.874344,loss:0.302351\n",
      "[epoch:15,batch:839]:acc: 0.873996,loss:0.303473\n"
     ]
    }
   ],
   "source": [
    "#train(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reuseTrain(path,epochNum):\n",
    "    writer=SummaryWriter('log/'+date+'/ResNet/') # 创建 /log/日期/InceptionResnet的组织形式  不同模型需要修改不同名称\n",
    "    train_dataset=MyDataSet(json_Description=ANNOTATION_TRAIN,transform=preprocess_with_augmentation(normalize_torch,IMAGE_SIZE),path_pre=IMAGE_TRAIN_PRE)\n",
    "    val_dataset=MyDataSet(json_Description=ANNOTATION_VAL,transform=preprocess(normalize_torch,IMAGE_SIZE),path_pre=IMAGE_VAL_PRE)\n",
    "    train_dataLoader=DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE,num_workers=16,shuffle=True)\n",
    "    val_dataLoader=DataLoader(dataset=val_dataset,batch_size=BATCH_SIZE,num_workers=1,shuffle=False)\n",
    "    model=getmodel()\n",
    "    weight=torch.Tensor([1,3,3,3,3,4,2,3,3,3,3,3,3,3,3,3,2,3,3,3,2,3,4,2,3,1,1,3,2,2,1,3,3,1,3,2,3,3,3,3,2,1,3,2,3,3,3,1,3,3,4,4,3,2,2,3,1,1,3]).cuda()\n",
    "    criterion=nn.CrossEntropyLoss(weight=weight).cuda()\n",
    "#     lx, px = utils.predict(model,val_dataLoader)\n",
    "#     min_loss = criterion(Variable(px), Variable(lx)).item()\n",
    "    min_loss=4.1\n",
    "    print('min_loss is :%f'%(min_loss))\n",
    "    min_acc=0.80\n",
    "    patience=0\n",
    "    lr=0.0\n",
    "    momentum=0.0\n",
    "    for epoch in range(epochNum):\n",
    "        print('Epoch {}/{}'.format(epoch, epochNum - 1))\n",
    "        print('-' * 10)\n",
    "        if epoch==3:\n",
    "            lr=1e-4\n",
    "            momentum=0.9\n",
    "            print('set lr=:%f,momentum=%f'%(lr,momentum))\n",
    "        if patience==2:\n",
    "            patience=0\n",
    "            model.load_state_dict(torch.load('../model/ResNet/'+date+'_loss_best.pth')['state_dict'])\n",
    "            lr=lr/10\n",
    "            print('loss has increased lr divide 10 lr now is :%f'%(lr))\n",
    "        if epoch==0 or epoch==1 or epoch==2: #第一轮首先训练全连接层\n",
    "            lr=1e-3\n",
    "#             optimizer=torch.optim.SGD(params=model.fresh_params(),lr=lr,momentum=0.9)\n",
    "            optimizer = torch.optim.Adam(model.fresh_params(),lr = lr,amsgrad=True,weight_decay=1e-4)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(model.parameters(),lr = lr,amsgrad=True,weight_decay=1e-4)\n",
    "#             optimizer=torch.optim.SGD(params=model.parameters(),lr=lr,momentum=momentum)\n",
    "        running_loss = RunningMean()\n",
    "        running_corrects = RunningMean()\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_dataLoader):\n",
    "            model.train(True)\n",
    "            n_batchsize=inputs.size(0)\n",
    "            inputs = Variable(inputs).cuda()\n",
    "            labels = Variable(labels).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            if isinstance(outputs,tuple):\n",
    "                loss=sum((criterion(o,labels)) for o in outputs)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "            running_loss.update(loss.item(),1)\n",
    "            running_corrects.update(torch.sum(preds == labels.data).data,n_batchsize)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx%30==29:\n",
    "                print('[epoch:%d,batch:%d]:acc: %f,loss:%f'%(epoch,batch_idx,running_corrects.value,running_loss.value))\n",
    "                if batch_idx%300==299: \n",
    "                    niter = epoch * len(train_dataset)/BATCH_SIZE + batch_idx\n",
    "                    writer.add_scalar('Train/Acc',running_corrects.value,niter)\n",
    "                    writer.add_scalar('Train/Loss',running_loss.value,niter)\n",
    "                    lx,px=utils.predict(model,val_dataLoader)\n",
    "                    log_loss = criterion(Variable(px), Variable(lx))\n",
    "                    log_loss = log_loss.item()\n",
    "                    _, preds = torch.max(px, dim=1)\n",
    "                    accuracy = torch.mean((preds == lx).float())\n",
    "                    writer.add_scalar('Val/Acc',accuracy,niter)\n",
    "                    writer.add_scalar('Val/Loss',log_loss,niter)\n",
    "                    print('[epoch:%d,batch:%d]: val_loss:%f,val_acc:%f,val_total:%d'%(epoch,batch_idx,log_loss,accuracy,len(val_dataset)))\n",
    "        print('[epoch:%d] :acc: %f,loss:%f,lr:%f,patience:%d'%(epoch,running_corrects.value,running_loss.value,lr,patience))       \n",
    "        lx,px=utils.predict(model,val_dataLoader)\n",
    "        log_loss = criterion(Variable(px), Variable(lx))\n",
    "        log_loss = log_loss.item()\n",
    "        _, preds = torch.max(px, dim=1)\n",
    "        accuracy = torch.mean((preds == lx).float())\n",
    "        writer.add_scalar('Val/Acc',accuracy,(epoch+1) * len(train_dataset)/BATCH_SIZE)\n",
    "        writer.add_scalar('Val/Loss',log_loss,(epoch+1) * len(train_dataset)/BATCH_SIZE)\n",
    "        print('[epoch:%d]: val_loss:%f,val_acc:%f,'%(epoch,log_loss,accuracy))\n",
    "        if  log_loss < min_loss:\n",
    "            utils.snapshot('../model/', 'ResNet', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy })          \n",
    "            patience = 0\n",
    "            min_loss=log_loss\n",
    "            print('save new model loss,now loss is ',min_loss)\n",
    "        else:\n",
    "            patience += 1\n",
    "        if accuracy>min_acc:\n",
    "            utils.snapshot('../model/', 'ResNet', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy },key='acc') \n",
    "            min_acc=accuracy\n",
    "            print('save new model acc,now acc is ',min_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainWithRawData(path,epochNum):\n",
    "    train_dataset=MyDataSet(json_Description=ANNOTATION_TRAIN,transform=preprocess(normalize_torch,IMAGE_SIZE),path_pre=IMAGE_TRAIN_PRE)\n",
    "    val_dataset=MyDataSet(json_Description=ANNOTATION_VAL,transform=preprocess(normalize_torch,IMAGE_SIZE),path_pre=IMAGE_VAL_PRE)\n",
    "    train_dataLoader=DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE,num_workers=16,shuffle=True)\n",
    "    val_dataLoader=DataLoader(dataset=val_dataset,batch_size=BATCH_SIZE,num_workers=1,shuffle=False)\n",
    "    model=getmodel()\n",
    "    criterion=nn.CrossEntropyLoss().cuda()\n",
    "    modelParams=torch.load(path)\n",
    "    model.load_state_dict(modelParams['state_dict'])\n",
    "    min_loss=modelParams['val_loss']\n",
    "    print('min_loss is :%f'%(min_loss))\n",
    "    print('val_correct is %f'%(modelParams['val_correct']))\n",
    "    min_acc=max(modelParams['val_correct'],0.81)\n",
    "    optinizerSave=modelParams['optimizer']\n",
    "    patience=0\n",
    "    lr=1e-4\n",
    "    momentum=0.9\n",
    "    beginepoch=modelParams['epoch']\n",
    "    for epoch in range(beginepoch,epochNum):\n",
    "        print('Epoch {}/{}'.format(epoch, epochNum - 1))\n",
    "        print('-' * 10)\n",
    "        if patience==3:\n",
    "            patience=0\n",
    "            model.load_state_dict(torch.load('../model/ResNet/'+date+'_loss_best.pth')['state_dict'])\n",
    "            lr=lr/10\n",
    "            print('lr desencd')\n",
    "        if epoch==beginepoch:\n",
    "            optimizer=torch.optim.SGD(params=model.parameters(),lr=lr,momentum=momentum)\n",
    "#             optimizer.load_state_dict(optinizerSave)\n",
    "#             lr=optimizer['lr']\n",
    "#             momentum=optimizer['momentum']\n",
    "            print('begin lr is ',lr)\n",
    "            \n",
    "        else:\n",
    "            optimizer=torch.optim.SGD(params=model.parameters(),lr=lr,momentum=momentum)\n",
    "                   \n",
    "        running_loss = RunningMean()\n",
    "        running_corrects = RunningMean()\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_dataLoader):\n",
    "            model.train(True)\n",
    "            n_batchsize=inputs.size(0)\n",
    "            inputs = Variable(inputs).cuda()\n",
    "            labels = Variable(labels).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            if isinstance(outputs,tuple):\n",
    "                loss=sum((criterion(o,labels)) for o in outputs)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "            running_loss.update(loss.item(),1)\n",
    "            running_corrects.update(torch.sum(preds == labels.data).data,n_batchsize)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx%30==29:\n",
    "                print('[epoch:%d,batch:%d]:acc: %f,loss:%f'%(epoch,batch_idx,running_corrects.value,running_loss.value))\n",
    "                if batch_idx%300==299: \n",
    "                    niter = epoch * len(train_dataset)/BATCH_SIZE + batch_idx\n",
    "                    lx,px=utils.predict(model,val_dataLoader)\n",
    "                    log_loss = criterion(Variable(px), Variable(lx))\n",
    "                    log_loss = log_loss.item()\n",
    "                    _, preds = torch.max(px, dim=1)\n",
    "                    accuracy = torch.mean((preds == lx).float())\n",
    "                    print('[epoch:%d,batch:%d]: val_loss:%f,val_acc:%f,val_total:%d'%(epoch,batch_idx,log_loss,accuracy,len(val_dataset)))\n",
    "                    if  log_loss < min_loss:\n",
    "                        utils.snapshot('../model/', 'ResNet', {\n",
    "                               'epoch': epoch + 1,\n",
    "                               'state_dict': model.state_dict(),\n",
    "                               'optimizer': optimizer.state_dict(),\n",
    "                               'val_loss': log_loss,\n",
    "                               'val_correct':accuracy })          \n",
    "\n",
    "                        min_loss=log_loss\n",
    "                        print('save new model loss,now loss is ',min_loss)\n",
    "\n",
    "                    if accuracy>min_acc:\n",
    "                        utils.snapshot('../model/', 'ResNet', {\n",
    "                               'epoch': epoch + 1,\n",
    "                               'state_dict': model.state_dict(),\n",
    "                               'optimizer': optimizer.state_dict(),\n",
    "                               'val_loss': log_loss,\n",
    "                               'val_correct':accuracy },key='acc') \n",
    "                        min_acc=accuracy\n",
    "                        print('save new model acc,now acc is ',min_acc)\n",
    "        print('[epoch:%d] :acc: %f,loss:%f,lr:%f,patience:%d'%(epoch,running_corrects.value,running_loss.value,lr,patience))         \n",
    "        lx,px=utils.predict(model,val_dataLoader)\n",
    "        log_loss = criterion(Variable(px), Variable(lx))\n",
    "        log_loss = log_loss.item()\n",
    "        _, preds = torch.max(px, dim=1)\n",
    "        accuracy = torch.mean((preds == lx).float())\n",
    "        print('[epoch:%d]: val_loss:%f,val_acc:%f,'%(epoch,log_loss,accuracy))\n",
    "        if  log_loss < min_loss:\n",
    "            utils.snapshot('../model/', 'ResNet', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy })          \n",
    "            patience = 0\n",
    "            min_loss=log_loss\n",
    "            print('save new model loss,now loss is ',min_loss)\n",
    "        else:\n",
    "            patience += 1\n",
    "        if accuracy>min_acc:\n",
    "            utils.snapshot('../model/', 'ResNet', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy },key='acc') \n",
    "            min_acc=accuracy\n",
    "            print('save new model acc,now acc is ',min_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] loading model... Done\n",
      "min_loss is :0.375886\n",
      "val_correct is 0.858779\n",
      "Epoch 15/59\n",
      "----------\n",
      "begin lr is  0.0001\n",
      "[epoch:15,batch:29]:acc: 0.871875,loss:0.289995\n",
      "[epoch:15,batch:59]:acc: 0.877604,loss:0.279860\n",
      "[epoch:15,batch:89]:acc: 0.878819,loss:0.283847\n",
      "[epoch:15,batch:119]:acc: 0.884635,loss:0.274498\n",
      "[epoch:15,batch:149]:acc: 0.882708,loss:0.278735\n",
      "[epoch:15,batch:179]:acc: 0.885417,loss:0.277195\n",
      "[epoch:15,batch:209]:acc: 0.883185,loss:0.279258\n",
      "[epoch:15,batch:239]:acc: 0.882422,loss:0.278958\n",
      "[epoch:15,batch:269]:acc: 0.882292,loss:0.277777\n",
      "[epoch:15,batch:299]:acc: 0.881146,loss:0.278261\n",
      "[epoch:15,batch:299]: val_loss:0.349366,val_acc:0.862304,val_total:4539\n",
      "save new model loss,now loss is  0.3493664860725403\n",
      "save new model acc,now acc is  tensor(0.8623, device='cuda:0')\n",
      "[epoch:15,batch:329]:acc: 0.881345,loss:0.277557\n",
      "[epoch:15,batch:359]:acc: 0.881250,loss:0.276392\n",
      "[epoch:15,batch:389]:acc: 0.880128,loss:0.277674\n",
      "[epoch:15,batch:419]:acc: 0.879167,loss:0.278443\n",
      "[epoch:15,batch:449]:acc: 0.879653,loss:0.278972\n",
      "[epoch:15,batch:479]:acc: 0.879557,loss:0.278795\n",
      "[epoch:15,batch:509]:acc: 0.880515,loss:0.276803\n",
      "[epoch:15,batch:539]:acc: 0.880787,loss:0.277011\n",
      "[epoch:15,batch:569]:acc: 0.879605,loss:0.278432\n",
      "[epoch:15,batch:599]:acc: 0.879531,loss:0.278586\n",
      "[epoch:15,batch:599]: val_loss:0.345076,val_acc:0.861423,val_total:4539\n",
      "save new model loss,now loss is  0.3450759947299957\n",
      "[epoch:15,batch:629]:acc: 0.880159,loss:0.278047\n",
      "[epoch:15,batch:659]:acc: 0.880303,loss:0.278033\n",
      "[epoch:15,batch:689]:acc: 0.880072,loss:0.278325\n",
      "[epoch:15,batch:719]:acc: 0.880469,loss:0.278031\n",
      "[epoch:15,batch:749]:acc: 0.880583,loss:0.277892\n",
      "[epoch:15,batch:779]:acc: 0.880489,loss:0.279414\n",
      "[epoch:15,batch:809]:acc: 0.880864,loss:0.278721\n",
      "[epoch:15,batch:839]:acc: 0.881250,loss:0.277588\n",
      "[epoch:15,batch:869]:acc: 0.881430,loss:0.277018\n",
      "[epoch:15,batch:899]:acc: 0.880972,loss:0.278340\n",
      "[epoch:15,batch:899]: val_loss:0.342211,val_acc:0.866711,val_total:4539\n",
      "save new model loss,now loss is  0.3422110676765442\n",
      "save new model acc,now acc is  tensor(0.8667, device='cuda:0')\n",
      "[epoch:15,batch:929]:acc: 0.881519,loss:0.277779\n",
      "[epoch:15,batch:959]:acc: 0.881934,loss:0.277146\n",
      "[epoch:15,batch:989]:acc: 0.882544,loss:0.276569\n",
      "[epoch:15] :acc: 0.882681,loss:0.276158,lr:0.000100,patience:0\n",
      "[epoch:15]: val_loss:0.341908,val_acc:0.864508,\n",
      "save new model loss,now loss is  0.34190797805786133\n",
      "Epoch 16/59\n",
      "----------\n",
      "[epoch:16,batch:29]:acc: 0.888542,loss:0.265790\n",
      "[epoch:16,batch:59]:acc: 0.892188,loss:0.264403\n",
      "[epoch:16,batch:89]:acc: 0.893056,loss:0.260614\n",
      "[epoch:16,batch:119]:acc: 0.893229,loss:0.258152\n",
      "[epoch:16,batch:149]:acc: 0.892917,loss:0.259473\n",
      "[epoch:16,batch:179]:acc: 0.893056,loss:0.258153\n",
      "[epoch:16,batch:209]:acc: 0.894345,loss:0.255436\n",
      "[epoch:16,batch:239]:acc: 0.890625,loss:0.260942\n",
      "[epoch:16,batch:269]:acc: 0.890972,loss:0.260319\n",
      "[epoch:16,batch:299]:acc: 0.891146,loss:0.261160\n",
      "[epoch:16,batch:299]: val_loss:0.339603,val_acc:0.867372,val_total:4539\n",
      "save new model loss,now loss is  0.33960282802581787\n",
      "save new model acc,now acc is  tensor(0.8674, device='cuda:0')\n",
      "[epoch:16,batch:329]:acc: 0.891098,loss:0.259797\n",
      "[epoch:16,batch:359]:acc: 0.891319,loss:0.259160\n",
      "[epoch:16,batch:389]:acc: 0.890705,loss:0.259319\n",
      "[epoch:16,batch:419]:acc: 0.890179,loss:0.260966\n",
      "[epoch:16,batch:449]:acc: 0.891528,loss:0.258282\n",
      "[epoch:16,batch:479]:acc: 0.891536,loss:0.258092\n",
      "[epoch:16,batch:509]:acc: 0.891054,loss:0.258698\n",
      "[epoch:16,batch:539]:acc: 0.890972,loss:0.258603\n",
      "[epoch:16,batch:569]:acc: 0.891228,loss:0.258947\n",
      "[epoch:16,batch:599]:acc: 0.891615,loss:0.257964\n",
      "[epoch:16,batch:599]: val_loss:0.340155,val_acc:0.867372,val_total:4539\n",
      "[epoch:16,batch:629]:acc: 0.891567,loss:0.257252\n",
      "[epoch:16,batch:659]:acc: 0.891193,loss:0.257616\n",
      "[epoch:16,batch:689]:acc: 0.891078,loss:0.258077\n",
      "[epoch:16,batch:719]:acc: 0.891233,loss:0.257703\n",
      "[epoch:16,batch:749]:acc: 0.891167,loss:0.258777\n",
      "[epoch:16,batch:779]:acc: 0.890665,loss:0.259523\n",
      "[epoch:16,batch:809]:acc: 0.890008,loss:0.259829\n",
      "[epoch:16,batch:839]:acc: 0.889583,loss:0.260338\n",
      "[epoch:16,batch:869]:acc: 0.889188,loss:0.261161\n",
      "[epoch:16,batch:899]:acc: 0.889271,loss:0.261755\n",
      "[epoch:16,batch:899]: val_loss:0.341105,val_acc:0.863186,val_total:4539\n",
      "[epoch:16,batch:929]:acc: 0.889281,loss:0.262033\n",
      "[epoch:16,batch:959]:acc: 0.889030,loss:0.262487\n",
      "[epoch:16,batch:989]:acc: 0.889268,loss:0.261953\n",
      "[epoch:16] :acc: 0.889271,loss:0.261925,lr:0.000100,patience:0\n",
      "[epoch:16]: val_loss:0.339306,val_acc:0.866931,\n",
      "save new model loss,now loss is  0.3393056392669678\n",
      "Epoch 17/59\n",
      "----------\n",
      "[epoch:17,batch:29]:acc: 0.906250,loss:0.232645\n",
      "[epoch:17,batch:59]:acc: 0.896354,loss:0.244036\n",
      "[epoch:17,batch:89]:acc: 0.890278,loss:0.248578\n",
      "[epoch:17,batch:119]:acc: 0.889062,loss:0.251535\n",
      "[epoch:17,batch:149]:acc: 0.890417,loss:0.250836\n",
      "[epoch:17,batch:179]:acc: 0.891667,loss:0.250519\n",
      "[epoch:17,batch:209]:acc: 0.893006,loss:0.249056\n",
      "[epoch:17,batch:239]:acc: 0.892708,loss:0.250863\n",
      "[epoch:17,batch:269]:acc: 0.893403,loss:0.251030\n",
      "[epoch:17,batch:299]:acc: 0.892917,loss:0.250817\n",
      "[epoch:17,batch:299]: val_loss:0.339161,val_acc:0.864067,val_total:4539\n",
      "save new model loss,now loss is  0.3391609787940979\n",
      "[epoch:17,batch:329]:acc: 0.893750,loss:0.249841\n",
      "[epoch:17,batch:359]:acc: 0.890538,loss:0.256048\n",
      "[epoch:17,batch:389]:acc: 0.891186,loss:0.255593\n",
      "[epoch:17,batch:419]:acc: 0.892634,loss:0.253931\n",
      "[epoch:17,batch:449]:acc: 0.892639,loss:0.253259\n",
      "[epoch:17,batch:479]:acc: 0.892839,loss:0.251936\n",
      "[epoch:17,batch:509]:acc: 0.893015,loss:0.252612\n",
      "[epoch:17,batch:539]:acc: 0.893287,loss:0.251821\n",
      "[epoch:17,batch:569]:acc: 0.893421,loss:0.251720\n",
      "[epoch:17,batch:599]:acc: 0.893958,loss:0.251350\n",
      "[epoch:17,batch:599]: val_loss:0.338256,val_acc:0.867151,val_total:4539\n",
      "save new model loss,now loss is  0.33825555443763733\n",
      "[epoch:17,batch:629]:acc: 0.893452,loss:0.252438\n",
      "[epoch:17,batch:659]:acc: 0.893750,loss:0.252470\n",
      "[epoch:17,batch:689]:acc: 0.893931,loss:0.251845\n",
      "[epoch:17,batch:719]:acc: 0.894184,loss:0.251496\n",
      "[epoch:17,batch:749]:acc: 0.893958,loss:0.251445\n",
      "[epoch:17,batch:779]:acc: 0.893910,loss:0.252108\n",
      "[epoch:17,batch:809]:acc: 0.894676,loss:0.251116\n",
      "[epoch:17,batch:839]:acc: 0.894494,loss:0.251034\n",
      "[epoch:17,batch:869]:acc: 0.894325,loss:0.251161\n",
      "[epoch:17,batch:899]:acc: 0.894340,loss:0.251349\n",
      "[epoch:17,batch:899]: val_loss:0.337597,val_acc:0.864287,val_total:4539\n",
      "save new model loss,now loss is  0.3375966548919678\n",
      "[epoch:17,batch:929]:acc: 0.894120,loss:0.251736\n",
      "[epoch:17,batch:959]:acc: 0.893880,loss:0.251799\n",
      "[epoch:17,batch:989]:acc: 0.894318,loss:0.251211\n",
      "[epoch:17] :acc: 0.894221,loss:0.251687,lr:0.000100,patience:0\n",
      "[epoch:17]: val_loss:0.341778,val_acc:0.864948,\n",
      "Epoch 18/59\n",
      "----------\n",
      "[epoch:18,batch:29]:acc: 0.897917,loss:0.258272\n",
      "[epoch:18,batch:59]:acc: 0.898958,loss:0.243535\n",
      "[epoch:18,batch:89]:acc: 0.894097,loss:0.247651\n",
      "[epoch:18,batch:119]:acc: 0.896094,loss:0.243197\n",
      "[epoch:18,batch:149]:acc: 0.894375,loss:0.247545\n",
      "[epoch:18,batch:179]:acc: 0.896007,loss:0.246670\n",
      "[epoch:18,batch:209]:acc: 0.896726,loss:0.246412\n",
      "[epoch:18,batch:239]:acc: 0.897526,loss:0.244800\n",
      "[epoch:18,batch:269]:acc: 0.896181,loss:0.246819\n",
      "[epoch:18,batch:299]:acc: 0.894896,loss:0.247768\n",
      "[epoch:18,batch:299]: val_loss:0.337644,val_acc:0.867372,val_total:4539\n",
      "[epoch:18,batch:329]:acc: 0.893277,loss:0.249727\n",
      "[epoch:18,batch:359]:acc: 0.893403,loss:0.248824\n",
      "[epoch:18,batch:389]:acc: 0.893750,loss:0.249754\n",
      "[epoch:18,batch:419]:acc: 0.893973,loss:0.249766\n",
      "[epoch:18,batch:449]:acc: 0.894375,loss:0.249767\n",
      "[epoch:18,batch:479]:acc: 0.893815,loss:0.250446\n",
      "[epoch:18,batch:509]:acc: 0.893627,loss:0.251026\n",
      "[epoch:18,batch:539]:acc: 0.894097,loss:0.250906\n",
      "[epoch:18,batch:569]:acc: 0.894134,loss:0.250428\n",
      "[epoch:18,batch:599]:acc: 0.894479,loss:0.249973\n",
      "[epoch:18,batch:599]: val_loss:0.336227,val_acc:0.865829,val_total:4539\n",
      "save new model loss,now loss is  0.3362272381782532\n",
      "[epoch:18,batch:629]:acc: 0.894395,loss:0.249630\n",
      "[epoch:18,batch:659]:acc: 0.894744,loss:0.248689\n",
      "[epoch:18,batch:689]:acc: 0.895018,loss:0.247851\n",
      "[epoch:18,batch:719]:acc: 0.894965,loss:0.247716\n",
      "[epoch:18,batch:749]:acc: 0.895375,loss:0.247113\n",
      "[epoch:18,batch:779]:acc: 0.895873,loss:0.246158\n",
      "[epoch:18,batch:809]:acc: 0.895679,loss:0.246240\n",
      "[epoch:18,batch:839]:acc: 0.896205,loss:0.245871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:18,batch:869]:acc: 0.896659,loss:0.245269\n",
      "[epoch:18,batch:899]:acc: 0.897014,loss:0.244852\n",
      "[epoch:18,batch:899]: val_loss:0.338935,val_acc:0.868033,val_total:4539\n",
      "save new model acc,now acc is  tensor(0.8680, device='cuda:0')\n",
      "[epoch:18,batch:929]:acc: 0.896438,loss:0.245278\n",
      "[epoch:18,batch:959]:acc: 0.896517,loss:0.245471\n",
      "[epoch:18,batch:989]:acc: 0.896402,loss:0.245895\n",
      "[epoch:18] :acc: 0.896333,loss:0.246377,lr:0.000100,patience:1\n",
      "[epoch:18]: val_loss:0.338687,val_acc:0.867592,\n",
      "Epoch 19/59\n",
      "----------\n",
      "[epoch:19,batch:29]:acc: 0.898958,loss:0.237218\n",
      "[epoch:19,batch:59]:acc: 0.893750,loss:0.248203\n",
      "[epoch:19,batch:89]:acc: 0.890625,loss:0.250446\n",
      "[epoch:19,batch:119]:acc: 0.894271,loss:0.247779\n",
      "[epoch:19,batch:149]:acc: 0.893542,loss:0.251480\n",
      "[epoch:19,batch:179]:acc: 0.895139,loss:0.245447\n",
      "[epoch:19,batch:209]:acc: 0.894345,loss:0.245270\n",
      "[epoch:19,batch:239]:acc: 0.894401,loss:0.245173\n",
      "[epoch:19,batch:269]:acc: 0.893981,loss:0.246789\n",
      "[epoch:19,batch:299]:acc: 0.894583,loss:0.246021\n",
      "[epoch:19,batch:299]: val_loss:0.337068,val_acc:0.866931,val_total:4539\n",
      "[epoch:19,batch:329]:acc: 0.895644,loss:0.243908\n",
      "[epoch:19,batch:359]:acc: 0.895399,loss:0.244642\n",
      "[epoch:19,batch:389]:acc: 0.895433,loss:0.243917\n",
      "[epoch:19,batch:419]:acc: 0.895982,loss:0.242942\n",
      "[epoch:19,batch:449]:acc: 0.895278,loss:0.244189\n",
      "[epoch:19,batch:479]:acc: 0.895898,loss:0.243965\n",
      "[epoch:19,batch:509]:acc: 0.896507,loss:0.242291\n",
      "[epoch:19,batch:539]:acc: 0.896528,loss:0.241536\n",
      "[epoch:19,batch:569]:acc: 0.897204,loss:0.240752\n",
      "[epoch:19,batch:599]:acc: 0.898021,loss:0.240055\n",
      "[epoch:19,batch:599]: val_loss:0.338989,val_acc:0.868694,val_total:4539\n",
      "save new model acc,now acc is  tensor(0.8687, device='cuda:0')\n",
      "[epoch:19,batch:629]:acc: 0.898313,loss:0.239014\n",
      "[epoch:19,batch:659]:acc: 0.898722,loss:0.238741\n",
      "[epoch:19,batch:689]:acc: 0.898551,loss:0.239451\n",
      "[epoch:19,batch:719]:acc: 0.898307,loss:0.239523\n",
      "[epoch:19,batch:749]:acc: 0.899208,loss:0.238529\n",
      "[epoch:19,batch:779]:acc: 0.899159,loss:0.239054\n",
      "[epoch:19,batch:809]:acc: 0.899074,loss:0.239159\n",
      "[epoch:19,batch:839]:acc: 0.898772,loss:0.240101\n",
      "[epoch:19,batch:869]:acc: 0.898096,loss:0.240350\n",
      "[epoch:19,batch:899]:acc: 0.898333,loss:0.239460\n",
      "[epoch:19,batch:899]: val_loss:0.336200,val_acc:0.866711,val_total:4539\n",
      "save new model loss,now loss is  0.33620017766952515\n",
      "[epoch:19,batch:929]:acc: 0.898858,loss:0.238642\n",
      "[epoch:19,batch:959]:acc: 0.899056,loss:0.238422\n",
      "[epoch:19,batch:989]:acc: 0.898864,loss:0.238667\n",
      "[epoch:19] :acc: 0.898887,loss:0.238821,lr:0.000100,patience:2\n",
      "[epoch:19]: val_loss:0.337533,val_acc:0.867151,\n",
      "Epoch 20/59\n",
      "----------\n",
      "lr desencd\n",
      "[epoch:20,batch:29]:acc: 0.896875,loss:0.246454\n",
      "[epoch:20,batch:59]:acc: 0.906250,loss:0.233334\n",
      "[epoch:20,batch:89]:acc: 0.902431,loss:0.238990\n",
      "[epoch:20,batch:119]:acc: 0.905990,loss:0.230140\n",
      "[epoch:20,batch:149]:acc: 0.906458,loss:0.229022\n",
      "[epoch:20,batch:179]:acc: 0.906424,loss:0.228358\n",
      "[epoch:20,batch:209]:acc: 0.903423,loss:0.230096\n",
      "[epoch:20,batch:239]:acc: 0.901953,loss:0.230673\n",
      "[epoch:20,batch:269]:acc: 0.901042,loss:0.232369\n",
      "[epoch:20,batch:299]:acc: 0.901563,loss:0.231851\n",
      "[epoch:20,batch:299]: val_loss:0.337570,val_acc:0.868033,val_total:4539\n",
      "[epoch:20,batch:329]:acc: 0.901042,loss:0.232412\n",
      "[epoch:20,batch:359]:acc: 0.902344,loss:0.231777\n",
      "[epoch:20,batch:389]:acc: 0.902163,loss:0.230509\n",
      "[epoch:20,batch:419]:acc: 0.901190,loss:0.231965\n",
      "[epoch:20,batch:449]:acc: 0.901597,loss:0.231754\n",
      "[epoch:20,batch:479]:acc: 0.901823,loss:0.230780\n",
      "[epoch:20,batch:509]:acc: 0.902635,loss:0.230133\n",
      "[epoch:20,batch:539]:acc: 0.902315,loss:0.230279\n",
      "[epoch:20,batch:569]:acc: 0.902029,loss:0.229909\n",
      "[epoch:20,batch:599]:acc: 0.901771,loss:0.231326\n",
      "[epoch:20,batch:599]: val_loss:0.335574,val_acc:0.869795,val_total:4539\n",
      "save new model loss,now loss is  0.33557406067848206\n",
      "save new model acc,now acc is  tensor(0.8698, device='cuda:0')\n",
      "[epoch:20,batch:629]:acc: 0.902827,loss:0.230365\n",
      "[epoch:20,batch:659]:acc: 0.902699,loss:0.230812\n",
      "[epoch:20,batch:689]:acc: 0.902808,loss:0.231168\n",
      "[epoch:20,batch:719]:acc: 0.903342,loss:0.230348\n",
      "[epoch:20,batch:749]:acc: 0.903667,loss:0.229839\n",
      "[epoch:20,batch:779]:acc: 0.903566,loss:0.229883\n",
      "[epoch:20,batch:809]:acc: 0.903279,loss:0.230416\n",
      "[epoch:20,batch:839]:acc: 0.903311,loss:0.230916\n",
      "[epoch:20,batch:869]:acc: 0.903412,loss:0.230552\n",
      "[epoch:20,batch:899]:acc: 0.903403,loss:0.230077\n",
      "[epoch:20,batch:899]: val_loss:0.335322,val_acc:0.867812,val_total:4539\n",
      "save new model loss,now loss is  0.33532172441482544\n",
      "[epoch:20,batch:929]:acc: 0.903360,loss:0.230142\n",
      "[epoch:20,batch:959]:acc: 0.903483,loss:0.229314\n",
      "[epoch:20,batch:989]:acc: 0.903883,loss:0.228806\n",
      "[epoch:20] :acc: 0.903806,loss:0.229145,lr:0.000010,patience:0\n",
      "[epoch:20]: val_loss:0.338495,val_acc:0.867812,\n",
      "Epoch 21/59\n",
      "----------\n",
      "[epoch:21,batch:29]:acc: 0.892708,loss:0.233876\n",
      "[epoch:21,batch:59]:acc: 0.898958,loss:0.235801\n",
      "[epoch:21,batch:89]:acc: 0.899306,loss:0.233653\n",
      "[epoch:21,batch:119]:acc: 0.900260,loss:0.235287\n",
      "[epoch:21,batch:149]:acc: 0.901667,loss:0.234774\n",
      "[epoch:21,batch:179]:acc: 0.898958,loss:0.236643\n",
      "[epoch:21,batch:209]:acc: 0.899702,loss:0.236768\n",
      "[epoch:21,batch:239]:acc: 0.899609,loss:0.237906\n",
      "[epoch:21,batch:269]:acc: 0.900810,loss:0.237342\n",
      "[epoch:21,batch:299]:acc: 0.899375,loss:0.238003\n",
      "[epoch:21,batch:299]: val_loss:0.335688,val_acc:0.868033,val_total:4539\n",
      "[epoch:21,batch:329]:acc: 0.899242,loss:0.235652\n",
      "[epoch:21,batch:359]:acc: 0.899913,loss:0.234713\n",
      "[epoch:21,batch:389]:acc: 0.900321,loss:0.233992\n",
      "[epoch:21,batch:419]:acc: 0.900893,loss:0.232865\n",
      "[epoch:21,batch:449]:acc: 0.901111,loss:0.232424\n",
      "[epoch:21,batch:479]:acc: 0.901302,loss:0.231044\n",
      "[epoch:21,batch:509]:acc: 0.901225,loss:0.230519\n",
      "[epoch:21,batch:539]:acc: 0.901505,loss:0.229743\n",
      "[epoch:21,batch:569]:acc: 0.901700,loss:0.229187\n",
      "[epoch:21,batch:599]:acc: 0.901563,loss:0.229694\n",
      "[epoch:21,batch:599]: val_loss:0.337769,val_acc:0.869575,val_total:4539\n",
      "[epoch:21,batch:629]:acc: 0.901935,loss:0.229600\n",
      "[epoch:21,batch:659]:acc: 0.902367,loss:0.228464\n",
      "[epoch:21,batch:689]:acc: 0.902083,loss:0.229256\n",
      "[epoch:21,batch:719]:acc: 0.902127,loss:0.229389\n",
      "[epoch:21,batch:749]:acc: 0.902500,loss:0.229274\n",
      "[epoch:21,batch:779]:acc: 0.902724,loss:0.228457\n",
      "[epoch:21,batch:809]:acc: 0.903164,loss:0.227774\n",
      "[epoch:21,batch:839]:acc: 0.902976,loss:0.228509\n",
      "[epoch:21,batch:869]:acc: 0.902371,loss:0.230456\n",
      "[epoch:21,batch:899]:acc: 0.902292,loss:0.230397\n",
      "[epoch:21,batch:899]: val_loss:0.335099,val_acc:0.868473,val_total:4539\n",
      "save new model loss,now loss is  0.33509936928749084\n",
      "[epoch:21,batch:929]:acc: 0.902151,loss:0.230194\n",
      "[epoch:21,batch:959]:acc: 0.902441,loss:0.230176\n",
      "[epoch:21,batch:989]:acc: 0.902494,loss:0.230284\n",
      "[epoch:21] :acc: 0.902513,loss:0.230380,lr:0.000010,patience:1\n",
      "[epoch:21]: val_loss:0.336406,val_acc:0.870015,\n",
      "save new model acc,now acc is  tensor(0.8700, device='cuda:0')\n",
      "Epoch 22/59\n",
      "----------\n",
      "[epoch:22,batch:29]:acc: 0.894792,loss:0.242983\n",
      "[epoch:22,batch:59]:acc: 0.900521,loss:0.232432\n",
      "[epoch:22,batch:89]:acc: 0.894792,loss:0.236163\n",
      "[epoch:22,batch:119]:acc: 0.901042,loss:0.226272\n",
      "[epoch:22,batch:149]:acc: 0.903125,loss:0.227058\n",
      "[epoch:22,batch:179]:acc: 0.904340,loss:0.225630\n",
      "[epoch:22,batch:209]:acc: 0.902381,loss:0.226126\n",
      "[epoch:22,batch:239]:acc: 0.901563,loss:0.225779\n",
      "[epoch:22,batch:269]:acc: 0.903356,loss:0.224288\n",
      "[epoch:22,batch:299]:acc: 0.903854,loss:0.225455\n",
      "[epoch:22,batch:299]: val_loss:0.336436,val_acc:0.867592,val_total:4539\n",
      "[epoch:22,batch:329]:acc: 0.904735,loss:0.223887\n",
      "[epoch:22,batch:359]:acc: 0.903819,loss:0.224029\n",
      "[epoch:22,batch:389]:acc: 0.903446,loss:0.224932\n",
      "[epoch:22,batch:419]:acc: 0.902827,loss:0.225296\n",
      "[epoch:22,batch:449]:acc: 0.903542,loss:0.225033\n",
      "[epoch:22,batch:479]:acc: 0.904427,loss:0.224530\n",
      "[epoch:22,batch:509]:acc: 0.903983,loss:0.225893\n",
      "[epoch:22,batch:539]:acc: 0.904977,loss:0.224914\n",
      "[epoch:22,batch:569]:acc: 0.905044,loss:0.225062\n",
      "[epoch:22,batch:599]:acc: 0.904271,loss:0.225709\n",
      "[epoch:22,batch:599]: val_loss:0.336182,val_acc:0.869134,val_total:4539\n",
      "[epoch:22,batch:629]:acc: 0.904266,loss:0.225776\n",
      "[epoch:22,batch:659]:acc: 0.904403,loss:0.225370\n",
      "[epoch:22,batch:689]:acc: 0.903533,loss:0.227203\n",
      "[epoch:22,batch:719]:acc: 0.903385,loss:0.227098\n",
      "[epoch:22,batch:749]:acc: 0.903625,loss:0.227227\n",
      "[epoch:22,batch:779]:acc: 0.903806,loss:0.227076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:22,batch:809]:acc: 0.904128,loss:0.227238\n",
      "[epoch:22,batch:839]:acc: 0.904129,loss:0.227655\n",
      "[epoch:22,batch:869]:acc: 0.904526,loss:0.227169\n",
      "[epoch:22,batch:899]:acc: 0.904306,loss:0.227561\n",
      "[epoch:22,batch:899]: val_loss:0.334947,val_acc:0.869575,val_total:4539\n",
      "save new model loss,now loss is  0.33494654297828674\n",
      "[epoch:22,batch:929]:acc: 0.904301,loss:0.227937\n",
      "[epoch:22,batch:959]:acc: 0.904004,loss:0.228362\n",
      "[epoch:22,batch:989]:acc: 0.903788,loss:0.228709\n",
      "[epoch:22] :acc: 0.903806,loss:0.228772,lr:0.000010,patience:2\n",
      "[epoch:22]: val_loss:0.336432,val_acc:0.868473,\n",
      "Epoch 23/59\n",
      "----------\n",
      "lr desencd\n",
      "[epoch:23,batch:29]:acc: 0.892708,loss:0.244052\n",
      "[epoch:23,batch:59]:acc: 0.892708,loss:0.246488\n",
      "[epoch:23,batch:89]:acc: 0.897917,loss:0.239494\n",
      "[epoch:23,batch:119]:acc: 0.904687,loss:0.234706\n",
      "[epoch:23,batch:149]:acc: 0.904583,loss:0.234311\n",
      "[epoch:23,batch:179]:acc: 0.906076,loss:0.232205\n",
      "[epoch:23,batch:209]:acc: 0.905655,loss:0.228710\n",
      "[epoch:23,batch:239]:acc: 0.904687,loss:0.228400\n",
      "[epoch:23,batch:269]:acc: 0.907523,loss:0.223828\n",
      "[epoch:23,batch:299]:acc: 0.906458,loss:0.224204\n",
      "[epoch:23,batch:299]: val_loss:0.337146,val_acc:0.868253,val_total:4539\n",
      "[epoch:23,batch:329]:acc: 0.905966,loss:0.224703\n",
      "[epoch:23,batch:359]:acc: 0.905729,loss:0.225962\n",
      "[epoch:23,batch:389]:acc: 0.904888,loss:0.225310\n",
      "[epoch:23,batch:419]:acc: 0.904539,loss:0.224866\n",
      "[epoch:23,batch:449]:acc: 0.904236,loss:0.226041\n",
      "[epoch:23,batch:479]:acc: 0.904687,loss:0.226014\n",
      "[epoch:23,batch:509]:acc: 0.903922,loss:0.227323\n",
      "[epoch:23,batch:539]:acc: 0.904282,loss:0.227090\n",
      "[epoch:23,batch:569]:acc: 0.903728,loss:0.227882\n",
      "[epoch:23,batch:599]:acc: 0.903698,loss:0.228147\n",
      "[epoch:23,batch:599]: val_loss:0.337552,val_acc:0.867592,val_total:4539\n",
      "[epoch:23,batch:629]:acc: 0.904167,loss:0.227042\n",
      "[epoch:23,batch:659]:acc: 0.904498,loss:0.226652\n",
      "[epoch:23,batch:689]:acc: 0.903986,loss:0.227009\n",
      "[epoch:23,batch:719]:acc: 0.904384,loss:0.226499\n",
      "[epoch:23,batch:749]:acc: 0.903917,loss:0.227244\n",
      "[epoch:23,batch:779]:acc: 0.903245,loss:0.227737\n",
      "[epoch:23,batch:809]:acc: 0.903627,loss:0.227111\n",
      "[epoch:23,batch:839]:acc: 0.904018,loss:0.227270\n",
      "[epoch:23,batch:869]:acc: 0.903772,loss:0.227718\n",
      "[epoch:23,batch:899]:acc: 0.903854,loss:0.227051\n",
      "[epoch:23,batch:899]: val_loss:0.335883,val_acc:0.868253,val_total:4539\n",
      "[epoch:23,batch:929]:acc: 0.904167,loss:0.226415\n",
      "[epoch:23,batch:959]:acc: 0.904329,loss:0.226824\n",
      "[epoch:23,batch:989]:acc: 0.903725,loss:0.227268\n",
      "[epoch:23] :acc: 0.903774,loss:0.227101,lr:0.000001,patience:0\n",
      "[epoch:23]: val_loss:0.336019,val_acc:0.866711,\n",
      "Epoch 24/59\n",
      "----------\n",
      "[epoch:24,batch:29]:acc: 0.896875,loss:0.238773\n",
      "[epoch:24,batch:59]:acc: 0.903646,loss:0.223808\n",
      "[epoch:24,batch:89]:acc: 0.909375,loss:0.216233\n",
      "[epoch:24,batch:119]:acc: 0.906250,loss:0.222089\n",
      "[epoch:24,batch:149]:acc: 0.905000,loss:0.224656\n",
      "[epoch:24,batch:179]:acc: 0.906424,loss:0.223778\n",
      "[epoch:24,batch:209]:acc: 0.906101,loss:0.224081\n",
      "[epoch:24,batch:239]:acc: 0.906510,loss:0.222409\n",
      "[epoch:24,batch:269]:acc: 0.905093,loss:0.224712\n",
      "[epoch:24,batch:299]:acc: 0.904583,loss:0.227277\n",
      "[epoch:24,batch:299]: val_loss:0.334152,val_acc:0.868914,val_total:4539\n",
      "save new model loss,now loss is  0.3341515362262726\n",
      "[epoch:24,batch:329]:acc: 0.905019,loss:0.226017\n",
      "[epoch:24,batch:359]:acc: 0.905903,loss:0.223745\n",
      "[epoch:24,batch:389]:acc: 0.906731,loss:0.222158\n",
      "[epoch:24,batch:419]:acc: 0.906101,loss:0.223201\n",
      "[epoch:24,batch:449]:acc: 0.904861,loss:0.225376\n",
      "[epoch:24,batch:479]:acc: 0.905143,loss:0.225911\n",
      "[epoch:24,batch:509]:acc: 0.904167,loss:0.227674\n",
      "[epoch:24,batch:539]:acc: 0.904340,loss:0.228111\n",
      "[epoch:24,batch:569]:acc: 0.903783,loss:0.229387\n",
      "[epoch:24,batch:599]:acc: 0.903854,loss:0.228853\n",
      "[epoch:24,batch:599]: val_loss:0.338480,val_acc:0.868033,val_total:4539\n",
      "[epoch:24,batch:629]:acc: 0.903522,loss:0.228980\n",
      "[epoch:24,batch:659]:acc: 0.903456,loss:0.229089\n",
      "[epoch:24,batch:689]:acc: 0.903080,loss:0.229098\n",
      "[epoch:24,batch:719]:acc: 0.902995,loss:0.229496\n",
      "[epoch:24,batch:749]:acc: 0.902917,loss:0.229207\n",
      "[epoch:24,batch:779]:acc: 0.902724,loss:0.229853\n",
      "[epoch:24,batch:809]:acc: 0.902739,loss:0.229848\n",
      "[epoch:24,batch:839]:acc: 0.902753,loss:0.229766\n",
      "[epoch:24,batch:869]:acc: 0.902514,loss:0.230172\n",
      "[epoch:24,batch:899]:acc: 0.903264,loss:0.229356\n",
      "[epoch:24,batch:899]: val_loss:0.335801,val_acc:0.866711,val_total:4539\n",
      "[epoch:24,batch:929]:acc: 0.903327,loss:0.228926\n",
      "[epoch:24,batch:959]:acc: 0.903451,loss:0.229204\n",
      "[epoch:24,batch:989]:acc: 0.903598,loss:0.229491\n",
      "[epoch:24] :acc: 0.903585,loss:0.229866,lr:0.000001,patience:1\n",
      "[epoch:24]: val_loss:0.341029,val_acc:0.868253,\n",
      "Epoch 25/59\n",
      "----------\n",
      "[epoch:25,batch:29]:acc: 0.914583,loss:0.205712\n",
      "[epoch:25,batch:59]:acc: 0.909375,loss:0.221459\n",
      "[epoch:25,batch:89]:acc: 0.905208,loss:0.222826\n",
      "[epoch:25,batch:119]:acc: 0.905729,loss:0.223255\n",
      "[epoch:25,batch:149]:acc: 0.906042,loss:0.225473\n",
      "[epoch:25,batch:179]:acc: 0.905382,loss:0.227228\n",
      "[epoch:25,batch:209]:acc: 0.906399,loss:0.225272\n",
      "[epoch:25,batch:239]:acc: 0.906120,loss:0.226246\n",
      "[epoch:25,batch:269]:acc: 0.908449,loss:0.222307\n",
      "[epoch:25,batch:299]:acc: 0.908333,loss:0.221687\n",
      "[epoch:25,batch:299]: val_loss:0.336072,val_acc:0.870897,val_total:4539\n",
      "save new model acc,now acc is  tensor(0.8709, device='cuda:0')\n",
      "[epoch:25,batch:329]:acc: 0.907955,loss:0.222362\n",
      "[epoch:25,batch:359]:acc: 0.908333,loss:0.221713\n",
      "[epoch:25,batch:389]:acc: 0.907051,loss:0.223842\n",
      "[epoch:25,batch:419]:acc: 0.905878,loss:0.224793\n",
      "[epoch:25,batch:449]:acc: 0.905972,loss:0.225601\n",
      "[epoch:25,batch:479]:acc: 0.906250,loss:0.225131\n",
      "[epoch:25,batch:509]:acc: 0.906679,loss:0.225561\n",
      "[epoch:25,batch:539]:acc: 0.906076,loss:0.228339\n",
      "[epoch:25,batch:569]:acc: 0.906524,loss:0.226781\n",
      "[epoch:25,batch:599]:acc: 0.906094,loss:0.227753\n",
      "[epoch:25,batch:599]: val_loss:0.335917,val_acc:0.866490,val_total:4539\n",
      "[epoch:25,batch:629]:acc: 0.905754,loss:0.227218\n",
      "[epoch:25,batch:659]:acc: 0.905303,loss:0.228561\n",
      "[epoch:25,batch:689]:acc: 0.905933,loss:0.227865\n",
      "[epoch:25,batch:719]:acc: 0.906033,loss:0.227126\n",
      "[epoch:25,batch:749]:acc: 0.906417,loss:0.226965\n",
      "[epoch:25,batch:779]:acc: 0.905569,loss:0.228903\n",
      "[epoch:25,batch:809]:acc: 0.904360,loss:0.230355\n",
      "[epoch:25,batch:839]:acc: 0.904501,loss:0.230398\n",
      "[epoch:25,batch:869]:acc: 0.904131,loss:0.230892\n",
      "[epoch:25,batch:899]:acc: 0.904340,loss:0.230726\n",
      "[epoch:25,batch:899]: val_loss:0.335627,val_acc:0.870015,val_total:4539\n",
      "[epoch:25,batch:929]:acc: 0.904032,loss:0.231176\n",
      "[epoch:25,batch:959]:acc: 0.904297,loss:0.230481\n",
      "[epoch:25,batch:989]:acc: 0.904640,loss:0.229737\n",
      "[epoch:25] :acc: 0.904594,loss:0.230245,lr:0.000001,patience:2\n",
      "[epoch:25]: val_loss:0.338487,val_acc:0.866490,\n",
      "Epoch 26/59\n",
      "----------\n",
      "lr desencd\n",
      "[epoch:26,batch:29]:acc: 0.898958,loss:0.220775\n",
      "[epoch:26,batch:59]:acc: 0.904687,loss:0.223939\n",
      "[epoch:26,batch:89]:acc: 0.903125,loss:0.224984\n",
      "[epoch:26,batch:119]:acc: 0.901563,loss:0.224974\n",
      "[epoch:26,batch:149]:acc: 0.900208,loss:0.227282\n",
      "[epoch:26,batch:179]:acc: 0.899479,loss:0.229577\n",
      "[epoch:26,batch:209]:acc: 0.899554,loss:0.230893\n",
      "[epoch:26,batch:239]:acc: 0.900651,loss:0.229301\n",
      "[epoch:26,batch:269]:acc: 0.901157,loss:0.228357\n",
      "[epoch:26,batch:299]:acc: 0.901667,loss:0.228467\n",
      "[epoch:26,batch:299]: val_loss:0.336106,val_acc:0.868253,val_total:4539\n",
      "[epoch:26,batch:329]:acc: 0.901894,loss:0.227593\n",
      "[epoch:26,batch:359]:acc: 0.902431,loss:0.226941\n",
      "[epoch:26,batch:389]:acc: 0.902244,loss:0.227382\n",
      "[epoch:26,batch:419]:acc: 0.903943,loss:0.225308\n",
      "[epoch:26,batch:449]:acc: 0.903472,loss:0.226658\n",
      "[epoch:26,batch:479]:acc: 0.903385,loss:0.227925\n",
      "[epoch:26,batch:509]:acc: 0.904044,loss:0.226667\n",
      "[epoch:26,batch:539]:acc: 0.903241,loss:0.227465\n",
      "[epoch:26,batch:569]:acc: 0.903728,loss:0.227193\n",
      "[epoch:26,batch:599]:acc: 0.902865,loss:0.228289\n",
      "[epoch:26,batch:599]: val_loss:0.335877,val_acc:0.869354,val_total:4539\n",
      "[epoch:26,batch:629]:acc: 0.903770,loss:0.227607\n",
      "[epoch:26,batch:659]:acc: 0.903930,loss:0.226910\n",
      "[epoch:26,batch:689]:acc: 0.904257,loss:0.226375\n",
      "[epoch:26,batch:719]:acc: 0.903993,loss:0.227114\n",
      "[epoch:26,batch:749]:acc: 0.904167,loss:0.227003\n",
      "[epoch:26,batch:779]:acc: 0.904247,loss:0.227198\n",
      "[epoch:26,batch:809]:acc: 0.904128,loss:0.227106\n",
      "[epoch:26,batch:839]:acc: 0.903423,loss:0.228380\n",
      "[epoch:26,batch:869]:acc: 0.903915,loss:0.227528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:26,batch:899]:acc: 0.903993,loss:0.227690\n",
      "[epoch:26,batch:899]: val_loss:0.335389,val_acc:0.869354,val_total:4539\n",
      "[epoch:26,batch:929]:acc: 0.903831,loss:0.228141\n",
      "[epoch:26,batch:959]:acc: 0.903581,loss:0.228291\n",
      "[epoch:26,batch:989]:acc: 0.904009,loss:0.227532\n",
      "[epoch:26] :acc: 0.903995,loss:0.227785,lr:0.000000,patience:0\n",
      "[epoch:26]: val_loss:0.341298,val_acc:0.868694,\n",
      "Epoch 27/59\n",
      "----------\n",
      "[epoch:27,batch:29]:acc: 0.908333,loss:0.224235\n",
      "[epoch:27,batch:59]:acc: 0.901042,loss:0.232707\n",
      "[epoch:27,batch:89]:acc: 0.901389,loss:0.230769\n",
      "[epoch:27,batch:119]:acc: 0.900521,loss:0.232782\n",
      "[epoch:27,batch:149]:acc: 0.898542,loss:0.234477\n",
      "[epoch:27,batch:179]:acc: 0.897569,loss:0.235424\n",
      "[epoch:27,batch:209]:acc: 0.901042,loss:0.230325\n",
      "[epoch:27,batch:239]:acc: 0.901563,loss:0.230396\n",
      "[epoch:27,batch:269]:acc: 0.900463,loss:0.231558\n",
      "[epoch:27,batch:299]:acc: 0.900937,loss:0.232109\n",
      "[epoch:27,batch:299]: val_loss:0.335757,val_acc:0.867592,val_total:4539\n",
      "[epoch:27,batch:329]:acc: 0.900663,loss:0.231795\n",
      "[epoch:27,batch:359]:acc: 0.901563,loss:0.230659\n",
      "[epoch:27,batch:389]:acc: 0.901763,loss:0.230324\n",
      "[epoch:27,batch:419]:acc: 0.902455,loss:0.228496\n",
      "[epoch:27,batch:449]:acc: 0.902778,loss:0.228031\n",
      "[epoch:27,batch:479]:acc: 0.902409,loss:0.229162\n",
      "[epoch:27,batch:509]:acc: 0.902328,loss:0.229946\n",
      "[epoch:27,batch:539]:acc: 0.902836,loss:0.230350\n",
      "[epoch:27,batch:569]:acc: 0.902961,loss:0.230449\n",
      "[epoch:27,batch:599]:acc: 0.902500,loss:0.232381\n",
      "[epoch:27,batch:599]: val_loss:0.334707,val_acc:0.869354,val_total:4539\n",
      "[epoch:27,batch:629]:acc: 0.902083,loss:0.232631\n",
      "[epoch:27,batch:659]:acc: 0.902557,loss:0.231735\n",
      "[epoch:27,batch:689]:acc: 0.902582,loss:0.231403\n",
      "[epoch:27,batch:719]:acc: 0.902648,loss:0.231157\n",
      "[epoch:27,batch:749]:acc: 0.902375,loss:0.231618\n",
      "[epoch:27,batch:779]:acc: 0.902524,loss:0.231285\n",
      "[epoch:27,batch:809]:acc: 0.902932,loss:0.230706\n",
      "[epoch:27,batch:839]:acc: 0.903720,loss:0.229616\n",
      "[epoch:27,batch:869]:acc: 0.903664,loss:0.230089\n",
      "[epoch:27,batch:899]:acc: 0.903542,loss:0.230003\n",
      "[epoch:27,batch:899]: val_loss:0.335564,val_acc:0.869795,val_total:4539\n",
      "[epoch:27,batch:929]:acc: 0.903831,loss:0.229466\n",
      "[epoch:27,batch:959]:acc: 0.903906,loss:0.229270\n",
      "[epoch:27,batch:989]:acc: 0.904009,loss:0.228953\n",
      "[epoch:27] :acc: 0.903900,loss:0.229151,lr:0.000000,patience:1\n",
      "[epoch:27]: val_loss:0.335654,val_acc:0.868253,\n",
      "Epoch 28/59\n",
      "----------\n",
      "[epoch:28,batch:29]:acc: 0.898958,loss:0.230629\n",
      "[epoch:28,batch:59]:acc: 0.902604,loss:0.232500\n",
      "[epoch:28,batch:89]:acc: 0.905208,loss:0.226316\n",
      "[epoch:28,batch:119]:acc: 0.905469,loss:0.229567\n",
      "[epoch:28,batch:149]:acc: 0.906042,loss:0.230082\n",
      "[epoch:28,batch:179]:acc: 0.905382,loss:0.228453\n",
      "[epoch:28,batch:209]:acc: 0.903869,loss:0.230402\n",
      "[epoch:28,batch:239]:acc: 0.904687,loss:0.230081\n",
      "[epoch:28,batch:269]:acc: 0.906250,loss:0.228331\n",
      "[epoch:28,batch:299]:acc: 0.906250,loss:0.229598\n",
      "[epoch:28,batch:299]: val_loss:0.336754,val_acc:0.869134,val_total:4539\n",
      "[epoch:28,batch:329]:acc: 0.907197,loss:0.226779\n",
      "[epoch:28,batch:359]:acc: 0.907378,loss:0.226282\n",
      "[epoch:28,batch:389]:acc: 0.907933,loss:0.224956\n",
      "[epoch:28,batch:419]:acc: 0.907143,loss:0.226683\n",
      "[epoch:28,batch:449]:acc: 0.907778,loss:0.224994\n",
      "[epoch:28,batch:479]:acc: 0.907357,loss:0.226452\n",
      "[epoch:28,batch:509]:acc: 0.907414,loss:0.226729\n",
      "[epoch:28,batch:539]:acc: 0.907176,loss:0.227070\n",
      "[epoch:28,batch:569]:acc: 0.906360,loss:0.228119\n",
      "[epoch:28,batch:599]:acc: 0.906458,loss:0.227846\n",
      "[epoch:28,batch:599]: val_loss:0.338217,val_acc:0.870456,val_total:4539\n",
      "[epoch:28,batch:629]:acc: 0.906151,loss:0.227903\n",
      "[epoch:28,batch:659]:acc: 0.905445,loss:0.228550\n",
      "[epoch:28,batch:689]:acc: 0.904257,loss:0.230450\n",
      "[epoch:28,batch:719]:acc: 0.904080,loss:0.230257\n"
     ]
    }
   ],
   "source": [
    "TrainWithRawData('../model/ResNet/2018-10-30_acc_best.pth',60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Conda_Env_Pytorch]",
   "language": "python",
   "name": "conda-env-Conda_Env_Pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
