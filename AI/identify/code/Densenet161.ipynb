{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CropModels\n",
    "from CropDataset import MyDataSet,normalize_torch,normalize_05,normalize_dataset,preprocess,preprocess_hflip,preprocess_with_augmentation\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from tensorboardX import SummaryWriter\n",
    "import datetime\n",
    "import os\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from utils import RunningMean\n",
    "import utils\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "NB_CLASS=59\n",
    "SEED=888\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=32\n",
    "IMAGE_SIZE=224    # 不同模型修改不同的Size\n",
    "IMAGE_TRAIN_PRE='../data/AgriculturalDisease_trainingset/images/'\n",
    "ANNOTATION_TRAIN='../data/AgriculturalDisease_trainingset/AgriculturalDisease_train_annotations_deleteNoise.json' #是否需要剔除两类异常类\n",
    "IMAGE_VAL_PRE='../data/AgriculturalDisease_validationset/images/'\n",
    "ANNOTATION_VAL='../data/AgriculturalDisease_validationset/AgriculturalDisease_validation_annotations_deleteNoise.json' #是否需要剔除两类异常类\n",
    "date=str(datetime.date.today())\n",
    "with open(ANNOTATION_TRAIN) as datafile1:\n",
    "    trainDataFram=pd.read_json(datafile1,orient='records')\n",
    "with open(ANNOTATION_VAL) as datafile2: #first check if it's a valid json file or not\n",
    "    validateDataFram =pd.read_json(datafile2,orient='records')    \n",
    "def getmodel():\n",
    "    print('[+] loading model... ', end='', flush=True)\n",
    "    model=CropModels.densenet161_finetune(NB_CLASS) # 需要修改为使用的model\n",
    "    model.cuda()\n",
    "    print('Done')\n",
    "    return model\n",
    "def train(epochNum):\n",
    "    writer=SummaryWriter('log/'+date+'/DesNet161/') # 创建 /log/日期/InceptionResnet的组织形式  不同模型需要修改不同名称\n",
    "    train_dataset=MyDataSet(json_Description=ANNOTATION_TRAIN,transform=preprocess_with_augmentation(normalize_torch,IMAGE_SIZE),path_pre=IMAGE_TRAIN_PRE)\n",
    "    val_dataset=MyDataSet(json_Description=ANNOTATION_VAL,transform=preprocess(normalize_torch,IMAGE_SIZE),path_pre=IMAGE_VAL_PRE)\n",
    "    train_dataLoader=DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE,num_workers=16,shuffle=True)\n",
    "    val_dataLoader=DataLoader(dataset=val_dataset,batch_size=BATCH_SIZE,num_workers=1,shuffle=False)\n",
    "    model=getmodel()\n",
    "    weight=torch.Tensor([1,3,3,3,3,4,2,3,3,3,3,3,3,3,3,3,2,3,3,3,2,3,4,2,3,1,1,3,2,2,1,3,3,1,3,2,3,3,3,3,2,1,3,2,3,3,3,1,3,3,4,4,3,2,2,3,1,1,3]).cuda()\n",
    "    criterion=nn.CrossEntropyLoss(weight=weight).cuda()\n",
    "#     lx, px = utils.predict(model,val_dataLoader)\n",
    "#     min_loss = criterion(Variable(px), Variable(lx)).item()\n",
    "    min_loss=4.1\n",
    "    print('min_loss is :%f'%(min_loss))\n",
    "    min_acc=0.80\n",
    "    patience=0\n",
    "    lr=0.0\n",
    "    momentum=0.9\n",
    "    for epoch in range(epochNum):\n",
    "        print('Epoch {}/{}'.format(epoch, epochNum - 1))\n",
    "        print('-' * 10)\n",
    "        if epoch==3 or epoch==4 or epoch==5:\n",
    "            lr=0.00003\n",
    "            momentum=0.95\n",
    "            print('set lr=:%f,momentum=%f'%(lr,momentum))\n",
    "        if epoch==6:\n",
    "            lr=1e-4\n",
    "            momentum=0.9\n",
    "            print('set lr=:%f,momentum=%f'%(lr,momentum))        \n",
    "        if patience==2:\n",
    "            patience=0\n",
    "            model.load_state_dict(torch.load('../model/DesNet161/'+date+'_loss_best.pth')['state_dict'])\n",
    "            lr=lr/5\n",
    "            print('loss has increased lr divide 5 lr now is :%f'%(lr))\n",
    "        if epoch==0 or epoch==1 or epoch==2: #第一轮首先训练全连接层\n",
    "            lr=1e-3\n",
    "            optimizer = torch.optim.Adam(model.fresh_params(),lr = lr,amsgrad=True,weight_decay=1e-4)\n",
    "#             optimizer=torch.optim.SGD(params=model.fresh_params(),lr=lr,momentum=0.9)\n",
    "\n",
    "            #optimizer = torch.optim.Adam(model.parameters(),lr = lr,amsgrad=True,weight_decay=1e-4)\n",
    "            #optimizer=torch.optim.SGD(params=model.fresh_params(),lr=lr,momentum=0.9)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(model.parameters(),lr = lr,amsgrad=True,weight_decay=1e-4)\n",
    "            \n",
    "             #optimizer=torch.optim.SGD(params=model.parameters(),lr=lr,momentum=momentum)\n",
    "#             optimizer=torch.optim.SGD(params=model.parameters(),lr=lr,momentum=momentum)\n",
    "        running_loss = RunningMean()\n",
    "        running_corrects = RunningMean()\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_dataLoader):\n",
    "            model.train(True)\n",
    "            n_batchsize=inputs.size(0)\n",
    "            inputs = Variable(inputs).cuda()\n",
    "            labels = Variable(labels).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            if isinstance(outputs,tuple):\n",
    "                loss=sum((criterion(o,labels)) for o in outputs)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "            running_loss.update(loss.item(),1)\n",
    "            running_corrects.update(torch.sum(preds == labels.data).data,n_batchsize)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx%30==29:\n",
    "                print('[epoch:%d,batch:%d]:acc: %f,loss:%f'%(epoch,batch_idx,running_corrects.value,running_loss.value))\n",
    "                if batch_idx%300==299: \n",
    "                    niter = epoch * len(train_dataset)/BATCH_SIZE + batch_idx\n",
    "                    writer.add_scalar('Train/Acc',running_corrects.value,niter)\n",
    "                    writer.add_scalar('Train/Loss',running_loss.value,niter)\n",
    "                    lx,px=utils.predict(model,val_dataLoader)\n",
    "                    log_loss = criterion(Variable(px), Variable(lx))\n",
    "                    log_loss = log_loss.item()\n",
    "                    _, preds = torch.max(px, dim=1)\n",
    "                    accuracy = torch.mean((preds == lx).float())\n",
    "                    writer.add_scalar('Val/Acc',accuracy,niter)\n",
    "                    writer.add_scalar('Val/Loss',log_loss,niter)\n",
    "                    print('[epoch:%d,batch:%d]: val_loss:%f,val_acc:%f,val_total:%d'%(epoch,batch_idx,log_loss,accuracy,len(val_dataset)))\n",
    "        print('[epoch:%d] :acc: %f,loss:%f,lr:%f,patience:%d'%(epoch,running_corrects.value,running_loss.value,lr,patience))       \n",
    "        lx,px=utils.predict(model,val_dataLoader)\n",
    "        log_loss = criterion(Variable(px), Variable(lx))\n",
    "        log_loss = log_loss.item()\n",
    "        _, preds = torch.max(px, dim=1)\n",
    "        accuracy = torch.mean((preds == lx).float())\n",
    "        writer.add_scalar('Val/Acc',accuracy,(epoch+1) * len(train_dataset)/BATCH_SIZE)\n",
    "        writer.add_scalar('Val/Loss',log_loss,(epoch+1) * len(train_dataset)/BATCH_SIZE)\n",
    "        print('[epoch:%d]: val_loss:%f,val_acc:%f,'%(epoch,log_loss,accuracy))\n",
    "        if  log_loss < min_loss:\n",
    "            utils.snapshot('../model/', 'DesNet161', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy })          \n",
    "            patience = 0\n",
    "            min_loss=log_loss\n",
    "            print('save new model loss,now loss is ',min_loss)\n",
    "        else:\n",
    "            patience += 1\n",
    "        if accuracy>min_acc:\n",
    "            utils.snapshot('../model/', 'DesNet161', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy },key='acc') \n",
    "            min_acc=accuracy\n",
    "            print('save new model acc,now acc is ',min_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] loading model... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lujunfeng/anaconda3/envs/Conda_Env_Pytorch/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "min_loss is :4.100000\n",
      "Epoch 0/59\n",
      "----------\n",
      "[epoch:0,batch:29]:acc: 0.165625,loss:3.837802\n",
      "[epoch:0,batch:59]:acc: 0.297396,loss:3.391238\n",
      "[epoch:0,batch:89]:acc: 0.367708,loss:3.063303\n",
      "[epoch:0,batch:119]:acc: 0.422396,loss:2.793565\n",
      "[epoch:0,batch:149]:acc: 0.466667,loss:2.584416\n",
      "[epoch:0,batch:179]:acc: 0.496007,loss:2.429176\n",
      "[epoch:0,batch:209]:acc: 0.520387,loss:2.292664\n",
      "[epoch:0,batch:239]:acc: 0.537370,loss:2.182895\n",
      "[epoch:0,batch:269]:acc: 0.555093,loss:2.083971\n",
      "[epoch:0,batch:299]:acc: 0.570937,loss:1.995705\n",
      "[epoch:0,batch:299]: val_loss:1.077655,val_acc:0.729676,val_total:4539\n",
      "[epoch:0,batch:329]:acc: 0.584659,loss:1.920064\n",
      "[epoch:0,batch:359]:acc: 0.593056,loss:1.857804\n",
      "[epoch:0,batch:389]:acc: 0.602644,loss:1.800236\n",
      "[epoch:0,batch:419]:acc: 0.611607,loss:1.747029\n",
      "[epoch:0,batch:449]:acc: 0.621250,loss:1.695188\n",
      "[epoch:0,batch:479]:acc: 0.626758,loss:1.654603\n",
      "[epoch:0,batch:509]:acc: 0.631311,loss:1.616521\n",
      "[epoch:0,batch:539]:acc: 0.636574,loss:1.581834\n",
      "[epoch:0,batch:569]:acc: 0.641393,loss:1.550063\n",
      "[epoch:0,batch:599]:acc: 0.645833,loss:1.521199\n",
      "[epoch:0,batch:599]: val_loss:0.823267,val_acc:0.757436,val_total:4539\n",
      "[epoch:0,batch:629]:acc: 0.649355,loss:1.492640\n",
      "[epoch:0,batch:659]:acc: 0.653220,loss:1.465760\n",
      "[epoch:0,batch:689]:acc: 0.656793,loss:1.440141\n",
      "[epoch:0,batch:719]:acc: 0.660851,loss:1.414003\n",
      "[epoch:0,batch:749]:acc: 0.663958,loss:1.391641\n",
      "[epoch:0,batch:779]:acc: 0.667147,loss:1.371204\n",
      "[epoch:0,batch:809]:acc: 0.669560,loss:1.351866\n",
      "[epoch:0,batch:839]:acc: 0.671763,loss:1.332486\n",
      "[epoch:0,batch:869]:acc: 0.675539,loss:1.313326\n",
      "[epoch:0,batch:899]:acc: 0.677917,loss:1.295436\n",
      "[epoch:0,batch:899]: val_loss:0.736800,val_acc:0.763604,val_total:4539\n",
      "[epoch:0,batch:929]:acc: 0.680208,loss:1.280364\n",
      "[epoch:0,batch:959]:acc: 0.681868,loss:1.267191\n",
      "[epoch:0,batch:989]:acc: 0.683902,loss:1.251662\n",
      "[epoch:0] :acc: 0.684081,loss:1.250481,lr:0.001000,patience:0\n",
      "[epoch:0]: val_loss:0.723103,val_acc:0.772417,\n",
      "save new model loss,now loss is  0.7231033444404602\n",
      "Epoch 1/59\n",
      "----------\n",
      "[epoch:1,batch:29]:acc: 0.733333,loss:0.888176\n",
      "[epoch:1,batch:59]:acc: 0.750000,loss:0.830758\n",
      "[epoch:1,batch:89]:acc: 0.748958,loss:0.800732\n",
      "[epoch:1,batch:119]:acc: 0.751302,loss:0.788144\n",
      "[epoch:1,batch:149]:acc: 0.751667,loss:0.779658\n",
      "[epoch:1,batch:179]:acc: 0.752431,loss:0.771877\n",
      "[epoch:1,batch:209]:acc: 0.754167,loss:0.769190\n",
      "[epoch:1,batch:239]:acc: 0.754687,loss:0.758120\n",
      "[epoch:1,batch:269]:acc: 0.756944,loss:0.755111\n",
      "[epoch:1,batch:299]:acc: 0.758854,loss:0.748489\n",
      "[epoch:1,batch:299]: val_loss:0.670686,val_acc:0.779247,val_total:4539\n",
      "[epoch:1,batch:329]:acc: 0.759659,loss:0.745150\n",
      "[epoch:1,batch:359]:acc: 0.759896,loss:0.741889\n",
      "[epoch:1,batch:389]:acc: 0.759615,loss:0.739395\n",
      "[epoch:1,batch:419]:acc: 0.758408,loss:0.740498\n",
      "[epoch:1,batch:449]:acc: 0.759167,loss:0.739234\n",
      "[epoch:1,batch:479]:acc: 0.759831,loss:0.736515\n",
      "[epoch:1,batch:509]:acc: 0.760049,loss:0.733479\n",
      "[epoch:1,batch:539]:acc: 0.760185,loss:0.732903\n",
      "[epoch:1,batch:569]:acc: 0.760746,loss:0.733507\n",
      "[epoch:1,batch:599]:acc: 0.761510,loss:0.730923\n",
      "[epoch:1,batch:599]: val_loss:0.638565,val_acc:0.790703,val_total:4539\n",
      "[epoch:1,batch:629]:acc: 0.761756,loss:0.728374\n",
      "[epoch:1,batch:659]:acc: 0.762358,loss:0.725786\n",
      "[epoch:1,batch:689]:acc: 0.761775,loss:0.725475\n",
      "[epoch:1,batch:719]:acc: 0.761068,loss:0.725127\n",
      "[epoch:1,batch:749]:acc: 0.761708,loss:0.722869\n",
      "[epoch:1,batch:779]:acc: 0.761418,loss:0.722704\n",
      "[epoch:1,batch:809]:acc: 0.762076,loss:0.720039\n",
      "[epoch:1,batch:839]:acc: 0.762240,loss:0.719808\n",
      "[epoch:1,batch:869]:acc: 0.763326,loss:0.718395\n",
      "[epoch:1,batch:899]:acc: 0.763819,loss:0.716655\n",
      "[epoch:1,batch:899]: val_loss:0.626319,val_acc:0.781009,val_total:4539\n",
      "[epoch:1,batch:929]:acc: 0.764449,loss:0.714943\n",
      "[epoch:1,batch:959]:acc: 0.764941,loss:0.714344\n",
      "[epoch:1,batch:989]:acc: 0.764678,loss:0.714323\n",
      "[epoch:1] :acc: 0.764700,loss:0.714455,lr:0.001000,patience:0\n",
      "[epoch:1]: val_loss:0.656365,val_acc:0.784754,\n",
      "save new model loss,now loss is  0.656365156173706\n",
      "Epoch 2/59\n",
      "----------\n",
      "[epoch:2,batch:29]:acc: 0.764583,loss:0.779855\n",
      "[epoch:2,batch:59]:acc: 0.778125,loss:0.707502\n",
      "[epoch:2,batch:89]:acc: 0.781944,loss:0.668878\n",
      "[epoch:2,batch:119]:acc: 0.780990,loss:0.672972\n",
      "[epoch:2,batch:149]:acc: 0.779375,loss:0.667393\n",
      "[epoch:2,batch:179]:acc: 0.778993,loss:0.668007\n",
      "[epoch:2,batch:209]:acc: 0.779315,loss:0.665062\n",
      "[epoch:2,batch:239]:acc: 0.782813,loss:0.655878\n",
      "[epoch:2,batch:269]:acc: 0.782523,loss:0.657989\n",
      "[epoch:2,batch:299]:acc: 0.781979,loss:0.660928\n",
      "[epoch:2,batch:299]: val_loss:0.628596,val_acc:0.785415,val_total:4539\n",
      "[epoch:2,batch:329]:acc: 0.780303,loss:0.661234\n",
      "[epoch:2,batch:359]:acc: 0.782292,loss:0.655294\n",
      "[epoch:2,batch:389]:acc: 0.782372,loss:0.650261\n",
      "[epoch:2,batch:419]:acc: 0.782366,loss:0.647134\n",
      "[epoch:2,batch:449]:acc: 0.782778,loss:0.643806\n",
      "[epoch:2,batch:479]:acc: 0.782552,loss:0.641475\n",
      "[epoch:2,batch:509]:acc: 0.783701,loss:0.637978\n",
      "[epoch:2,batch:539]:acc: 0.784201,loss:0.636653\n",
      "[epoch:2,batch:569]:acc: 0.783662,loss:0.635277\n",
      "[epoch:2,batch:599]:acc: 0.783646,loss:0.634839\n",
      "[epoch:2,batch:599]: val_loss:0.617506,val_acc:0.781450,val_total:4539\n",
      "[epoch:2,batch:629]:acc: 0.783383,loss:0.632984\n",
      "[epoch:2,batch:659]:acc: 0.784044,loss:0.631359\n",
      "[epoch:2,batch:689]:acc: 0.784194,loss:0.633855\n",
      "[epoch:2,batch:719]:acc: 0.784245,loss:0.634719\n",
      "[epoch:2,batch:749]:acc: 0.782750,loss:0.637682\n",
      "[epoch:2,batch:779]:acc: 0.783333,loss:0.635155\n",
      "[epoch:2,batch:809]:acc: 0.783140,loss:0.634868\n",
      "[epoch:2,batch:839]:acc: 0.783743,loss:0.635344\n",
      "[epoch:2,batch:869]:acc: 0.783549,loss:0.635385\n",
      "[epoch:2,batch:899]:acc: 0.783194,loss:0.634004\n",
      "[epoch:2,batch:899]: val_loss:0.583733,val_acc:0.800176,val_total:4539\n",
      "[epoch:2,batch:929]:acc: 0.782897,loss:0.635756\n",
      "[epoch:2,batch:959]:acc: 0.782227,loss:0.636924\n",
      "[epoch:2,batch:989]:acc: 0.782229,loss:0.636816\n",
      "[epoch:2] :acc: 0.782041,loss:0.637118,lr:0.001000,patience:0\n",
      "[epoch:2]: val_loss:0.633215,val_acc:0.789381,\n",
      "save new model loss,now loss is  0.6332149505615234\n",
      "Epoch 3/59\n",
      "----------\n",
      "set lr=:0.000030,momentum=0.950000\n",
      "[epoch:3,batch:29]:acc: 0.803125,loss:0.532576\n",
      "[epoch:3,batch:59]:acc: 0.819792,loss:0.499595\n",
      "[epoch:3,batch:89]:acc: 0.817361,loss:0.508050\n",
      "[epoch:3,batch:119]:acc: 0.819010,loss:0.495810\n",
      "[epoch:3,batch:149]:acc: 0.823125,loss:0.487842\n",
      "[epoch:3,batch:179]:acc: 0.822396,loss:0.487631\n",
      "[epoch:3,batch:209]:acc: 0.822321,loss:0.493012\n",
      "[epoch:3,batch:239]:acc: 0.821745,loss:0.494517\n",
      "[epoch:3,batch:269]:acc: 0.820486,loss:0.494320\n",
      "[epoch:3,batch:299]:acc: 0.817917,loss:0.501518\n",
      "[epoch:3,batch:299]: val_loss:0.497633,val_acc:0.820886,val_total:4539\n",
      "[epoch:3,batch:329]:acc: 0.818466,loss:0.496621\n",
      "[epoch:3,batch:359]:acc: 0.820573,loss:0.489043\n",
      "[epoch:3,batch:389]:acc: 0.820593,loss:0.488382\n",
      "[epoch:3,batch:419]:acc: 0.821280,loss:0.483779\n",
      "[epoch:3,batch:449]:acc: 0.821736,loss:0.481977\n",
      "[epoch:3,batch:479]:acc: 0.821159,loss:0.483224\n",
      "[epoch:3,batch:509]:acc: 0.820895,loss:0.484346\n",
      "[epoch:3,batch:539]:acc: 0.822049,loss:0.482981\n",
      "[epoch:3,batch:569]:acc: 0.821985,loss:0.486130\n",
      "[epoch:3,batch:599]:acc: 0.822865,loss:0.482599\n",
      "[epoch:3,batch:599]: val_loss:0.484093,val_acc:0.821547,val_total:4539\n",
      "[epoch:3,batch:629]:acc: 0.821875,loss:0.483593\n",
      "[epoch:3,batch:659]:acc: 0.822964,loss:0.480168\n",
      "[epoch:3,batch:689]:acc: 0.823007,loss:0.478812\n",
      "[epoch:3,batch:719]:acc: 0.823351,loss:0.477662\n",
      "[epoch:3,batch:749]:acc: 0.824042,loss:0.475362\n",
      "[epoch:3,batch:779]:acc: 0.823958,loss:0.474598\n",
      "[epoch:3,batch:809]:acc: 0.825309,loss:0.471550\n",
      "[epoch:3,batch:839]:acc: 0.825967,loss:0.469665\n",
      "[epoch:3,batch:869]:acc: 0.826473,loss:0.468125\n",
      "[epoch:3,batch:899]:acc: 0.826771,loss:0.466529\n",
      "[epoch:3,batch:899]: val_loss:0.458499,val_acc:0.827495,val_total:4539\n",
      "[epoch:3,batch:929]:acc: 0.827016,loss:0.464548\n",
      "[epoch:3,batch:959]:acc: 0.828092,loss:0.461454\n",
      "[epoch:3,batch:989]:acc: 0.828535,loss:0.460083\n",
      "[epoch:3] :acc: 0.828515,loss:0.460819,lr:0.000030,patience:0\n",
      "[epoch:3]: val_loss:0.448955,val_acc:0.830579,\n",
      "save new model loss,now loss is  0.4489552676677704\n",
      "save new model acc,now acc is  tensor(0.8306, device='cuda:0')\n",
      "Epoch 4/59\n",
      "----------\n",
      "set lr=:0.000030,momentum=0.950000\n",
      "[epoch:4,batch:29]:acc: 0.869792,loss:0.338517\n",
      "[epoch:4,batch:59]:acc: 0.862500,loss:0.338713\n",
      "[epoch:4,batch:89]:acc: 0.857986,loss:0.360513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:4,batch:119]:acc: 0.859375,loss:0.355627\n",
      "[epoch:4,batch:149]:acc: 0.853750,loss:0.358028\n",
      "[epoch:4,batch:179]:acc: 0.852951,loss:0.363766\n",
      "[epoch:4,batch:209]:acc: 0.851637,loss:0.374592\n",
      "[epoch:4,batch:239]:acc: 0.853385,loss:0.372864\n",
      "[epoch:4,batch:269]:acc: 0.854282,loss:0.372449\n",
      "[epoch:4,batch:299]:acc: 0.854896,loss:0.372060\n",
      "[epoch:4,batch:299]: val_loss:0.454710,val_acc:0.834986,val_total:4539\n",
      "[epoch:4,batch:329]:acc: 0.856155,loss:0.369862\n",
      "[epoch:4,batch:359]:acc: 0.856510,loss:0.367403\n",
      "[epoch:4,batch:389]:acc: 0.856410,loss:0.366987\n",
      "[epoch:4,batch:419]:acc: 0.856696,loss:0.367973\n",
      "[epoch:4,batch:449]:acc: 0.855764,loss:0.368838\n",
      "[epoch:4,batch:479]:acc: 0.855924,loss:0.368469\n",
      "[epoch:4,batch:509]:acc: 0.856005,loss:0.369404\n",
      "[epoch:4,batch:539]:acc: 0.856539,loss:0.368446\n",
      "[epoch:4,batch:569]:acc: 0.855263,loss:0.369900\n",
      "[epoch:4,batch:599]:acc: 0.855729,loss:0.369174\n",
      "[epoch:4,batch:599]: val_loss:0.438078,val_acc:0.836308,val_total:4539\n",
      "[epoch:4,batch:629]:acc: 0.856002,loss:0.367815\n",
      "[epoch:4,batch:659]:acc: 0.855919,loss:0.368687\n",
      "[epoch:4,batch:689]:acc: 0.855797,loss:0.368869\n",
      "[epoch:4,batch:719]:acc: 0.855295,loss:0.368940\n",
      "[epoch:4,batch:749]:acc: 0.854792,loss:0.369346\n",
      "[epoch:4,batch:779]:acc: 0.854607,loss:0.367752\n",
      "[epoch:4,batch:809]:acc: 0.854784,loss:0.368050\n",
      "[epoch:4,batch:839]:acc: 0.854576,loss:0.368677\n",
      "[epoch:4,batch:869]:acc: 0.854634,loss:0.368789\n",
      "[epoch:4,batch:899]:acc: 0.854201,loss:0.369596\n",
      "[epoch:4,batch:899]: val_loss:0.436085,val_acc:0.838951,val_total:4539\n",
      "[epoch:4,batch:929]:acc: 0.853965,loss:0.369666\n",
      "[epoch:4,batch:959]:acc: 0.853646,loss:0.369489\n",
      "[epoch:4,batch:989]:acc: 0.853946,loss:0.369629\n",
      "[epoch:4] :acc: 0.853895,loss:0.369736,lr:0.000030,patience:0\n",
      "[epoch:4]: val_loss:0.430867,val_acc:0.842036,\n",
      "save new model loss,now loss is  0.4308667480945587\n",
      "save new model acc,now acc is  tensor(0.8420, device='cuda:0')\n",
      "Epoch 5/59\n",
      "----------\n",
      "set lr=:0.000030,momentum=0.950000\n",
      "[epoch:5,batch:29]:acc: 0.873958,loss:0.293697\n",
      "[epoch:5,batch:59]:acc: 0.883333,loss:0.279575\n",
      "[epoch:5,batch:89]:acc: 0.877778,loss:0.286365\n",
      "[epoch:5,batch:119]:acc: 0.877083,loss:0.294251\n",
      "[epoch:5,batch:149]:acc: 0.876250,loss:0.302220\n",
      "[epoch:5,batch:179]:acc: 0.876563,loss:0.305586\n",
      "[epoch:5,batch:209]:acc: 0.876637,loss:0.304479\n",
      "[epoch:5,batch:239]:acc: 0.874479,loss:0.305962\n",
      "[epoch:5,batch:269]:acc: 0.873958,loss:0.305924\n",
      "[epoch:5,batch:299]:acc: 0.873333,loss:0.304478\n",
      "[epoch:5,batch:299]: val_loss:0.437048,val_acc:0.844459,val_total:4539\n",
      "[epoch:5,batch:329]:acc: 0.872254,loss:0.304641\n",
      "[epoch:5,batch:359]:acc: 0.872396,loss:0.306814\n",
      "[epoch:5,batch:389]:acc: 0.871955,loss:0.308069\n",
      "[epoch:5,batch:419]:acc: 0.871429,loss:0.310875\n",
      "[epoch:5,batch:449]:acc: 0.871111,loss:0.311552\n",
      "[epoch:5,batch:479]:acc: 0.869792,loss:0.312873\n",
      "[epoch:5,batch:509]:acc: 0.868873,loss:0.315115\n",
      "[epoch:5,batch:539]:acc: 0.868981,loss:0.316412\n",
      "[epoch:5,batch:569]:acc: 0.869792,loss:0.315331\n",
      "[epoch:5,batch:599]:acc: 0.870521,loss:0.314131\n",
      "[epoch:5,batch:599]: val_loss:0.429535,val_acc:0.851950,val_total:4539\n",
      "[epoch:5,batch:629]:acc: 0.870685,loss:0.313945\n",
      "[epoch:5,batch:659]:acc: 0.870360,loss:0.314517\n",
      "[epoch:5,batch:689]:acc: 0.870426,loss:0.315253\n",
      "[epoch:5,batch:719]:acc: 0.870486,loss:0.315308\n",
      "[epoch:5,batch:749]:acc: 0.870667,loss:0.315080\n",
      "[epoch:5,batch:779]:acc: 0.869832,loss:0.316898\n",
      "[epoch:5,batch:809]:acc: 0.869830,loss:0.318147\n",
      "[epoch:5,batch:839]:acc: 0.870350,loss:0.316416\n",
      "[epoch:5,batch:869]:acc: 0.869828,loss:0.316478\n",
      "[epoch:5,batch:899]:acc: 0.869583,loss:0.316036\n",
      "[epoch:5,batch:899]: val_loss:0.437708,val_acc:0.843137,val_total:4539\n",
      "[epoch:5,batch:929]:acc: 0.869489,loss:0.316963\n",
      "[epoch:5,batch:959]:acc: 0.870117,loss:0.317260\n",
      "[epoch:5,batch:989]:acc: 0.870044,loss:0.318337\n",
      "[epoch:5] :acc: 0.869944,loss:0.318744,lr:0.000030,patience:0\n",
      "[epoch:5]: val_loss:0.430037,val_acc:0.844459,\n",
      "save new model loss,now loss is  0.43003663420677185\n",
      "save new model acc,now acc is  tensor(0.8445, device='cuda:0')\n",
      "Epoch 6/59\n",
      "----------\n",
      "set lr=:0.000100,momentum=0.900000\n",
      "[epoch:6,batch:29]:acc: 0.852083,loss:0.340461\n",
      "[epoch:6,batch:59]:acc: 0.842708,loss:0.382046\n",
      "[epoch:6,batch:89]:acc: 0.846528,loss:0.400770\n",
      "[epoch:6,batch:119]:acc: 0.847396,loss:0.398516\n",
      "[epoch:6,batch:149]:acc: 0.846042,loss:0.399651\n",
      "[epoch:6,batch:179]:acc: 0.848437,loss:0.394125\n",
      "[epoch:6,batch:209]:acc: 0.847321,loss:0.401003\n",
      "[epoch:6,batch:239]:acc: 0.844531,loss:0.405106\n",
      "[epoch:6,batch:269]:acc: 0.843287,loss:0.404928\n",
      "[epoch:6,batch:299]:acc: 0.842500,loss:0.404485\n",
      "[epoch:6,batch:299]: val_loss:0.533482,val_acc:0.822648,val_total:4539\n",
      "[epoch:6,batch:329]:acc: 0.841288,loss:0.407618\n",
      "[epoch:6,batch:359]:acc: 0.839063,loss:0.412882\n",
      "[epoch:6,batch:389]:acc: 0.838542,loss:0.417101\n",
      "[epoch:6,batch:419]:acc: 0.839211,loss:0.415311\n",
      "[epoch:6,batch:449]:acc: 0.838264,loss:0.415017\n",
      "[epoch:6,batch:479]:acc: 0.838281,loss:0.414996\n",
      "[epoch:6,batch:509]:acc: 0.836275,loss:0.417606\n",
      "[epoch:6,batch:539]:acc: 0.834491,loss:0.419752\n",
      "[epoch:6,batch:569]:acc: 0.834265,loss:0.422173\n",
      "[epoch:6,batch:599]:acc: 0.834063,loss:0.423370\n",
      "[epoch:6,batch:599]: val_loss:0.468462,val_acc:0.827715,val_total:4539\n",
      "[epoch:6,batch:629]:acc: 0.833433,loss:0.423718\n",
      "[epoch:6,batch:659]:acc: 0.833665,loss:0.424145\n",
      "[epoch:6,batch:689]:acc: 0.834239,loss:0.423155\n",
      "[epoch:6,batch:719]:acc: 0.834028,loss:0.423638\n",
      "[epoch:6,batch:749]:acc: 0.833833,loss:0.423935\n",
      "[epoch:6,batch:779]:acc: 0.833894,loss:0.424453\n",
      "[epoch:6,batch:809]:acc: 0.833333,loss:0.425775\n",
      "[epoch:6,batch:839]:acc: 0.833036,loss:0.425444\n",
      "[epoch:6,batch:869]:acc: 0.833010,loss:0.425343\n",
      "[epoch:6,batch:899]:acc: 0.833229,loss:0.425136\n",
      "[epoch:6,batch:899]: val_loss:0.455385,val_acc:0.832342,val_total:4539\n",
      "[epoch:6,batch:929]:acc: 0.833199,loss:0.425939\n"
     ]
    }
   ],
   "source": [
    "train(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reuseTrain(path,epochNum):\n",
    "    writer=SummaryWriter('log/'+date+'/DesNet161/') # 创建 /log/日期/InceptionResnet的组织形式\n",
    "    train_dataset=MyDataSet(json_Description=ANNOTATION_TRAIN,transform=preprocess_with_augmentation(normalize_torch,IMAGE_SIZE),path_pre=IMAGE_TRAIN_PRE)\n",
    "    val_dataset=MyDataSet(json_Description=ANNOTATION_VAL,transform=preprocess(normalize_torch,IMAGE_SIZE),path_pre=IMAGE_VAL_PRE)\n",
    "    train_dataLoader=DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE,num_workers=16,shuffle=True)\n",
    "    val_dataLoader=DataLoader(dataset=val_dataset,batch_size=BATCH_SIZE,num_workers=1,shuffle=False)\n",
    "    model=getmodel()\n",
    "    criterion=nn.CrossEntropyLoss().cuda()\n",
    "    modelParams=torch.load(path)\n",
    "    model.load_state_dict(modelParams['state_dict'])\n",
    "    min_loss=modelParams['val_loss']\n",
    "    print('min_loss is :%f'%(min_loss))   \n",
    "    min_acc=max(modelParams['val_correct'],0.81)\n",
    "    print('val_correct is %f'%(min_acc))\n",
    "    patience=0\n",
    "    lr=0.00003\n",
    "    momentum=0.9\n",
    "    beginepoch=modelParams['epoch']\n",
    "    for epoch in range(beginepoch,epochNum):\n",
    "        print('Epoch {}/{}'.format(epoch, epochNum - 1))\n",
    "        print('-' * 10)\n",
    "        if patience==2:\n",
    "            patience=0\n",
    "            model.load_state_dict(torch.load('../model/DesNet161/'+date+'_loss_best.pth')['state_dict'])\n",
    "            lr=lr/10\n",
    "        optimizer = torch.optim.Adam(model.parameters(),lr = lr,amsgrad=True,weight_decay=1e-4)\n",
    "        print('lr now is %f'%(lr))\n",
    "        print('now patience is %d '%(patience))\n",
    "        running_loss = RunningMean()\n",
    "        running_corrects = RunningMean()\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_dataLoader):\n",
    "            model.train(True)\n",
    "            n_batchsize=inputs.size(0)\n",
    "            inputs = Variable(inputs).cuda()\n",
    "            labels = Variable(labels).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            if isinstance(outputs,tuple):\n",
    "                loss=sum((criterion(o,labels)) for o in outputs)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "            running_loss.update(loss.item(),1)\n",
    "            running_corrects.update(torch.sum(preds == labels.data).data,n_batchsize)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx%30==29:\n",
    "                print('[epoch:%d,batch:%d]:acc: %f,loss:%f'%(epoch,batch_idx,running_corrects.value,running_loss.value))\n",
    "                if batch_idx%300==299: \n",
    "                    niter = epoch * len(train_dataset)/BATCH_SIZE + batch_idx\n",
    "                    writer.add_scalar('Train/Acc',running_corrects.value,niter)\n",
    "                    writer.add_scalar('Train/Loss',running_loss.value,niter)\n",
    "                    lx,px=utils.predict(model,val_dataLoader)\n",
    "                    log_loss = criterion(Variable(px), Variable(lx))\n",
    "                    log_loss = log_loss.item()\n",
    "                    _, preds = torch.max(px, dim=1)\n",
    "                    accuracy = torch.mean((preds == lx).float())\n",
    "                    writer.add_scalar('Val/Acc',accuracy,niter)\n",
    "                    writer.add_scalar('Val/Loss',log_loss,niter)\n",
    "                    print('[epoch:%d,batch:%d]: val_loss:%f,val_acc:%f,val_total:%d'%(epoch,batch_idx,log_loss,accuracy,len(val_dataset)))\n",
    "        print('[epoch:%d] :acc: %f,loss:%f,lr:%f,patience:%d'%(epoch,running_corrects.value,running_loss.value,lr,patience))      \n",
    "        lx,px=utils.predict(model,val_dataLoader)\n",
    "        log_loss = criterion(Variable(px), Variable(lx))\n",
    "        log_loss = log_loss.item()\n",
    "        _, preds = torch.max(px, dim=1)\n",
    "        accuracy = torch.mean((preds == lx).float())\n",
    "        writer.add_scalar('Val/Acc',accuracy,(epoch+1) * len(train_dataset)/BATCH_SIZE)\n",
    "        writer.add_scalar('Val/Loss',log_loss,(epoch+1) * len(train_dataset)/BATCH_SIZE)\n",
    "        print('[epoch:%d]: val_loss:%f,val_acc:%f,'%(epoch,log_loss,accuracy))\n",
    "        if  log_loss < min_loss:\n",
    "            utils.snapshot('../model/', 'DesNet161', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy })          \n",
    "            patience = 0\n",
    "            min_loss=log_loss\n",
    "            print('save new model loss,now loss is ',min_loss)\n",
    "        else:\n",
    "            patience += 1\n",
    "        if accuracy>min_acc:\n",
    "            utils.snapshot('../model/', 'DesNet161', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy },key='acc') \n",
    "            min_acc=accuracy\n",
    "            print('save new model acc,now acc is ',min_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] loading model... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lujunfeng/anaconda3/envs/Conda_Env_Pytorch/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "min_loss is :0.430037\n",
      "val_correct is 0.844459\n",
      "Epoch 6/59\n",
      "----------\n",
      "lr now is 0.000030\n",
      "now patience is 0 \n",
      "[epoch:6,batch:29]:acc: 0.909375,loss:0.241067\n",
      "[epoch:6,batch:59]:acc: 0.904167,loss:0.250481\n",
      "[epoch:6,batch:89]:acc: 0.904514,loss:0.240705\n",
      "[epoch:6,batch:119]:acc: 0.902083,loss:0.242806\n",
      "[epoch:6,batch:149]:acc: 0.901458,loss:0.242425\n",
      "[epoch:6,batch:179]:acc: 0.900521,loss:0.243219\n",
      "[epoch:6,batch:209]:acc: 0.899554,loss:0.245344\n",
      "[epoch:6,batch:239]:acc: 0.897786,loss:0.248654\n",
      "[epoch:6,batch:269]:acc: 0.896296,loss:0.252380\n",
      "[epoch:6,batch:299]:acc: 0.894896,loss:0.255700\n",
      "[epoch:6,batch:299]: val_loss:0.397014,val_acc:0.852611,val_total:4539\n",
      "[epoch:6,batch:329]:acc: 0.895833,loss:0.253637\n",
      "[epoch:6,batch:359]:acc: 0.893663,loss:0.257207\n",
      "[epoch:6,batch:389]:acc: 0.894231,loss:0.256767\n",
      "[epoch:6,batch:419]:acc: 0.893155,loss:0.258344\n",
      "[epoch:6,batch:449]:acc: 0.893403,loss:0.257544\n",
      "[epoch:6,batch:479]:acc: 0.892448,loss:0.259064\n",
      "[epoch:6,batch:509]:acc: 0.891789,loss:0.260278\n",
      "[epoch:6,batch:539]:acc: 0.891493,loss:0.262197\n",
      "[epoch:6,batch:569]:acc: 0.891831,loss:0.262004\n",
      "[epoch:6,batch:599]:acc: 0.891510,loss:0.262652\n",
      "[epoch:6,batch:599]: val_loss:0.412501,val_acc:0.842697,val_total:4539\n",
      "[epoch:6,batch:629]:acc: 0.892113,loss:0.261815\n",
      "[epoch:6,batch:659]:acc: 0.891383,loss:0.262030\n",
      "[epoch:6,batch:689]:acc: 0.891123,loss:0.263156\n",
      "[epoch:6,batch:719]:acc: 0.891753,loss:0.262354\n",
      "[epoch:6,batch:749]:acc: 0.891500,loss:0.262503\n",
      "[epoch:6,batch:779]:acc: 0.891346,loss:0.262389\n",
      "[epoch:6,batch:809]:acc: 0.890702,loss:0.263083\n",
      "[epoch:6,batch:839]:acc: 0.891146,loss:0.262549\n",
      "[epoch:6,batch:869]:acc: 0.890912,loss:0.262847\n",
      "[epoch:6,batch:899]:acc: 0.890729,loss:0.262915\n",
      "[epoch:6,batch:899]: val_loss:0.416409,val_acc:0.840273,val_total:4539\n",
      "[epoch:6,batch:929]:acc: 0.890356,loss:0.263920\n",
      "[epoch:6,batch:959]:acc: 0.890039,loss:0.264892\n",
      "[epoch:6,batch:989]:acc: 0.889457,loss:0.265594\n",
      "[epoch:6] :acc: 0.889523,loss:0.265747,lr:0.000030,patience:0\n",
      "[epoch:6]: val_loss:0.405938,val_acc:0.853272,\n",
      "save new model loss,now loss is  0.4059378206729889\n",
      "save new model acc,now acc is  tensor(0.8533, device='cuda:0')\n",
      "Epoch 7/59\n",
      "----------\n",
      "lr now is 0.000030\n",
      "now patience is 0 \n",
      "[epoch:7,batch:29]:acc: 0.897917,loss:0.263113\n",
      "[epoch:7,batch:59]:acc: 0.900000,loss:0.242453\n",
      "[epoch:7,batch:89]:acc: 0.900347,loss:0.236150\n",
      "[epoch:7,batch:119]:acc: 0.903385,loss:0.233289\n",
      "[epoch:7,batch:149]:acc: 0.902917,loss:0.233982\n",
      "[epoch:7,batch:179]:acc: 0.903125,loss:0.234469\n",
      "[epoch:7,batch:209]:acc: 0.904613,loss:0.231534\n",
      "[epoch:7,batch:239]:acc: 0.903776,loss:0.233032\n",
      "[epoch:7,batch:269]:acc: 0.904977,loss:0.229606\n",
      "[epoch:7,batch:299]:acc: 0.904271,loss:0.230660\n",
      "[epoch:7,batch:299]: val_loss:0.394880,val_acc:0.848865,val_total:4539\n",
      "[epoch:7,batch:329]:acc: 0.902746,loss:0.232720\n",
      "[epoch:7,batch:359]:acc: 0.902170,loss:0.232642\n",
      "[epoch:7,batch:389]:acc: 0.902644,loss:0.232401\n",
      "[epoch:7,batch:419]:acc: 0.902232,loss:0.232528\n",
      "[epoch:7,batch:449]:acc: 0.901944,loss:0.233009\n",
      "[epoch:7,batch:479]:acc: 0.901237,loss:0.234077\n",
      "[epoch:7,batch:509]:acc: 0.900858,loss:0.234098\n",
      "[epoch:7,batch:539]:acc: 0.900058,loss:0.235172\n",
      "[epoch:7,batch:569]:acc: 0.899507,loss:0.235871\n",
      "[epoch:7,batch:599]:acc: 0.899792,loss:0.235798\n",
      "[epoch:7,batch:599]: val_loss:0.399513,val_acc:0.850848,val_total:4539\n",
      "[epoch:7,batch:629]:acc: 0.899405,loss:0.235932\n",
      "[epoch:7,batch:659]:acc: 0.899432,loss:0.235534\n",
      "[epoch:7,batch:689]:acc: 0.899004,loss:0.236648\n",
      "[epoch:7,batch:719]:acc: 0.899045,loss:0.236731\n",
      "[epoch:7,batch:749]:acc: 0.899250,loss:0.236269\n",
      "[epoch:7,batch:779]:acc: 0.898558,loss:0.237448\n",
      "[epoch:7,batch:809]:acc: 0.899190,loss:0.236726\n",
      "[epoch:7,batch:839]:acc: 0.899665,loss:0.236728\n",
      "[epoch:7,batch:869]:acc: 0.900036,loss:0.236188\n",
      "[epoch:7,batch:899]:acc: 0.900451,loss:0.235767\n",
      "[epoch:7,batch:899]: val_loss:0.406874,val_acc:0.842697,val_total:4539\n",
      "[epoch:7,batch:929]:acc: 0.900269,loss:0.235740\n",
      "[epoch:7,batch:959]:acc: 0.900391,loss:0.235399\n",
      "[epoch:7,batch:989]:acc: 0.900347,loss:0.235752\n",
      "[epoch:7] :acc: 0.900369,loss:0.235971,lr:0.000030,patience:0\n",
      "[epoch:7]: val_loss:0.420748,val_acc:0.841375,\n",
      "Epoch 8/59\n",
      "----------\n",
      "lr now is 0.000030\n",
      "now patience is 1 \n",
      "[epoch:8,batch:29]:acc: 0.936458,loss:0.186795\n",
      "[epoch:8,batch:59]:acc: 0.928646,loss:0.189189\n",
      "[epoch:8,batch:89]:acc: 0.925694,loss:0.190019\n",
      "[epoch:8,batch:119]:acc: 0.925781,loss:0.194815\n",
      "[epoch:8,batch:149]:acc: 0.923958,loss:0.196429\n",
      "[epoch:8,batch:179]:acc: 0.920833,loss:0.200983\n",
      "[epoch:8,batch:209]:acc: 0.920685,loss:0.200993\n",
      "[epoch:8,batch:239]:acc: 0.921615,loss:0.198724\n",
      "[epoch:8,batch:269]:acc: 0.919792,loss:0.201247\n",
      "[epoch:8,batch:299]:acc: 0.918438,loss:0.203670\n",
      "[epoch:8,batch:299]: val_loss:0.409281,val_acc:0.843798,val_total:4539\n",
      "[epoch:8,batch:329]:acc: 0.917992,loss:0.203860\n",
      "[epoch:8,batch:359]:acc: 0.918837,loss:0.202482\n",
      "[epoch:8,batch:389]:acc: 0.918510,loss:0.202174\n",
      "[epoch:8,batch:419]:acc: 0.918080,loss:0.201211\n",
      "[epoch:8,batch:449]:acc: 0.918264,loss:0.201684\n",
      "[epoch:8,batch:479]:acc: 0.917773,loss:0.201694\n",
      "[epoch:8,batch:509]:acc: 0.918137,loss:0.200554\n",
      "[epoch:8,batch:539]:acc: 0.917882,loss:0.200791\n",
      "[epoch:8,batch:569]:acc: 0.917270,loss:0.201524\n",
      "[epoch:8,batch:599]:acc: 0.916719,loss:0.202252\n",
      "[epoch:8,batch:599]: val_loss:0.438892,val_acc:0.838731,val_total:4539\n",
      "[epoch:8,batch:629]:acc: 0.916121,loss:0.203219\n",
      "[epoch:8,batch:659]:acc: 0.916761,loss:0.202432\n",
      "[epoch:8,batch:689]:acc: 0.916395,loss:0.203061\n",
      "[epoch:8,batch:719]:acc: 0.916493,loss:0.203454\n",
      "[epoch:8,batch:749]:acc: 0.915458,loss:0.204875\n",
      "[epoch:8,batch:779]:acc: 0.915224,loss:0.205659\n",
      "[epoch:8,batch:809]:acc: 0.915355,loss:0.204899\n",
      "[epoch:8,batch:839]:acc: 0.915551,loss:0.204664\n",
      "[epoch:8,batch:869]:acc: 0.914763,loss:0.205647\n",
      "[epoch:8,batch:899]:acc: 0.914410,loss:0.206538\n",
      "[epoch:8,batch:899]: val_loss:0.411170,val_acc:0.847984,val_total:4539\n",
      "[epoch:8,batch:929]:acc: 0.914348,loss:0.206931\n",
      "[epoch:8,batch:959]:acc: 0.913997,loss:0.207573\n",
      "[epoch:8,batch:989]:acc: 0.913920,loss:0.207728\n",
      "[epoch:8] :acc: 0.913800,loss:0.208555,lr:0.000030,patience:1\n",
      "[epoch:8]: val_loss:0.409339,val_acc:0.851509,\n",
      "Epoch 9/59\n",
      "----------\n",
      "lr now is 0.000003\n",
      "now patience is 0 \n",
      "[epoch:9,batch:29]:acc: 0.927083,loss:0.178528\n",
      "[epoch:9,batch:59]:acc: 0.922396,loss:0.182746\n",
      "[epoch:9,batch:89]:acc: 0.926042,loss:0.183735\n",
      "[epoch:9,batch:119]:acc: 0.926302,loss:0.182407\n",
      "[epoch:9,batch:149]:acc: 0.927083,loss:0.182003\n",
      "[epoch:9,batch:179]:acc: 0.927257,loss:0.183842\n",
      "[epoch:9,batch:209]:acc: 0.929167,loss:0.181618\n",
      "[epoch:9,batch:239]:acc: 0.927474,loss:0.183207\n",
      "[epoch:9,batch:269]:acc: 0.927315,loss:0.184483\n",
      "[epoch:9,batch:299]:acc: 0.928125,loss:0.182594\n",
      "[epoch:9,batch:299]: val_loss:0.379775,val_acc:0.860322,val_total:4539\n",
      "[epoch:9,batch:329]:acc: 0.929735,loss:0.180207\n",
      "[epoch:9,batch:359]:acc: 0.931250,loss:0.178380\n",
      "[epoch:9,batch:389]:acc: 0.932212,loss:0.177275\n",
      "[epoch:9,batch:419]:acc: 0.933408,loss:0.175501\n",
      "[epoch:9,batch:449]:acc: 0.934514,loss:0.173814\n",
      "[epoch:9,batch:479]:acc: 0.934440,loss:0.173856\n",
      "[epoch:9,batch:509]:acc: 0.935110,loss:0.173487\n",
      "[epoch:9,batch:539]:acc: 0.935822,loss:0.172880\n",
      "[epoch:9,batch:569]:acc: 0.936294,loss:0.172487\n",
      "[epoch:9,batch:599]:acc: 0.936719,loss:0.171148\n",
      "[epoch:9,batch:599]: val_loss:0.380181,val_acc:0.859220,val_total:4539\n",
      "[epoch:9,batch:629]:acc: 0.936657,loss:0.171450\n",
      "[epoch:9,batch:659]:acc: 0.937263,loss:0.170041\n",
      "[epoch:9,batch:689]:acc: 0.937228,loss:0.169870\n",
      "[epoch:9,batch:719]:acc: 0.937934,loss:0.168673\n",
      "[epoch:9,batch:749]:acc: 0.937917,loss:0.168082\n",
      "[epoch:9,batch:779]:acc: 0.938141,loss:0.167689\n",
      "[epoch:9,batch:809]:acc: 0.938735,loss:0.166529\n",
      "[epoch:9,batch:839]:acc: 0.939063,loss:0.165583\n",
      "[epoch:9,batch:869]:acc: 0.939116,loss:0.164839\n",
      "[epoch:9,batch:899]:acc: 0.939410,loss:0.163983\n",
      "[epoch:9,batch:899]: val_loss:0.379203,val_acc:0.860322,val_total:4539\n",
      "[epoch:9,batch:929]:acc: 0.939348,loss:0.163610\n",
      "[epoch:9,batch:959]:acc: 0.939681,loss:0.162844\n",
      "[epoch:9,batch:989]:acc: 0.939710,loss:0.162772\n",
      "[epoch:9] :acc: 0.939654,loss:0.163117,lr:0.000003,patience:0\n",
      "[epoch:9]: val_loss:0.378558,val_acc:0.860983,\n",
      "save new model loss,now loss is  0.378557950258255\n",
      "save new model acc,now acc is  tensor(0.8610, device='cuda:0')\n",
      "Epoch 10/59\n",
      "----------\n",
      "lr now is 0.000003\n",
      "now patience is 0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:10,batch:29]:acc: 0.966667,loss:0.117303\n",
      "[epoch:10,batch:59]:acc: 0.963021,loss:0.124353\n",
      "[epoch:10,batch:89]:acc: 0.959028,loss:0.132778\n",
      "[epoch:10,batch:119]:acc: 0.960677,loss:0.130835\n",
      "[epoch:10,batch:149]:acc: 0.957500,loss:0.133087\n",
      "[epoch:10,batch:179]:acc: 0.955556,loss:0.136296\n",
      "[epoch:10,batch:209]:acc: 0.953720,loss:0.137364\n",
      "[epoch:10,batch:239]:acc: 0.952344,loss:0.137779\n",
      "[epoch:10,batch:269]:acc: 0.953241,loss:0.136723\n",
      "[epoch:10,batch:299]:acc: 0.953125,loss:0.136260\n",
      "[epoch:10,batch:299]: val_loss:0.376543,val_acc:0.863847,val_total:4539\n",
      "[epoch:10,batch:329]:acc: 0.953409,loss:0.135150\n",
      "[epoch:10,batch:359]:acc: 0.952865,loss:0.134897\n",
      "[epoch:10,batch:389]:acc: 0.952804,loss:0.134715\n",
      "[epoch:10,batch:419]:acc: 0.953125,loss:0.134908\n",
      "[epoch:10,batch:449]:acc: 0.953472,loss:0.134584\n",
      "[epoch:10,batch:479]:acc: 0.953581,loss:0.133909\n",
      "[epoch:10,batch:509]:acc: 0.953922,loss:0.133184\n",
      "[epoch:10,batch:539]:acc: 0.954109,loss:0.132964\n",
      "[epoch:10,batch:569]:acc: 0.954386,loss:0.132806\n",
      "[epoch:10,batch:599]:acc: 0.954948,loss:0.131786\n",
      "[epoch:10,batch:599]: val_loss:0.380144,val_acc:0.859000,val_total:4539\n",
      "[epoch:10,batch:629]:acc: 0.954663,loss:0.131857\n",
      "[epoch:10,batch:659]:acc: 0.955161,loss:0.131356\n",
      "[epoch:10,batch:689]:acc: 0.955163,loss:0.131117\n",
      "[epoch:10,batch:719]:acc: 0.954731,loss:0.131192\n",
      "[epoch:10,batch:749]:acc: 0.954750,loss:0.131238\n",
      "[epoch:10,batch:779]:acc: 0.954768,loss:0.131061\n",
      "[epoch:10,batch:809]:acc: 0.954861,loss:0.130768\n",
      "[epoch:10,batch:839]:acc: 0.954836,loss:0.130543\n",
      "[epoch:10,batch:869]:acc: 0.954993,loss:0.130147\n",
      "[epoch:10,batch:899]:acc: 0.955069,loss:0.130446\n",
      "[epoch:10,batch:899]: val_loss:0.381275,val_acc:0.859661,val_total:4539\n",
      "[epoch:10,batch:929]:acc: 0.954839,loss:0.130314\n",
      "[epoch:10,batch:959]:acc: 0.954980,loss:0.130107\n",
      "[epoch:10,batch:989]:acc: 0.954830,loss:0.130687\n",
      "[epoch:10] :acc: 0.954756,loss:0.130949,lr:0.000003,patience:0\n",
      "[epoch:10]: val_loss:0.380187,val_acc:0.859440,\n",
      "Epoch 11/59\n",
      "----------\n",
      "lr now is 0.000003\n",
      "now patience is 1 \n",
      "[epoch:11,batch:29]:acc: 0.961458,loss:0.111208\n",
      "[epoch:11,batch:59]:acc: 0.962500,loss:0.108733\n",
      "[epoch:11,batch:89]:acc: 0.962500,loss:0.109496\n",
      "[epoch:11,batch:119]:acc: 0.962760,loss:0.109451\n",
      "[epoch:11,batch:149]:acc: 0.961458,loss:0.112262\n",
      "[epoch:11,batch:179]:acc: 0.963021,loss:0.111093\n",
      "[epoch:11,batch:209]:acc: 0.962946,loss:0.110495\n",
      "[epoch:11,batch:239]:acc: 0.964844,loss:0.109417\n",
      "[epoch:11,batch:269]:acc: 0.964931,loss:0.109616\n",
      "[epoch:11,batch:299]:acc: 0.965104,loss:0.109611\n",
      "[epoch:11,batch:299]: val_loss:0.383611,val_acc:0.857898,val_total:4539\n",
      "[epoch:11,batch:329]:acc: 0.965152,loss:0.110419\n",
      "[epoch:11,batch:359]:acc: 0.965191,loss:0.110018\n",
      "[epoch:11,batch:389]:acc: 0.964343,loss:0.110782\n",
      "[epoch:11,batch:419]:acc: 0.963467,loss:0.111952\n",
      "[epoch:11,batch:449]:acc: 0.963542,loss:0.111802\n",
      "[epoch:11,batch:479]:acc: 0.963281,loss:0.112412\n",
      "[epoch:11,batch:509]:acc: 0.963542,loss:0.112012\n",
      "[epoch:11,batch:539]:acc: 0.963310,loss:0.112268\n",
      "[epoch:11,batch:569]:acc: 0.963596,loss:0.111868\n",
      "[epoch:11,batch:599]:acc: 0.964010,loss:0.111241\n",
      "[epoch:11,batch:599]: val_loss:0.382836,val_acc:0.859440,val_total:4539\n",
      "[epoch:11,batch:629]:acc: 0.964385,loss:0.110386\n",
      "[epoch:11,batch:659]:acc: 0.964489,loss:0.110617\n",
      "[epoch:11,batch:689]:acc: 0.964674,loss:0.110134\n",
      "[epoch:11,batch:719]:acc: 0.964931,loss:0.109392\n",
      "[epoch:11,batch:749]:acc: 0.965125,loss:0.108704\n",
      "[epoch:11,batch:779]:acc: 0.964864,loss:0.108838\n",
      "[epoch:11,batch:809]:acc: 0.965085,loss:0.108696\n",
      "[epoch:11,batch:839]:acc: 0.965067,loss:0.108760\n",
      "[epoch:11,batch:869]:acc: 0.965158,loss:0.108757\n",
      "[epoch:11,batch:899]:acc: 0.965139,loss:0.108689\n",
      "[epoch:11,batch:899]: val_loss:0.381253,val_acc:0.862745,val_total:4539\n",
      "[epoch:11,batch:929]:acc: 0.965222,loss:0.108555\n",
      "[epoch:11,batch:959]:acc: 0.965365,loss:0.108144\n",
      "[epoch:11,batch:989]:acc: 0.965278,loss:0.108507\n",
      "[epoch:11] :acc: 0.965287,loss:0.108833,lr:0.000003,patience:1\n",
      "[epoch:11]: val_loss:0.382938,val_acc:0.865389,\n",
      "save new model acc,now acc is  tensor(0.8654, device='cuda:0')\n",
      "Epoch 12/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 0 \n",
      "[epoch:12,batch:29]:acc: 0.917708,loss:0.197840\n",
      "[epoch:12,batch:59]:acc: 0.919792,loss:0.193711\n",
      "[epoch:12,batch:89]:acc: 0.920139,loss:0.195389\n",
      "[epoch:12,batch:119]:acc: 0.921875,loss:0.192030\n",
      "[epoch:12,batch:149]:acc: 0.921250,loss:0.190849\n",
      "[epoch:12,batch:179]:acc: 0.922743,loss:0.188206\n",
      "[epoch:12,batch:209]:acc: 0.922768,loss:0.190143\n",
      "[epoch:12,batch:239]:acc: 0.923307,loss:0.189822\n",
      "[epoch:12,batch:269]:acc: 0.924306,loss:0.188276\n",
      "[epoch:12,batch:299]:acc: 0.925000,loss:0.188291\n",
      "[epoch:12,batch:299]: val_loss:0.377706,val_acc:0.860542,val_total:4539\n",
      "[epoch:12,batch:329]:acc: 0.924432,loss:0.188235\n",
      "[epoch:12,batch:359]:acc: 0.923438,loss:0.189947\n",
      "[epoch:12,batch:389]:acc: 0.924199,loss:0.190024\n",
      "[epoch:12,batch:419]:acc: 0.925298,loss:0.188482\n",
      "[epoch:12,batch:449]:acc: 0.924861,loss:0.188408\n",
      "[epoch:12,batch:479]:acc: 0.923893,loss:0.189266\n",
      "[epoch:12,batch:509]:acc: 0.923652,loss:0.190785\n",
      "[epoch:12,batch:539]:acc: 0.923380,loss:0.190790\n",
      "[epoch:12,batch:569]:acc: 0.923849,loss:0.191225\n",
      "[epoch:12,batch:599]:acc: 0.923646,loss:0.191258\n",
      "[epoch:12,batch:599]: val_loss:0.375980,val_acc:0.862965,val_total:4539\n",
      "[epoch:12,batch:629]:acc: 0.923413,loss:0.191506\n",
      "[epoch:12,batch:659]:acc: 0.923438,loss:0.191323\n",
      "[epoch:12,batch:689]:acc: 0.923143,loss:0.191244\n",
      "[epoch:12,batch:719]:acc: 0.922656,loss:0.191908\n",
      "[epoch:12,batch:749]:acc: 0.922875,loss:0.191912\n",
      "[epoch:12,batch:779]:acc: 0.922877,loss:0.191690\n",
      "[epoch:12,batch:809]:acc: 0.922338,loss:0.192451\n",
      "[epoch:12,batch:839]:acc: 0.922173,loss:0.192446\n",
      "[epoch:12,batch:869]:acc: 0.921947,loss:0.192854\n",
      "[epoch:12,batch:899]:acc: 0.922049,loss:0.192573\n",
      "[epoch:12,batch:899]: val_loss:0.378461,val_acc:0.859440,val_total:4539\n",
      "[epoch:12,batch:929]:acc: 0.921808,loss:0.193260\n",
      "[epoch:12,batch:959]:acc: 0.921908,loss:0.192854\n",
      "[epoch:12,batch:989]:acc: 0.921749,loss:0.192587\n",
      "[epoch:12] :acc: 0.921714,loss:0.192883,lr:0.000000,patience:0\n",
      "[epoch:12]: val_loss:0.378339,val_acc:0.861203,\n",
      "save new model loss,now loss is  0.3783390522003174\n",
      "Epoch 13/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 0 \n",
      "[epoch:13,batch:29]:acc: 0.928125,loss:0.186379\n",
      "[epoch:13,batch:59]:acc: 0.923438,loss:0.189463\n",
      "[epoch:13,batch:89]:acc: 0.926736,loss:0.186890\n",
      "[epoch:13,batch:119]:acc: 0.922135,loss:0.193092\n",
      "[epoch:13,batch:149]:acc: 0.924792,loss:0.191995\n",
      "[epoch:13,batch:179]:acc: 0.923611,loss:0.190299\n",
      "[epoch:13,batch:209]:acc: 0.922321,loss:0.189889\n",
      "[epoch:13,batch:239]:acc: 0.922396,loss:0.190932\n",
      "[epoch:13,batch:269]:acc: 0.921759,loss:0.190984\n",
      "[epoch:13,batch:299]:acc: 0.922500,loss:0.189399\n",
      "[epoch:13,batch:299]: val_loss:0.378331,val_acc:0.861203,val_total:4539\n",
      "[epoch:13,batch:329]:acc: 0.921780,loss:0.191384\n",
      "[epoch:13,batch:359]:acc: 0.921007,loss:0.193176\n",
      "[epoch:13,batch:389]:acc: 0.921314,loss:0.193513\n",
      "[epoch:13,batch:419]:acc: 0.920536,loss:0.194576\n",
      "[epoch:13,batch:449]:acc: 0.921528,loss:0.192765\n",
      "[epoch:13,batch:479]:acc: 0.921354,loss:0.193391\n",
      "[epoch:13,batch:509]:acc: 0.921324,loss:0.193673\n",
      "[epoch:13,batch:539]:acc: 0.921759,loss:0.193484\n",
      "[epoch:13,batch:569]:acc: 0.921711,loss:0.193033\n",
      "[epoch:13,batch:599]:acc: 0.922188,loss:0.192443\n",
      "[epoch:13,batch:599]: val_loss:0.375226,val_acc:0.861203,val_total:4539\n",
      "[epoch:13,batch:629]:acc: 0.922619,loss:0.192126\n",
      "[epoch:13,batch:659]:acc: 0.922585,loss:0.192412\n",
      "[epoch:13,batch:689]:acc: 0.922147,loss:0.192389\n",
      "[epoch:13,batch:719]:acc: 0.922179,loss:0.192095\n",
      "[epoch:13,batch:749]:acc: 0.922250,loss:0.191929\n",
      "[epoch:13,batch:779]:acc: 0.922356,loss:0.191845\n",
      "[epoch:13,batch:809]:acc: 0.922647,loss:0.191834\n",
      "[epoch:13,batch:839]:acc: 0.922768,loss:0.191436\n",
      "[epoch:13,batch:869]:acc: 0.923204,loss:0.190821\n",
      "[epoch:13,batch:899]:acc: 0.922847,loss:0.191453\n",
      "[epoch:13,batch:899]: val_loss:0.374869,val_acc:0.861203,val_total:4539\n",
      "[epoch:13,batch:929]:acc: 0.922480,loss:0.191454\n",
      "[epoch:13,batch:959]:acc: 0.922363,loss:0.191589\n",
      "[epoch:13,batch:989]:acc: 0.922506,loss:0.191425\n",
      "[epoch:13] :acc: 0.922502,loss:0.191757,lr:0.000000,patience:0\n",
      "[epoch:13]: val_loss:0.379548,val_acc:0.859881,\n",
      "Epoch 14/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 1 \n",
      "[epoch:14,batch:29]:acc: 0.922917,loss:0.185972\n",
      "[epoch:14,batch:59]:acc: 0.928125,loss:0.187787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:14,batch:89]:acc: 0.928125,loss:0.187564\n",
      "[epoch:14,batch:119]:acc: 0.929948,loss:0.183464\n",
      "[epoch:14,batch:149]:acc: 0.929583,loss:0.185591\n",
      "[epoch:14,batch:179]:acc: 0.928299,loss:0.186684\n",
      "[epoch:14,batch:209]:acc: 0.928869,loss:0.185868\n",
      "[epoch:14,batch:239]:acc: 0.928516,loss:0.185647\n",
      "[epoch:14,batch:269]:acc: 0.928472,loss:0.185984\n",
      "[epoch:14,batch:299]:acc: 0.927813,loss:0.186368\n",
      "[epoch:14,batch:299]: val_loss:0.382635,val_acc:0.860322,val_total:4539\n",
      "[epoch:14,batch:329]:acc: 0.927178,loss:0.185940\n",
      "[epoch:14,batch:359]:acc: 0.925955,loss:0.187493\n",
      "[epoch:14,batch:389]:acc: 0.925240,loss:0.188513\n",
      "[epoch:14,batch:419]:acc: 0.925223,loss:0.188277\n",
      "[epoch:14,batch:449]:acc: 0.926389,loss:0.185967\n",
      "[epoch:14,batch:479]:acc: 0.925586,loss:0.186388\n",
      "[epoch:14,batch:509]:acc: 0.925858,loss:0.185765\n",
      "[epoch:14,batch:539]:acc: 0.925347,loss:0.186683\n",
      "[epoch:14,batch:569]:acc: 0.925329,loss:0.186867\n",
      "[epoch:14,batch:599]:acc: 0.925729,loss:0.186247\n",
      "[epoch:14,batch:599]: val_loss:0.374961,val_acc:0.864067,val_total:4539\n",
      "[epoch:14,batch:629]:acc: 0.925645,loss:0.185795\n",
      "[epoch:14,batch:659]:acc: 0.925426,loss:0.186637\n",
      "[epoch:14,batch:689]:acc: 0.925045,loss:0.187026\n",
      "[epoch:14,batch:719]:acc: 0.925000,loss:0.186721\n",
      "[epoch:14,batch:749]:acc: 0.925167,loss:0.186451\n",
      "[epoch:14,batch:779]:acc: 0.924639,loss:0.186952\n",
      "[epoch:14,batch:809]:acc: 0.924614,loss:0.187168\n",
      "[epoch:14,batch:839]:acc: 0.923921,loss:0.188118\n",
      "[epoch:14,batch:869]:acc: 0.924389,loss:0.187821\n",
      "[epoch:14,batch:899]:acc: 0.924410,loss:0.187941\n",
      "[epoch:14,batch:899]: val_loss:0.378849,val_acc:0.861423,val_total:4539\n",
      "[epoch:14,batch:929]:acc: 0.924328,loss:0.188050\n",
      "[epoch:14,batch:959]:acc: 0.923926,loss:0.188297\n",
      "[epoch:14,batch:989]:acc: 0.924274,loss:0.187709\n",
      "[epoch:14] :acc: 0.924268,loss:0.187975,lr:0.000000,patience:1\n",
      "[epoch:14]: val_loss:0.377246,val_acc:0.861203,\n",
      "save new model loss,now loss is  0.37724581360816956\n",
      "Epoch 15/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 0 \n",
      "[epoch:15,batch:29]:acc: 0.922917,loss:0.195209\n",
      "[epoch:15,batch:59]:acc: 0.921354,loss:0.193459\n",
      "[epoch:15,batch:89]:acc: 0.921528,loss:0.192140\n",
      "[epoch:15,batch:119]:acc: 0.920052,loss:0.196144\n",
      "[epoch:15,batch:149]:acc: 0.919583,loss:0.195880\n",
      "[epoch:15,batch:179]:acc: 0.918403,loss:0.195102\n",
      "[epoch:15,batch:209]:acc: 0.919643,loss:0.193721\n",
      "[epoch:15,batch:239]:acc: 0.919661,loss:0.194120\n",
      "[epoch:15,batch:269]:acc: 0.920833,loss:0.191780\n",
      "[epoch:15,batch:299]:acc: 0.923333,loss:0.188687\n",
      "[epoch:15,batch:299]: val_loss:0.373823,val_acc:0.861864,val_total:4539\n",
      "[epoch:15,batch:329]:acc: 0.924527,loss:0.187473\n",
      "[epoch:15,batch:359]:acc: 0.925260,loss:0.186051\n",
      "[epoch:15,batch:389]:acc: 0.924920,loss:0.186854\n",
      "[epoch:15,batch:419]:acc: 0.924702,loss:0.186898\n",
      "[epoch:15,batch:449]:acc: 0.924861,loss:0.186868\n",
      "[epoch:15,batch:479]:acc: 0.924740,loss:0.187202\n",
      "[epoch:15,batch:509]:acc: 0.925000,loss:0.186939\n",
      "[epoch:15,batch:539]:acc: 0.924711,loss:0.188110\n",
      "[epoch:15,batch:569]:acc: 0.925329,loss:0.187181\n",
      "[epoch:15,batch:599]:acc: 0.925573,loss:0.187362\n",
      "[epoch:15,batch:599]: val_loss:0.375318,val_acc:0.863186,val_total:4539\n",
      "[epoch:15,batch:629]:acc: 0.925446,loss:0.187307\n",
      "[epoch:15,batch:659]:acc: 0.924716,loss:0.188196\n",
      "[epoch:15,batch:689]:acc: 0.924683,loss:0.188710\n",
      "[epoch:15,batch:719]:acc: 0.924696,loss:0.188275\n",
      "[epoch:15,batch:749]:acc: 0.924958,loss:0.188259\n",
      "[epoch:15,batch:779]:acc: 0.924880,loss:0.188212\n",
      "[epoch:15,batch:809]:acc: 0.925386,loss:0.187535\n",
      "[epoch:15,batch:839]:acc: 0.925037,loss:0.188306\n",
      "[epoch:15,batch:869]:acc: 0.924928,loss:0.188435\n",
      "[epoch:15,batch:899]:acc: 0.925000,loss:0.188447\n",
      "[epoch:15,batch:899]: val_loss:0.375533,val_acc:0.862525,val_total:4539\n",
      "[epoch:15,batch:929]:acc: 0.924966,loss:0.188448\n",
      "[epoch:15,batch:959]:acc: 0.924870,loss:0.188413\n",
      "[epoch:15,batch:989]:acc: 0.924842,loss:0.188486\n",
      "[epoch:15] :acc: 0.924772,loss:0.188584,lr:0.000000,patience:0\n",
      "[epoch:15]: val_loss:0.379119,val_acc:0.860983,\n",
      "Epoch 16/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 1 \n",
      "[epoch:16,batch:29]:acc: 0.925000,loss:0.185732\n",
      "[epoch:16,batch:59]:acc: 0.930729,loss:0.182704\n",
      "[epoch:16,batch:89]:acc: 0.925000,loss:0.181840\n",
      "[epoch:16,batch:119]:acc: 0.923698,loss:0.185773\n",
      "[epoch:16,batch:149]:acc: 0.923125,loss:0.186777\n",
      "[epoch:16,batch:179]:acc: 0.925868,loss:0.182319\n",
      "[epoch:16,batch:209]:acc: 0.925744,loss:0.182987\n",
      "[epoch:16,batch:239]:acc: 0.925000,loss:0.184215\n",
      "[epoch:16,batch:269]:acc: 0.925810,loss:0.182194\n",
      "[epoch:16,batch:299]:acc: 0.925000,loss:0.183886\n",
      "[epoch:16,batch:299]: val_loss:0.379376,val_acc:0.860983,val_total:4539\n",
      "[epoch:16,batch:329]:acc: 0.924905,loss:0.184051\n",
      "[epoch:16,batch:359]:acc: 0.924740,loss:0.183702\n",
      "[epoch:16,batch:389]:acc: 0.924279,loss:0.184506\n",
      "[epoch:16,batch:419]:acc: 0.924777,loss:0.184087\n",
      "[epoch:16,batch:449]:acc: 0.924931,loss:0.184534\n",
      "[epoch:16,batch:479]:acc: 0.924870,loss:0.186060\n",
      "[epoch:16,batch:509]:acc: 0.924939,loss:0.185963\n",
      "[epoch:16,batch:539]:acc: 0.923553,loss:0.187406\n",
      "[epoch:16,batch:569]:acc: 0.922917,loss:0.188511\n",
      "[epoch:16,batch:599]:acc: 0.923229,loss:0.188728\n",
      "[epoch:16,batch:599]: val_loss:0.373281,val_acc:0.864508,val_total:4539\n",
      "[epoch:16,batch:629]:acc: 0.923611,loss:0.189139\n",
      "[epoch:16,batch:659]:acc: 0.924195,loss:0.188488\n",
      "[epoch:16,batch:689]:acc: 0.923822,loss:0.189483\n",
      "[epoch:16,batch:719]:acc: 0.924219,loss:0.189184\n",
      "[epoch:16,batch:749]:acc: 0.924708,loss:0.188454\n",
      "[epoch:16,batch:779]:acc: 0.924800,loss:0.188255\n",
      "[epoch:16,batch:809]:acc: 0.924846,loss:0.187979\n",
      "[epoch:16,batch:839]:acc: 0.925112,loss:0.187451\n",
      "[epoch:16,batch:869]:acc: 0.925180,loss:0.187189\n",
      "[epoch:16,batch:899]:acc: 0.924931,loss:0.187549\n",
      "[epoch:16,batch:899]: val_loss:0.374919,val_acc:0.862084,val_total:4539\n",
      "[epoch:16,batch:929]:acc: 0.924798,loss:0.187637\n",
      "[epoch:16,batch:959]:acc: 0.924870,loss:0.188119\n",
      "[epoch:16,batch:989]:acc: 0.925032,loss:0.188143\n",
      "[epoch:16] :acc: 0.924961,loss:0.188998,lr:0.000000,patience:1\n",
      "[epoch:16]: val_loss:0.377916,val_acc:0.862965,\n",
      "Epoch 17/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 0 \n",
      "[epoch:17,batch:29]:acc: 0.933333,loss:0.172021\n",
      "[epoch:17,batch:59]:acc: 0.932292,loss:0.178632\n",
      "[epoch:17,batch:89]:acc: 0.929861,loss:0.181606\n",
      "[epoch:17,batch:119]:acc: 0.929688,loss:0.181633\n",
      "[epoch:17,batch:149]:acc: 0.931250,loss:0.178943\n",
      "[epoch:17,batch:179]:acc: 0.930729,loss:0.180906\n",
      "[epoch:17,batch:209]:acc: 0.928720,loss:0.183534\n",
      "[epoch:17,batch:239]:acc: 0.929557,loss:0.182894\n",
      "[epoch:17,batch:269]:acc: 0.927083,loss:0.186442\n",
      "[epoch:17,batch:299]:acc: 0.927292,loss:0.186663\n",
      "[epoch:17,batch:299]: val_loss:0.376842,val_acc:0.859881,val_total:4539\n",
      "[epoch:17,batch:329]:acc: 0.927273,loss:0.186670\n",
      "[epoch:17,batch:359]:acc: 0.926389,loss:0.188266\n",
      "[epoch:17,batch:389]:acc: 0.926522,loss:0.187291\n",
      "[epoch:17,batch:419]:acc: 0.926042,loss:0.187510\n",
      "[epoch:17,batch:449]:acc: 0.925139,loss:0.188320\n",
      "[epoch:17,batch:479]:acc: 0.925651,loss:0.186989\n",
      "[epoch:17,batch:509]:acc: 0.925797,loss:0.186558\n",
      "[epoch:17,batch:539]:acc: 0.925868,loss:0.187171\n",
      "[epoch:17,batch:569]:acc: 0.925987,loss:0.186504\n",
      "[epoch:17,batch:599]:acc: 0.926198,loss:0.185409\n",
      "[epoch:17,batch:599]: val_loss:0.375550,val_acc:0.862084,val_total:4539\n",
      "[epoch:17,batch:629]:acc: 0.925893,loss:0.185960\n",
      "[epoch:17,batch:659]:acc: 0.925237,loss:0.187091\n",
      "[epoch:17,batch:689]:acc: 0.925000,loss:0.187773\n",
      "[epoch:17,batch:719]:acc: 0.925087,loss:0.188206\n",
      "[epoch:17,batch:749]:acc: 0.925083,loss:0.187941\n",
      "[epoch:17,batch:779]:acc: 0.924159,loss:0.189515\n",
      "[epoch:17,batch:809]:acc: 0.924228,loss:0.189303\n",
      "[epoch:17,batch:839]:acc: 0.923438,loss:0.190357\n",
      "[epoch:17,batch:869]:acc: 0.923455,loss:0.190734\n",
      "[epoch:17,batch:899]:acc: 0.923472,loss:0.190689\n",
      "[epoch:17,batch:899]: val_loss:0.375687,val_acc:0.862965,val_total:4539\n",
      "[epoch:17,batch:929]:acc: 0.923757,loss:0.190714\n",
      "[epoch:17,batch:959]:acc: 0.923861,loss:0.190433\n",
      "[epoch:17,batch:989]:acc: 0.923832,loss:0.190312\n",
      "[epoch:17] :acc: 0.923700,loss:0.190369,lr:0.000000,patience:0\n",
      "[epoch:17]: val_loss:0.380948,val_acc:0.862304,\n",
      "Epoch 18/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 1 \n",
      "[epoch:18,batch:29]:acc: 0.917708,loss:0.220556\n",
      "[epoch:18,batch:59]:acc: 0.916667,loss:0.204053\n",
      "[epoch:18,batch:89]:acc: 0.918056,loss:0.197768\n",
      "[epoch:18,batch:119]:acc: 0.920312,loss:0.195166\n",
      "[epoch:18,batch:149]:acc: 0.918958,loss:0.199174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:18,batch:179]:acc: 0.920660,loss:0.195902\n",
      "[epoch:18,batch:209]:acc: 0.920833,loss:0.194000\n",
      "[epoch:18,batch:239]:acc: 0.921745,loss:0.194296\n",
      "[epoch:18,batch:269]:acc: 0.922222,loss:0.194354\n",
      "[epoch:18,batch:299]:acc: 0.922604,loss:0.193689\n",
      "[epoch:18,batch:299]: val_loss:0.375768,val_acc:0.862304,val_total:4539\n",
      "[epoch:18,batch:329]:acc: 0.923011,loss:0.193240\n",
      "[epoch:18,batch:359]:acc: 0.923438,loss:0.192099\n",
      "[epoch:18,batch:389]:acc: 0.923718,loss:0.191717\n",
      "[epoch:18,batch:419]:acc: 0.923512,loss:0.191998\n",
      "[epoch:18,batch:449]:acc: 0.922361,loss:0.193631\n",
      "[epoch:18,batch:479]:acc: 0.923047,loss:0.192387\n",
      "[epoch:18,batch:509]:acc: 0.922855,loss:0.192868\n",
      "[epoch:18,batch:539]:acc: 0.922512,loss:0.192773\n",
      "[epoch:18,batch:569]:acc: 0.923410,loss:0.192328\n",
      "[epoch:18,batch:599]:acc: 0.923594,loss:0.192127\n",
      "[epoch:18,batch:599]: val_loss:0.373807,val_acc:0.862965,val_total:4539\n",
      "[epoch:18,batch:629]:acc: 0.923958,loss:0.191853\n",
      "[epoch:18,batch:659]:acc: 0.924148,loss:0.192031\n",
      "[epoch:18,batch:689]:acc: 0.924411,loss:0.191503\n",
      "[epoch:18,batch:719]:acc: 0.923915,loss:0.191394\n",
      "[epoch:18,batch:749]:acc: 0.924042,loss:0.191082\n",
      "[epoch:18,batch:779]:acc: 0.924439,loss:0.190809\n",
      "[epoch:18,batch:809]:acc: 0.924228,loss:0.191610\n",
      "[epoch:18,batch:839]:acc: 0.924516,loss:0.190770\n",
      "[epoch:18,batch:869]:acc: 0.924461,loss:0.190904\n",
      "[epoch:18,batch:899]:acc: 0.924306,loss:0.190533\n",
      "[epoch:18,batch:899]: val_loss:0.374119,val_acc:0.864728,val_total:4539\n",
      "[epoch:18,batch:929]:acc: 0.924765,loss:0.189787\n",
      "[epoch:18,batch:959]:acc: 0.924707,loss:0.189907\n",
      "[epoch:18,batch:989]:acc: 0.925347,loss:0.189029\n",
      "[epoch:18] :acc: 0.925340,loss:0.188839,lr:0.000000,patience:1\n",
      "[epoch:18]: val_loss:0.379635,val_acc:0.863406,\n",
      "Epoch 19/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 0 \n",
      "[epoch:19,batch:29]:acc: 0.946875,loss:0.157182\n",
      "[epoch:19,batch:59]:acc: 0.934375,loss:0.177859\n",
      "[epoch:19,batch:89]:acc: 0.930556,loss:0.183080\n",
      "[epoch:19,batch:119]:acc: 0.930990,loss:0.179179\n",
      "[epoch:19,batch:149]:acc: 0.930000,loss:0.180117\n",
      "[epoch:19,batch:179]:acc: 0.926215,loss:0.183629\n",
      "[epoch:19,batch:209]:acc: 0.927083,loss:0.182662\n",
      "[epoch:19,batch:239]:acc: 0.926432,loss:0.185080\n",
      "[epoch:19,batch:269]:acc: 0.926505,loss:0.183627\n",
      "[epoch:19,batch:299]:acc: 0.925625,loss:0.185501\n",
      "[epoch:19,batch:299]: val_loss:0.374867,val_acc:0.861643,val_total:4539\n",
      "[epoch:19,batch:329]:acc: 0.925758,loss:0.184911\n",
      "[epoch:19,batch:359]:acc: 0.925608,loss:0.185508\n",
      "[epoch:19,batch:389]:acc: 0.924119,loss:0.187576\n",
      "[epoch:19,batch:419]:acc: 0.923586,loss:0.188199\n",
      "[epoch:19,batch:449]:acc: 0.922986,loss:0.189556\n",
      "[epoch:19,batch:479]:acc: 0.923177,loss:0.189676\n",
      "[epoch:19,batch:509]:acc: 0.922243,loss:0.190699\n",
      "[epoch:19,batch:539]:acc: 0.922222,loss:0.191573\n",
      "[epoch:19,batch:569]:acc: 0.922752,loss:0.190559\n",
      "[epoch:19,batch:599]:acc: 0.922917,loss:0.190381\n",
      "[epoch:19,batch:599]: val_loss:0.378411,val_acc:0.861864,val_total:4539\n",
      "[epoch:19,batch:629]:acc: 0.922768,loss:0.190220\n",
      "[epoch:19,batch:659]:acc: 0.923153,loss:0.189312\n",
      "[epoch:19,batch:689]:acc: 0.923324,loss:0.188985\n",
      "[epoch:19,batch:719]:acc: 0.923438,loss:0.188952\n",
      "[epoch:19,batch:749]:acc: 0.924125,loss:0.188076\n",
      "[epoch:19,batch:779]:acc: 0.924079,loss:0.188090\n",
      "[epoch:19,batch:809]:acc: 0.923032,loss:0.189508\n",
      "[epoch:19,batch:839]:acc: 0.923177,loss:0.189666\n",
      "[epoch:19,batch:869]:acc: 0.923707,loss:0.189042\n",
      "[epoch:19,batch:899]:acc: 0.923854,loss:0.189184\n",
      "[epoch:19,batch:899]: val_loss:0.375896,val_acc:0.860322,val_total:4539\n",
      "[epoch:19,batch:929]:acc: 0.923320,loss:0.189814\n",
      "[epoch:19,batch:959]:acc: 0.923307,loss:0.189801\n",
      "[epoch:19,batch:989]:acc: 0.923516,loss:0.189362\n",
      "[epoch:19] :acc: 0.923511,loss:0.189481,lr:0.000000,patience:0\n",
      "[epoch:19]: val_loss:0.377594,val_acc:0.861643,\n",
      "Epoch 20/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 1 \n",
      "[epoch:20,batch:29]:acc: 0.919792,loss:0.187904\n",
      "[epoch:20,batch:59]:acc: 0.926042,loss:0.178745\n",
      "[epoch:20,batch:89]:acc: 0.924653,loss:0.180988\n",
      "[epoch:20,batch:119]:acc: 0.923958,loss:0.181757\n",
      "[epoch:20,batch:149]:acc: 0.924375,loss:0.179965\n",
      "[epoch:20,batch:179]:acc: 0.924132,loss:0.181497\n",
      "[epoch:20,batch:209]:acc: 0.925446,loss:0.183243\n",
      "[epoch:20,batch:239]:acc: 0.926823,loss:0.182580\n",
      "[epoch:20,batch:269]:acc: 0.926968,loss:0.183122\n",
      "[epoch:20,batch:299]:acc: 0.926250,loss:0.184338\n",
      "[epoch:20,batch:299]: val_loss:0.375212,val_acc:0.862084,val_total:4539\n",
      "[epoch:20,batch:329]:acc: 0.926799,loss:0.184427\n",
      "[epoch:20,batch:359]:acc: 0.925608,loss:0.186381\n",
      "[epoch:20,batch:389]:acc: 0.925641,loss:0.185662\n",
      "[epoch:20,batch:419]:acc: 0.924554,loss:0.188012\n",
      "[epoch:20,batch:449]:acc: 0.924583,loss:0.188398\n",
      "[epoch:20,batch:479]:acc: 0.924284,loss:0.188511\n",
      "[epoch:20,batch:509]:acc: 0.924142,loss:0.188441\n",
      "[epoch:20,batch:539]:acc: 0.924537,loss:0.187378\n",
      "[epoch:20,batch:569]:acc: 0.924397,loss:0.187693\n",
      "[epoch:20,batch:599]:acc: 0.924219,loss:0.187895\n",
      "[epoch:20,batch:599]: val_loss:0.377579,val_acc:0.859881,val_total:4539\n",
      "[epoch:20,batch:629]:acc: 0.923958,loss:0.188062\n",
      "[epoch:20,batch:659]:acc: 0.923438,loss:0.189357\n",
      "[epoch:20,batch:689]:acc: 0.923641,loss:0.188837\n",
      "[epoch:20,batch:719]:acc: 0.923958,loss:0.188473\n",
      "[epoch:20,batch:749]:acc: 0.924083,loss:0.188513\n",
      "[epoch:20,batch:779]:acc: 0.923878,loss:0.189388\n",
      "[epoch:20,batch:809]:acc: 0.924151,loss:0.188789\n",
      "[epoch:20,batch:839]:acc: 0.923698,loss:0.189513\n",
      "[epoch:20,batch:869]:acc: 0.923527,loss:0.189726\n",
      "[epoch:20,batch:899]:acc: 0.923854,loss:0.189125\n",
      "[epoch:20,batch:899]: val_loss:0.373611,val_acc:0.863186,val_total:4539\n",
      "[epoch:20,batch:929]:acc: 0.923925,loss:0.188795\n",
      "[epoch:20,batch:959]:acc: 0.924316,loss:0.188721\n",
      "[epoch:20,batch:989]:acc: 0.924463,loss:0.188474\n",
      "[epoch:20] :acc: 0.924520,loss:0.188424,lr:0.000000,patience:1\n",
      "[epoch:20]: val_loss:0.375886,val_acc:0.861203,\n",
      "save new model loss,now loss is  0.3758857548236847\n",
      "Epoch 21/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 0 \n",
      "[epoch:21,batch:29]:acc: 0.919792,loss:0.196160\n",
      "[epoch:21,batch:59]:acc: 0.925000,loss:0.187488\n",
      "[epoch:21,batch:89]:acc: 0.922917,loss:0.192308\n",
      "[epoch:21,batch:119]:acc: 0.927083,loss:0.185358\n",
      "[epoch:21,batch:149]:acc: 0.928333,loss:0.183536\n",
      "[epoch:21,batch:179]:acc: 0.928125,loss:0.183243\n",
      "[epoch:21,batch:209]:acc: 0.926488,loss:0.185778\n",
      "[epoch:21,batch:239]:acc: 0.925391,loss:0.186795\n",
      "[epoch:21,batch:269]:acc: 0.926505,loss:0.184453\n",
      "[epoch:21,batch:299]:acc: 0.926875,loss:0.184729\n",
      "[epoch:21,batch:299]: val_loss:0.375816,val_acc:0.863186,val_total:4539\n",
      "[epoch:21,batch:329]:acc: 0.926420,loss:0.184418\n",
      "[epoch:21,batch:359]:acc: 0.926215,loss:0.184608\n",
      "[epoch:21,batch:389]:acc: 0.926202,loss:0.185172\n",
      "[epoch:21,batch:419]:acc: 0.926265,loss:0.185515\n",
      "[epoch:21,batch:449]:acc: 0.926111,loss:0.186337\n",
      "[epoch:21,batch:479]:acc: 0.925911,loss:0.187559\n",
      "[epoch:21,batch:509]:acc: 0.925184,loss:0.188504\n",
      "[epoch:21,batch:539]:acc: 0.924826,loss:0.188542\n",
      "[epoch:21,batch:569]:acc: 0.925110,loss:0.188437\n",
      "[epoch:21,batch:599]:acc: 0.926042,loss:0.186797\n",
      "[epoch:21,batch:599]: val_loss:0.376608,val_acc:0.861203,val_total:4539\n",
      "[epoch:21,batch:629]:acc: 0.925496,loss:0.188159\n",
      "[epoch:21,batch:659]:acc: 0.925284,loss:0.188881\n",
      "[epoch:21,batch:689]:acc: 0.925000,loss:0.188927\n",
      "[epoch:21,batch:719]:acc: 0.924957,loss:0.188734\n",
      "[epoch:21,batch:749]:acc: 0.924792,loss:0.188833\n",
      "[epoch:21,batch:779]:acc: 0.924559,loss:0.189603\n",
      "[epoch:21,batch:809]:acc: 0.925039,loss:0.188827\n",
      "[epoch:21,batch:839]:acc: 0.925260,loss:0.188640\n",
      "[epoch:21,batch:869]:acc: 0.925000,loss:0.189035\n",
      "[epoch:21,batch:899]:acc: 0.924826,loss:0.189262\n",
      "[epoch:21,batch:899]: val_loss:0.376410,val_acc:0.860322,val_total:4539\n",
      "[epoch:21,batch:929]:acc: 0.925000,loss:0.189238\n",
      "[epoch:21,batch:959]:acc: 0.924935,loss:0.189357\n",
      "[epoch:21,batch:989]:acc: 0.924905,loss:0.189000\n",
      "[epoch:21] :acc: 0.924867,loss:0.189402,lr:0.000000,patience:0\n",
      "[epoch:21]: val_loss:0.377045,val_acc:0.860983,\n",
      "Epoch 22/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 1 \n",
      "[epoch:22,batch:29]:acc: 0.925000,loss:0.190525\n",
      "[epoch:22,batch:59]:acc: 0.928646,loss:0.184001\n",
      "[epoch:22,batch:89]:acc: 0.929514,loss:0.180313\n",
      "[epoch:22,batch:119]:acc: 0.927344,loss:0.187794\n",
      "[epoch:22,batch:149]:acc: 0.927708,loss:0.186103\n",
      "[epoch:22,batch:179]:acc: 0.927951,loss:0.185573\n",
      "[epoch:22,batch:209]:acc: 0.927679,loss:0.184851\n",
      "[epoch:22,batch:239]:acc: 0.929427,loss:0.183163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:22,batch:269]:acc: 0.929398,loss:0.183706\n",
      "[epoch:22,batch:299]:acc: 0.930104,loss:0.182740\n",
      "[epoch:22,batch:299]: val_loss:0.376354,val_acc:0.862084,val_total:4539\n",
      "[epoch:22,batch:329]:acc: 0.929545,loss:0.182936\n",
      "[epoch:22,batch:359]:acc: 0.929948,loss:0.182571\n",
      "[epoch:22,batch:389]:acc: 0.929087,loss:0.183027\n",
      "[epoch:22,batch:419]:acc: 0.929018,loss:0.183022\n",
      "[epoch:22,batch:449]:acc: 0.929514,loss:0.181700\n",
      "[epoch:22,batch:479]:acc: 0.928516,loss:0.182915\n",
      "[epoch:22,batch:509]:acc: 0.928309,loss:0.182811\n",
      "[epoch:22,batch:539]:acc: 0.927373,loss:0.183525\n",
      "[epoch:22,batch:569]:acc: 0.927083,loss:0.184545\n",
      "[epoch:22,batch:599]:acc: 0.926771,loss:0.184803\n",
      "[epoch:22,batch:599]: val_loss:0.375380,val_acc:0.860983,val_total:4539\n",
      "[epoch:22,batch:629]:acc: 0.926736,loss:0.184800\n",
      "[epoch:22,batch:659]:acc: 0.927036,loss:0.184552\n",
      "[epoch:22,batch:689]:acc: 0.927400,loss:0.183898\n",
      "[epoch:22,batch:719]:acc: 0.927517,loss:0.183655\n",
      "[epoch:22,batch:749]:acc: 0.926875,loss:0.184496\n",
      "[epoch:22,batch:779]:acc: 0.927163,loss:0.184675\n",
      "[epoch:22,batch:809]:acc: 0.926736,loss:0.185141\n",
      "[epoch:22,batch:839]:acc: 0.926674,loss:0.185564\n",
      "[epoch:22,batch:869]:acc: 0.926652,loss:0.186177\n",
      "[epoch:22,batch:899]:acc: 0.925903,loss:0.187027\n",
      "[epoch:22,batch:899]: val_loss:0.378693,val_acc:0.860322,val_total:4539\n",
      "[epoch:22,batch:929]:acc: 0.925874,loss:0.187030\n",
      "[epoch:22,batch:959]:acc: 0.925684,loss:0.187638\n",
      "[epoch:22,batch:989]:acc: 0.925789,loss:0.187261\n",
      "[epoch:22] :acc: 0.925781,loss:0.187336,lr:0.000000,patience:1\n",
      "[epoch:22]: val_loss:0.380357,val_acc:0.858119,\n",
      "Epoch 23/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 0 \n",
      "[epoch:23,batch:29]:acc: 0.928125,loss:0.176569\n",
      "[epoch:23,batch:59]:acc: 0.924479,loss:0.185248\n",
      "[epoch:23,batch:89]:acc: 0.927778,loss:0.178307\n",
      "[epoch:23,batch:119]:acc: 0.925521,loss:0.178636\n",
      "[epoch:23,batch:149]:acc: 0.926667,loss:0.176387\n",
      "[epoch:23,batch:179]:acc: 0.925868,loss:0.177064\n",
      "[epoch:23,batch:209]:acc: 0.925595,loss:0.179476\n",
      "[epoch:23,batch:239]:acc: 0.927214,loss:0.177463\n",
      "[epoch:23,batch:269]:acc: 0.925116,loss:0.179870\n",
      "[epoch:23,batch:299]:acc: 0.925937,loss:0.180694\n",
      "[epoch:23,batch:299]: val_loss:0.376449,val_acc:0.862304,val_total:4539\n",
      "[epoch:23,batch:329]:acc: 0.924811,loss:0.182293\n",
      "[epoch:23,batch:359]:acc: 0.925694,loss:0.182566\n",
      "[epoch:23,batch:389]:acc: 0.923397,loss:0.185390\n",
      "[epoch:23,batch:419]:acc: 0.924033,loss:0.184878\n",
      "[epoch:23,batch:449]:acc: 0.923681,loss:0.186052\n",
      "[epoch:23,batch:479]:acc: 0.924284,loss:0.186011\n",
      "[epoch:23,batch:509]:acc: 0.924081,loss:0.186050\n",
      "[epoch:23,batch:539]:acc: 0.924769,loss:0.185716\n",
      "[epoch:23,batch:569]:acc: 0.924452,loss:0.185814\n",
      "[epoch:23,batch:599]:acc: 0.924531,loss:0.185704\n",
      "[epoch:23,batch:599]: val_loss:0.378254,val_acc:0.860322,val_total:4539\n",
      "[epoch:23,batch:629]:acc: 0.923562,loss:0.187281\n",
      "[epoch:23,batch:659]:acc: 0.923958,loss:0.187228\n",
      "[epoch:23,batch:689]:acc: 0.924411,loss:0.186585\n",
      "[epoch:23,batch:719]:acc: 0.924436,loss:0.186510\n",
      "[epoch:23,batch:749]:acc: 0.924917,loss:0.186414\n",
      "[epoch:23,batch:779]:acc: 0.925641,loss:0.185237\n",
      "[epoch:23,batch:809]:acc: 0.925617,loss:0.185075\n",
      "[epoch:23,batch:839]:acc: 0.925037,loss:0.185803\n",
      "[epoch:23,batch:869]:acc: 0.925826,loss:0.185179\n",
      "[epoch:23,batch:899]:acc: 0.925694,loss:0.185108\n",
      "[epoch:23,batch:899]: val_loss:0.379472,val_acc:0.860322,val_total:4539\n",
      "[epoch:23,batch:929]:acc: 0.925168,loss:0.186083\n",
      "[epoch:23,batch:959]:acc: 0.925293,loss:0.186154\n",
      "[epoch:23,batch:989]:acc: 0.925126,loss:0.186337\n",
      "[epoch:23] :acc: 0.925151,loss:0.186278,lr:0.000000,patience:0\n",
      "[epoch:23]: val_loss:0.376883,val_acc:0.862304,\n",
      "Epoch 24/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 1 \n",
      "[epoch:24,batch:29]:acc: 0.935417,loss:0.173347\n",
      "[epoch:24,batch:59]:acc: 0.928646,loss:0.189238\n",
      "[epoch:24,batch:89]:acc: 0.925694,loss:0.186599\n",
      "[epoch:24,batch:119]:acc: 0.923958,loss:0.188101\n",
      "[epoch:24,batch:149]:acc: 0.924583,loss:0.189037\n",
      "[epoch:24,batch:179]:acc: 0.923090,loss:0.190585\n",
      "[epoch:24,batch:209]:acc: 0.924851,loss:0.188415\n",
      "[epoch:24,batch:239]:acc: 0.924609,loss:0.186829\n",
      "[epoch:24,batch:269]:acc: 0.924421,loss:0.187687\n",
      "[epoch:24,batch:299]:acc: 0.925208,loss:0.188820\n",
      "[epoch:24,batch:299]: val_loss:0.375855,val_acc:0.862965,val_total:4539\n",
      "[epoch:24,batch:329]:acc: 0.925379,loss:0.188082\n",
      "[epoch:24,batch:359]:acc: 0.925694,loss:0.187941\n",
      "[epoch:24,batch:389]:acc: 0.926522,loss:0.186564\n",
      "[epoch:24,batch:419]:acc: 0.925967,loss:0.187123\n",
      "[epoch:24,batch:449]:acc: 0.925417,loss:0.186971\n",
      "[epoch:24,batch:479]:acc: 0.924284,loss:0.187588\n",
      "[epoch:24,batch:509]:acc: 0.924326,loss:0.187442\n",
      "[epoch:24,batch:539]:acc: 0.923843,loss:0.188838\n",
      "[epoch:24,batch:569]:acc: 0.923849,loss:0.188714\n",
      "[epoch:24,batch:599]:acc: 0.923594,loss:0.189711\n",
      "[epoch:24,batch:599]: val_loss:0.375177,val_acc:0.862525,val_total:4539\n",
      "[epoch:24,batch:629]:acc: 0.923760,loss:0.190067\n",
      "[epoch:24,batch:659]:acc: 0.923390,loss:0.190622\n",
      "[epoch:24,batch:689]:acc: 0.923551,loss:0.190585\n",
      "[epoch:24,batch:719]:acc: 0.923698,loss:0.189763\n",
      "[epoch:24,batch:749]:acc: 0.923375,loss:0.190102\n",
      "[epoch:24,batch:779]:acc: 0.923237,loss:0.189564\n",
      "[epoch:24,batch:809]:acc: 0.923071,loss:0.189688\n",
      "[epoch:24,batch:839]:acc: 0.923326,loss:0.189544\n",
      "[epoch:24,batch:869]:acc: 0.923384,loss:0.189485\n",
      "[epoch:24,batch:899]:acc: 0.923542,loss:0.189344\n",
      "[epoch:24,batch:899]: val_loss:0.376644,val_acc:0.862084,val_total:4539\n",
      "[epoch:24,batch:929]:acc: 0.923589,loss:0.189282\n",
      "[epoch:24,batch:959]:acc: 0.923372,loss:0.189517\n",
      "[epoch:24,batch:989]:acc: 0.923548,loss:0.189159\n",
      "[epoch:24] :acc: 0.923606,loss:0.189029,lr:0.000000,patience:1\n",
      "[epoch:24]: val_loss:0.373792,val_acc:0.862745,\n",
      "save new model loss,now loss is  0.37379202246665955\n",
      "Epoch 25/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 0 \n",
      "[epoch:25,batch:29]:acc: 0.927083,loss:0.179648\n",
      "[epoch:25,batch:59]:acc: 0.929167,loss:0.181551\n",
      "[epoch:25,batch:89]:acc: 0.921528,loss:0.193485\n",
      "[epoch:25,batch:119]:acc: 0.923698,loss:0.188918\n",
      "[epoch:25,batch:149]:acc: 0.920833,loss:0.190017\n",
      "[epoch:25,batch:179]:acc: 0.921528,loss:0.190078\n",
      "[epoch:25,batch:209]:acc: 0.921429,loss:0.190200\n",
      "[epoch:25,batch:239]:acc: 0.920312,loss:0.190099\n",
      "[epoch:25,batch:269]:acc: 0.920486,loss:0.189032\n",
      "[epoch:25,batch:299]:acc: 0.920104,loss:0.188693\n",
      "[epoch:25,batch:299]: val_loss:0.376657,val_acc:0.862745,val_total:4539\n",
      "[epoch:25,batch:329]:acc: 0.920455,loss:0.189024\n",
      "[epoch:25,batch:359]:acc: 0.920660,loss:0.188319\n",
      "[epoch:25,batch:389]:acc: 0.921314,loss:0.187454\n",
      "[epoch:25,batch:419]:acc: 0.920610,loss:0.189309\n",
      "[epoch:25,batch:449]:acc: 0.920069,loss:0.190592\n",
      "[epoch:25,batch:479]:acc: 0.920117,loss:0.191115\n",
      "[epoch:25,batch:509]:acc: 0.920282,loss:0.190231\n",
      "[epoch:25,batch:539]:acc: 0.919907,loss:0.191574\n",
      "[epoch:25,batch:569]:acc: 0.920175,loss:0.191371\n",
      "[epoch:25,batch:599]:acc: 0.919844,loss:0.191614\n",
      "[epoch:25,batch:599]: val_loss:0.373680,val_acc:0.862304,val_total:4539\n",
      "[epoch:25,batch:629]:acc: 0.920536,loss:0.190817\n",
      "[epoch:25,batch:659]:acc: 0.920881,loss:0.190156\n",
      "[epoch:25,batch:689]:acc: 0.920969,loss:0.189906\n",
      "[epoch:25,batch:719]:acc: 0.920616,loss:0.190685\n",
      "[epoch:25,batch:749]:acc: 0.920917,loss:0.190039\n",
      "[epoch:25,batch:779]:acc: 0.921394,loss:0.189680\n",
      "[epoch:25,batch:809]:acc: 0.921528,loss:0.189578\n",
      "[epoch:25,batch:839]:acc: 0.921726,loss:0.189436\n",
      "[epoch:25,batch:869]:acc: 0.922019,loss:0.188742\n",
      "[epoch:25,batch:899]:acc: 0.921910,loss:0.188942\n",
      "[epoch:25,batch:899]: val_loss:0.374445,val_acc:0.861423,val_total:4539\n",
      "[epoch:25,batch:929]:acc: 0.921942,loss:0.188717\n",
      "[epoch:25,batch:959]:acc: 0.921973,loss:0.188670\n",
      "[epoch:25,batch:989]:acc: 0.921938,loss:0.188793\n",
      "[epoch:25] :acc: 0.921903,loss:0.189068,lr:0.000000,patience:0\n",
      "[epoch:25]: val_loss:0.376653,val_acc:0.863186,\n",
      "Epoch 26/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 1 \n",
      "[epoch:26,batch:29]:acc: 0.919792,loss:0.183921\n",
      "[epoch:26,batch:59]:acc: 0.918750,loss:0.190962\n",
      "[epoch:26,batch:89]:acc: 0.920833,loss:0.189615\n",
      "[epoch:26,batch:119]:acc: 0.919271,loss:0.194414\n",
      "[epoch:26,batch:149]:acc: 0.918750,loss:0.195884\n",
      "[epoch:26,batch:179]:acc: 0.918403,loss:0.195145\n",
      "[epoch:26,batch:209]:acc: 0.918899,loss:0.194417\n",
      "[epoch:26,batch:239]:acc: 0.920052,loss:0.194889\n",
      "[epoch:26,batch:269]:acc: 0.921181,loss:0.193223\n",
      "[epoch:26,batch:299]:acc: 0.920521,loss:0.194102\n",
      "[epoch:26,batch:299]: val_loss:0.373911,val_acc:0.864508,val_total:4539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:26,batch:329]:acc: 0.921212,loss:0.192735\n",
      "[epoch:26,batch:359]:acc: 0.920052,loss:0.194884\n",
      "[epoch:26,batch:389]:acc: 0.920513,loss:0.194238\n",
      "[epoch:26,batch:419]:acc: 0.920982,loss:0.193447\n",
      "[epoch:26,batch:449]:acc: 0.921875,loss:0.193123\n",
      "[epoch:26,batch:479]:acc: 0.921549,loss:0.193953\n",
      "[epoch:26,batch:509]:acc: 0.921385,loss:0.194392\n",
      "[epoch:26,batch:539]:acc: 0.921875,loss:0.193947\n",
      "[epoch:26,batch:569]:acc: 0.922423,loss:0.193227\n",
      "[epoch:26,batch:599]:acc: 0.923490,loss:0.191041\n",
      "[epoch:26,batch:599]: val_loss:0.377411,val_acc:0.859661,val_total:4539\n",
      "[epoch:26,batch:629]:acc: 0.924306,loss:0.190137\n",
      "[epoch:26,batch:659]:acc: 0.924527,loss:0.189665\n",
      "[epoch:26,batch:689]:acc: 0.924457,loss:0.189984\n",
      "[epoch:26,batch:719]:acc: 0.924392,loss:0.190440\n",
      "[epoch:26,batch:749]:acc: 0.924208,loss:0.190591\n",
      "[epoch:26,batch:779]:acc: 0.924159,loss:0.190127\n",
      "[epoch:26,batch:809]:acc: 0.924460,loss:0.189461\n",
      "[epoch:26,batch:839]:acc: 0.924405,loss:0.188784\n",
      "[epoch:26,batch:869]:acc: 0.924174,loss:0.188629\n",
      "[epoch:26,batch:899]:acc: 0.924375,loss:0.188489\n",
      "[epoch:26,batch:899]: val_loss:0.377102,val_acc:0.862304,val_total:4539\n",
      "[epoch:26,batch:929]:acc: 0.924261,loss:0.188736\n",
      "[epoch:26,batch:959]:acc: 0.924316,loss:0.188639\n",
      "[epoch:26,batch:989]:acc: 0.924242,loss:0.189345\n",
      "[epoch:26] :acc: 0.924205,loss:0.189518,lr:0.000000,patience:1\n",
      "[epoch:26]: val_loss:0.378573,val_acc:0.862084,\n",
      "Epoch 27/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 0 \n",
      "[epoch:27,batch:29]:acc: 0.917708,loss:0.213514\n",
      "[epoch:27,batch:59]:acc: 0.916667,loss:0.209055\n",
      "[epoch:27,batch:89]:acc: 0.914583,loss:0.208892\n",
      "[epoch:27,batch:119]:acc: 0.918490,loss:0.199593\n",
      "[epoch:27,batch:149]:acc: 0.923333,loss:0.190943\n",
      "[epoch:27,batch:179]:acc: 0.927257,loss:0.186482\n",
      "[epoch:27,batch:209]:acc: 0.927530,loss:0.184372\n",
      "[epoch:27,batch:239]:acc: 0.924479,loss:0.186948\n",
      "[epoch:27,batch:269]:acc: 0.925000,loss:0.187709\n",
      "[epoch:27,batch:299]:acc: 0.924896,loss:0.187238\n",
      "[epoch:27,batch:299]: val_loss:0.376648,val_acc:0.861864,val_total:4539\n",
      "[epoch:27,batch:329]:acc: 0.923295,loss:0.189699\n",
      "[epoch:27,batch:359]:acc: 0.925174,loss:0.187192\n",
      "[epoch:27,batch:389]:acc: 0.925000,loss:0.188117\n",
      "[epoch:27,batch:419]:acc: 0.924182,loss:0.189234\n",
      "[epoch:27,batch:449]:acc: 0.924306,loss:0.188888\n",
      "[epoch:27,batch:479]:acc: 0.924219,loss:0.188941\n",
      "[epoch:27,batch:509]:acc: 0.923591,loss:0.189024\n",
      "[epoch:27,batch:539]:acc: 0.924074,loss:0.188249\n",
      "[epoch:27,batch:569]:acc: 0.924287,loss:0.188061\n",
      "[epoch:27,batch:599]:acc: 0.925000,loss:0.186499\n",
      "[epoch:27,batch:599]: val_loss:0.376878,val_acc:0.861643,val_total:4539\n",
      "[epoch:27,batch:629]:acc: 0.925446,loss:0.186372\n",
      "[epoch:27,batch:659]:acc: 0.925426,loss:0.186572\n",
      "[epoch:27,batch:689]:acc: 0.925725,loss:0.186481\n",
      "[epoch:27,batch:719]:acc: 0.925521,loss:0.186982\n",
      "[epoch:27,batch:749]:acc: 0.926042,loss:0.186363\n",
      "[epoch:27,batch:779]:acc: 0.926002,loss:0.186152\n",
      "[epoch:27,batch:809]:acc: 0.925926,loss:0.186584\n",
      "[epoch:27,batch:839]:acc: 0.926042,loss:0.186381\n",
      "[epoch:27,batch:869]:acc: 0.926149,loss:0.186477\n",
      "[epoch:27,batch:899]:acc: 0.925868,loss:0.186863\n",
      "[epoch:27,batch:899]: val_loss:0.373516,val_acc:0.863186,val_total:4539\n",
      "[epoch:27,batch:929]:acc: 0.926109,loss:0.186486\n",
      "[epoch:27,batch:959]:acc: 0.926107,loss:0.186651\n",
      "[epoch:27,batch:989]:acc: 0.926484,loss:0.186192\n",
      "[epoch:27] :acc: 0.926475,loss:0.186286,lr:0.000000,patience:0\n",
      "[epoch:27]: val_loss:0.379746,val_acc:0.859000,\n",
      "Epoch 28/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 1 \n",
      "[epoch:28,batch:29]:acc: 0.922917,loss:0.204549\n",
      "[epoch:28,batch:59]:acc: 0.932813,loss:0.185052\n",
      "[epoch:28,batch:89]:acc: 0.927083,loss:0.192464\n",
      "[epoch:28,batch:119]:acc: 0.927344,loss:0.191495\n",
      "[epoch:28,batch:149]:acc: 0.926667,loss:0.193742\n",
      "[epoch:28,batch:179]:acc: 0.925000,loss:0.195753\n",
      "[epoch:28,batch:209]:acc: 0.926190,loss:0.194913\n",
      "[epoch:28,batch:239]:acc: 0.924479,loss:0.194989\n",
      "[epoch:28,batch:269]:acc: 0.923611,loss:0.194666\n",
      "[epoch:28,batch:299]:acc: 0.924479,loss:0.193080\n",
      "[epoch:28,batch:299]: val_loss:0.377005,val_acc:0.860983,val_total:4539\n",
      "[epoch:28,batch:329]:acc: 0.925095,loss:0.192067\n",
      "[epoch:28,batch:359]:acc: 0.924740,loss:0.191007\n",
      "[epoch:28,batch:389]:acc: 0.924599,loss:0.189871\n",
      "[epoch:28,batch:419]:acc: 0.924033,loss:0.189793\n",
      "[epoch:28,batch:449]:acc: 0.924375,loss:0.189005\n",
      "[epoch:28,batch:479]:acc: 0.924935,loss:0.188325\n",
      "[epoch:28,batch:509]:acc: 0.924877,loss:0.187837\n",
      "[epoch:28,batch:539]:acc: 0.924306,loss:0.188085\n",
      "[epoch:28,batch:569]:acc: 0.924013,loss:0.188033\n",
      "[epoch:28,batch:599]:acc: 0.923750,loss:0.188913\n",
      "[epoch:28,batch:599]: val_loss:0.377412,val_acc:0.864067,val_total:4539\n",
      "[epoch:28,batch:629]:acc: 0.923363,loss:0.190032\n",
      "[epoch:28,batch:659]:acc: 0.923722,loss:0.189849\n",
      "[epoch:28,batch:689]:acc: 0.923641,loss:0.190025\n",
      "[epoch:28,batch:719]:acc: 0.923872,loss:0.189598\n",
      "[epoch:28,batch:749]:acc: 0.923458,loss:0.189633\n",
      "[epoch:28,batch:779]:acc: 0.923037,loss:0.189738\n",
      "[epoch:28,batch:809]:acc: 0.923071,loss:0.189690\n",
      "[epoch:28,batch:839]:acc: 0.923289,loss:0.189393\n",
      "[epoch:28,batch:869]:acc: 0.923240,loss:0.189460\n",
      "[epoch:28,batch:899]:acc: 0.923090,loss:0.189896\n",
      "[epoch:28,batch:899]: val_loss:0.373010,val_acc:0.862745,val_total:4539\n",
      "[epoch:28,batch:929]:acc: 0.923085,loss:0.189982\n",
      "[epoch:28,batch:959]:acc: 0.923275,loss:0.189875\n",
      "[epoch:28,batch:989]:acc: 0.923327,loss:0.189890\n",
      "[epoch:28] :acc: 0.923353,loss:0.189794,lr:0.000000,patience:1\n",
      "[epoch:28]: val_loss:0.379953,val_acc:0.858779,\n",
      "Epoch 29/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 0 \n",
      "[epoch:29,batch:29]:acc: 0.936458,loss:0.171978\n",
      "[epoch:29,batch:59]:acc: 0.929688,loss:0.177384\n",
      "[epoch:29,batch:89]:acc: 0.923611,loss:0.184700\n",
      "[epoch:29,batch:119]:acc: 0.924479,loss:0.185596\n",
      "[epoch:29,batch:149]:acc: 0.925417,loss:0.182939\n",
      "[epoch:29,batch:179]:acc: 0.922917,loss:0.186368\n",
      "[epoch:29,batch:209]:acc: 0.925298,loss:0.183636\n",
      "[epoch:29,batch:239]:acc: 0.923177,loss:0.186437\n",
      "[epoch:29,batch:269]:acc: 0.922338,loss:0.187968\n",
      "[epoch:29,batch:299]:acc: 0.921458,loss:0.189736\n",
      "[epoch:29,batch:299]: val_loss:0.374587,val_acc:0.863186,val_total:4539\n",
      "[epoch:29,batch:329]:acc: 0.921402,loss:0.189785\n",
      "[epoch:29,batch:359]:acc: 0.921181,loss:0.190220\n",
      "[epoch:29,batch:389]:acc: 0.922115,loss:0.189025\n",
      "[epoch:29,batch:419]:acc: 0.922768,loss:0.188173\n",
      "[epoch:29,batch:449]:acc: 0.923125,loss:0.187685\n",
      "[epoch:29,batch:479]:acc: 0.923242,loss:0.188247\n",
      "[epoch:29,batch:509]:acc: 0.923529,loss:0.188789\n",
      "[epoch:29,batch:539]:acc: 0.924306,loss:0.187331\n",
      "[epoch:29,batch:569]:acc: 0.924507,loss:0.187834\n",
      "[epoch:29,batch:599]:acc: 0.924323,loss:0.187662\n",
      "[epoch:29,batch:599]: val_loss:0.375515,val_acc:0.862745,val_total:4539\n",
      "[epoch:29,batch:629]:acc: 0.924752,loss:0.186877\n",
      "[epoch:29,batch:659]:acc: 0.925568,loss:0.186591\n",
      "[epoch:29,batch:689]:acc: 0.925362,loss:0.186774\n",
      "[epoch:29,batch:719]:acc: 0.925911,loss:0.186272\n",
      "[epoch:29,batch:749]:acc: 0.926042,loss:0.186413\n",
      "[epoch:29,batch:779]:acc: 0.925962,loss:0.186463\n",
      "[epoch:29,batch:809]:acc: 0.925887,loss:0.186630\n",
      "[epoch:29,batch:839]:acc: 0.925484,loss:0.187154\n",
      "[epoch:29,batch:869]:acc: 0.925323,loss:0.187357\n",
      "[epoch:29,batch:899]:acc: 0.925278,loss:0.187490\n",
      "[epoch:29,batch:899]: val_loss:0.375488,val_acc:0.863626,val_total:4539\n",
      "[epoch:29,batch:929]:acc: 0.925067,loss:0.187663\n",
      "[epoch:29,batch:959]:acc: 0.925228,loss:0.187425\n",
      "[epoch:29,batch:989]:acc: 0.924937,loss:0.187765\n",
      "[epoch:29] :acc: 0.924930,loss:0.188281,lr:0.000000,patience:0\n",
      "[epoch:29]: val_loss:0.379327,val_acc:0.860542,\n",
      "Epoch 30/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 1 \n",
      "[epoch:30,batch:29]:acc: 0.909375,loss:0.224018\n",
      "[epoch:30,batch:59]:acc: 0.913542,loss:0.207244\n",
      "[epoch:30,batch:89]:acc: 0.920486,loss:0.195849\n",
      "[epoch:30,batch:119]:acc: 0.922656,loss:0.191216\n",
      "[epoch:30,batch:149]:acc: 0.921875,loss:0.190802\n",
      "[epoch:30,batch:179]:acc: 0.923611,loss:0.188536\n",
      "[epoch:30,batch:209]:acc: 0.923214,loss:0.189978\n",
      "[epoch:30,batch:239]:acc: 0.923307,loss:0.188132\n",
      "[epoch:30,batch:269]:acc: 0.922917,loss:0.188939\n",
      "[epoch:30,batch:299]:acc: 0.923125,loss:0.188118\n",
      "[epoch:30,batch:299]: val_loss:0.376126,val_acc:0.860322,val_total:4539\n",
      "[epoch:30,batch:329]:acc: 0.923390,loss:0.188693\n",
      "[epoch:30,batch:359]:acc: 0.922743,loss:0.189687\n",
      "[epoch:30,batch:389]:acc: 0.921474,loss:0.191324\n",
      "[epoch:30,batch:419]:acc: 0.922768,loss:0.190288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:30,batch:449]:acc: 0.922708,loss:0.190006\n",
      "[epoch:30,batch:479]:acc: 0.922070,loss:0.190954\n",
      "[epoch:30,batch:509]:acc: 0.922181,loss:0.190865\n",
      "[epoch:30,batch:539]:acc: 0.922512,loss:0.190321\n",
      "[epoch:30,batch:569]:acc: 0.922314,loss:0.190958\n",
      "[epoch:30,batch:599]:acc: 0.922813,loss:0.190600\n",
      "[epoch:30,batch:599]: val_loss:0.374017,val_acc:0.860542,val_total:4539\n",
      "[epoch:30,batch:629]:acc: 0.922222,loss:0.191580\n",
      "[epoch:30,batch:659]:acc: 0.922396,loss:0.192410\n",
      "[epoch:30,batch:689]:acc: 0.922645,loss:0.192019\n",
      "[epoch:30,batch:719]:acc: 0.922266,loss:0.192013\n",
      "[epoch:30,batch:749]:acc: 0.921958,loss:0.192240\n",
      "[epoch:30,batch:779]:acc: 0.922276,loss:0.192385\n",
      "[epoch:30,batch:809]:acc: 0.922492,loss:0.192139\n",
      "[epoch:30,batch:839]:acc: 0.922582,loss:0.191702\n",
      "[epoch:30,batch:869]:acc: 0.922881,loss:0.191378\n",
      "[epoch:30,batch:899]:acc: 0.923056,loss:0.190892\n",
      "[epoch:30,batch:899]: val_loss:0.376285,val_acc:0.860983,val_total:4539\n",
      "[epoch:30,batch:929]:acc: 0.923522,loss:0.190119\n",
      "[epoch:30,batch:959]:acc: 0.923600,loss:0.190184\n",
      "[epoch:30,batch:989]:acc: 0.923453,loss:0.190574\n",
      "[epoch:30] :acc: 0.923448,loss:0.190458,lr:0.000000,patience:1\n",
      "[epoch:30]: val_loss:0.378910,val_acc:0.860762,\n",
      "Epoch 31/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 0 \n",
      "[epoch:31,batch:29]:acc: 0.910417,loss:0.203324\n",
      "[epoch:31,batch:59]:acc: 0.924479,loss:0.198853\n",
      "[epoch:31,batch:89]:acc: 0.923264,loss:0.196985\n",
      "[epoch:31,batch:119]:acc: 0.923958,loss:0.191878\n",
      "[epoch:31,batch:149]:acc: 0.924792,loss:0.189528\n",
      "[epoch:31,batch:179]:acc: 0.922049,loss:0.192649\n",
      "[epoch:31,batch:209]:acc: 0.921577,loss:0.195259\n",
      "[epoch:31,batch:239]:acc: 0.922396,loss:0.192675\n",
      "[epoch:31,batch:269]:acc: 0.921875,loss:0.192898\n",
      "[epoch:31,batch:299]:acc: 0.922083,loss:0.193224\n",
      "[epoch:31,batch:299]: val_loss:0.376083,val_acc:0.862304,val_total:4539\n",
      "[epoch:31,batch:329]:acc: 0.922633,loss:0.192312\n",
      "[epoch:31,batch:359]:acc: 0.923264,loss:0.190913\n",
      "[epoch:31,batch:389]:acc: 0.922917,loss:0.191143\n",
      "[epoch:31,batch:419]:acc: 0.922619,loss:0.192475\n",
      "[epoch:31,batch:449]:acc: 0.923403,loss:0.190749\n",
      "[epoch:31,batch:479]:acc: 0.923307,loss:0.190273\n",
      "[epoch:31,batch:509]:acc: 0.923039,loss:0.190634\n",
      "[epoch:31,batch:539]:acc: 0.922801,loss:0.191571\n",
      "[epoch:31,batch:569]:acc: 0.923191,loss:0.190942\n",
      "[epoch:31,batch:599]:acc: 0.923594,loss:0.189416\n",
      "[epoch:31,batch:599]: val_loss:0.375764,val_acc:0.863847,val_total:4539\n",
      "[epoch:31,batch:629]:acc: 0.924256,loss:0.189094\n",
      "[epoch:31,batch:659]:acc: 0.924716,loss:0.188908\n",
      "[epoch:31,batch:689]:acc: 0.924502,loss:0.188991\n",
      "[epoch:31,batch:719]:acc: 0.924523,loss:0.189020\n",
      "[epoch:31,batch:749]:acc: 0.924500,loss:0.188811\n",
      "[epoch:31,batch:779]:acc: 0.924679,loss:0.188980\n",
      "[epoch:31,batch:809]:acc: 0.925193,loss:0.188273\n",
      "[epoch:31,batch:839]:acc: 0.925037,loss:0.189006\n",
      "[epoch:31,batch:869]:acc: 0.925826,loss:0.188244\n",
      "[epoch:31,batch:899]:acc: 0.926181,loss:0.187746\n",
      "[epoch:31,batch:899]: val_loss:0.376003,val_acc:0.860983,val_total:4539\n",
      "[epoch:31,batch:929]:acc: 0.926075,loss:0.188037\n",
      "[epoch:31,batch:959]:acc: 0.925846,loss:0.188139\n",
      "[epoch:31,batch:989]:acc: 0.925631,loss:0.187874\n",
      "[epoch:31] :acc: 0.925655,loss:0.187824,lr:0.000000,patience:0\n",
      "[epoch:31]: val_loss:0.379501,val_acc:0.862084,\n",
      "Epoch 32/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 1 \n",
      "[epoch:32,batch:29]:acc: 0.914583,loss:0.215865\n",
      "[epoch:32,batch:59]:acc: 0.923958,loss:0.199647\n",
      "[epoch:32,batch:89]:acc: 0.925694,loss:0.194992\n",
      "[epoch:32,batch:119]:acc: 0.927344,loss:0.191762\n",
      "[epoch:32,batch:149]:acc: 0.925625,loss:0.192292\n",
      "[epoch:32,batch:179]:acc: 0.926736,loss:0.190122\n",
      "[epoch:32,batch:209]:acc: 0.926339,loss:0.189679\n",
      "[epoch:32,batch:239]:acc: 0.926042,loss:0.189751\n",
      "[epoch:32,batch:269]:acc: 0.925694,loss:0.190003\n",
      "[epoch:32,batch:299]:acc: 0.925833,loss:0.189295\n",
      "[epoch:32,batch:299]: val_loss:0.377738,val_acc:0.859661,val_total:4539\n",
      "[epoch:32,batch:329]:acc: 0.925189,loss:0.189864\n",
      "[epoch:32,batch:359]:acc: 0.923872,loss:0.191135\n",
      "[epoch:32,batch:389]:acc: 0.924920,loss:0.189170\n",
      "[epoch:32,batch:419]:acc: 0.924554,loss:0.188455\n",
      "[epoch:32,batch:449]:acc: 0.925000,loss:0.187116\n",
      "[epoch:32,batch:479]:acc: 0.924805,loss:0.187605\n",
      "[epoch:32,batch:509]:acc: 0.925123,loss:0.187729\n",
      "[epoch:32,batch:539]:acc: 0.925058,loss:0.187440\n",
      "[epoch:32,batch:569]:acc: 0.925384,loss:0.188106\n",
      "[epoch:32,batch:599]:acc: 0.925469,loss:0.188321\n",
      "[epoch:32,batch:599]: val_loss:0.379594,val_acc:0.858779,val_total:4539\n",
      "[epoch:32,batch:629]:acc: 0.925149,loss:0.188785\n",
      "[epoch:32,batch:659]:acc: 0.925805,loss:0.188196\n",
      "[epoch:32,batch:689]:acc: 0.926178,loss:0.187887\n",
      "[epoch:32,batch:719]:acc: 0.925651,loss:0.187885\n",
      "[epoch:32,batch:749]:acc: 0.925708,loss:0.187332\n",
      "[epoch:32,batch:779]:acc: 0.925761,loss:0.187788\n",
      "[epoch:32,batch:809]:acc: 0.926119,loss:0.187201\n",
      "[epoch:32,batch:839]:acc: 0.925744,loss:0.187781\n",
      "[epoch:32,batch:869]:acc: 0.925718,loss:0.187429\n",
      "[epoch:32,batch:899]:acc: 0.925590,loss:0.187531\n",
      "[epoch:32,batch:899]: val_loss:0.372480,val_acc:0.862745,val_total:4539\n",
      "[epoch:32,batch:929]:acc: 0.925437,loss:0.188084\n",
      "[epoch:32,batch:959]:acc: 0.925293,loss:0.188398\n",
      "[epoch:32,batch:989]:acc: 0.925158,loss:0.188627\n",
      "[epoch:32] :acc: 0.925245,loss:0.188611,lr:0.000000,patience:1\n",
      "[epoch:32]: val_loss:0.378366,val_acc:0.860542,\n",
      "Epoch 33/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 0 \n",
      "[epoch:33,batch:29]:acc: 0.910417,loss:0.201154\n",
      "[epoch:33,batch:59]:acc: 0.917708,loss:0.197639\n",
      "[epoch:33,batch:89]:acc: 0.920833,loss:0.193136\n",
      "[epoch:33,batch:119]:acc: 0.922135,loss:0.190513\n",
      "[epoch:33,batch:149]:acc: 0.924792,loss:0.188397\n",
      "[epoch:33,batch:179]:acc: 0.923958,loss:0.185376\n",
      "[epoch:33,batch:209]:acc: 0.924554,loss:0.186882\n",
      "[epoch:33,batch:239]:acc: 0.924609,loss:0.187073\n",
      "[epoch:33,batch:269]:acc: 0.923264,loss:0.187374\n",
      "[epoch:33,batch:299]:acc: 0.923854,loss:0.186956\n",
      "[epoch:33,batch:299]: val_loss:0.375507,val_acc:0.862745,val_total:4539\n",
      "[epoch:33,batch:329]:acc: 0.924621,loss:0.187074\n",
      "[epoch:33,batch:359]:acc: 0.924306,loss:0.187977\n",
      "[epoch:33,batch:389]:acc: 0.922676,loss:0.190069\n",
      "[epoch:33,batch:419]:acc: 0.921354,loss:0.191911\n",
      "[epoch:33,batch:449]:acc: 0.920833,loss:0.192549\n",
      "[epoch:33,batch:479]:acc: 0.920898,loss:0.192166\n",
      "[epoch:33,batch:509]:acc: 0.921569,loss:0.190874\n",
      "[epoch:33,batch:539]:acc: 0.922049,loss:0.189908\n",
      "[epoch:33,batch:569]:acc: 0.922423,loss:0.189650\n",
      "[epoch:33,batch:599]:acc: 0.922656,loss:0.189128\n",
      "[epoch:33,batch:599]: val_loss:0.374214,val_acc:0.860762,val_total:4539\n",
      "[epoch:33,batch:629]:acc: 0.922669,loss:0.189572\n",
      "[epoch:33,batch:659]:acc: 0.922159,loss:0.189982\n",
      "[epoch:33,batch:689]:acc: 0.922871,loss:0.189036\n",
      "[epoch:33,batch:719]:acc: 0.923047,loss:0.189022\n",
      "[epoch:33,batch:749]:acc: 0.923000,loss:0.189263\n",
      "[epoch:33,batch:779]:acc: 0.922756,loss:0.190050\n",
      "[epoch:33,batch:809]:acc: 0.923264,loss:0.189237\n",
      "[epoch:33,batch:839]:acc: 0.922917,loss:0.189837\n",
      "[epoch:33,batch:869]:acc: 0.923024,loss:0.189778\n",
      "[epoch:33,batch:899]:acc: 0.922882,loss:0.189856\n",
      "[epoch:33,batch:899]: val_loss:0.375605,val_acc:0.861643,val_total:4539\n",
      "[epoch:33,batch:929]:acc: 0.922681,loss:0.190661\n",
      "[epoch:33,batch:959]:acc: 0.923112,loss:0.190003\n",
      "[epoch:33,batch:989]:acc: 0.923422,loss:0.189706\n",
      "[epoch:33] :acc: 0.923353,loss:0.189732,lr:0.000000,patience:0\n",
      "[epoch:33]: val_loss:0.374698,val_acc:0.860101,\n",
      "Epoch 34/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 1 \n",
      "[epoch:34,batch:29]:acc: 0.933333,loss:0.180715\n",
      "[epoch:34,batch:59]:acc: 0.924479,loss:0.186584\n",
      "[epoch:34,batch:89]:acc: 0.928819,loss:0.179734\n",
      "[epoch:34,batch:119]:acc: 0.926823,loss:0.178441\n",
      "[epoch:34,batch:149]:acc: 0.926042,loss:0.184305\n",
      "[epoch:34,batch:179]:acc: 0.925000,loss:0.184813\n",
      "[epoch:34,batch:209]:acc: 0.926935,loss:0.184143\n",
      "[epoch:34,batch:239]:acc: 0.925130,loss:0.185607\n",
      "[epoch:34,batch:269]:acc: 0.925463,loss:0.185319\n",
      "[epoch:34,batch:299]:acc: 0.924687,loss:0.187413\n",
      "[epoch:34,batch:299]: val_loss:0.376448,val_acc:0.861864,val_total:4539\n",
      "[epoch:34,batch:329]:acc: 0.924811,loss:0.187121\n",
      "[epoch:34,batch:359]:acc: 0.925434,loss:0.187104\n",
      "[epoch:34,batch:389]:acc: 0.925801,loss:0.186113\n",
      "[epoch:34,batch:419]:acc: 0.925893,loss:0.185959\n",
      "[epoch:34,batch:449]:acc: 0.925069,loss:0.187627\n",
      "[epoch:34,batch:479]:acc: 0.924740,loss:0.188114\n",
      "[epoch:34,batch:509]:acc: 0.924510,loss:0.188414\n",
      "[epoch:34,batch:539]:acc: 0.923380,loss:0.189697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:34,batch:569]:acc: 0.923739,loss:0.188724\n",
      "[epoch:34,batch:599]:acc: 0.923594,loss:0.189752\n",
      "[epoch:34,batch:599]: val_loss:0.375891,val_acc:0.861643,val_total:4539\n",
      "[epoch:34,batch:629]:acc: 0.924157,loss:0.188861\n",
      "[epoch:34,batch:659]:acc: 0.923816,loss:0.189308\n",
      "[epoch:34,batch:689]:acc: 0.923551,loss:0.189815\n",
      "[epoch:34,batch:719]:acc: 0.923047,loss:0.190547\n",
      "[epoch:34,batch:749]:acc: 0.923167,loss:0.190437\n",
      "[epoch:34,batch:779]:acc: 0.923197,loss:0.190270\n",
      "[epoch:34,batch:809]:acc: 0.923727,loss:0.189407\n",
      "[epoch:34,batch:839]:acc: 0.923884,loss:0.189024\n",
      "[epoch:34,batch:869]:acc: 0.923707,loss:0.189481\n",
      "[epoch:34,batch:899]:acc: 0.923507,loss:0.189569\n",
      "[epoch:34,batch:899]: val_loss:0.377183,val_acc:0.861423,val_total:4539\n",
      "[epoch:34,batch:929]:acc: 0.923387,loss:0.189450\n",
      "[epoch:34,batch:959]:acc: 0.923177,loss:0.189896\n",
      "[epoch:34,batch:989]:acc: 0.923548,loss:0.189507\n",
      "[epoch:34] :acc: 0.923606,loss:0.189339,lr:0.000000,patience:1\n",
      "[epoch:34]: val_loss:0.377849,val_acc:0.862304,\n",
      "Epoch 35/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 0 \n",
      "[epoch:35,batch:29]:acc: 0.915625,loss:0.194358\n",
      "[epoch:35,batch:59]:acc: 0.915104,loss:0.189368\n",
      "[epoch:35,batch:89]:acc: 0.922222,loss:0.181754\n",
      "[epoch:35,batch:119]:acc: 0.917969,loss:0.186565\n",
      "[epoch:35,batch:149]:acc: 0.918750,loss:0.188142\n",
      "[epoch:35,batch:179]:acc: 0.919792,loss:0.188787\n",
      "[epoch:35,batch:209]:acc: 0.920089,loss:0.188756\n",
      "[epoch:35,batch:239]:acc: 0.920182,loss:0.190715\n",
      "[epoch:35,batch:269]:acc: 0.918866,loss:0.192543\n",
      "[epoch:35,batch:299]:acc: 0.918646,loss:0.192699\n",
      "[epoch:35,batch:299]: val_loss:0.374187,val_acc:0.863186,val_total:4539\n",
      "[epoch:35,batch:329]:acc: 0.920739,loss:0.190423\n",
      "[epoch:35,batch:359]:acc: 0.921701,loss:0.189261\n",
      "[epoch:35,batch:389]:acc: 0.921635,loss:0.190790\n",
      "[epoch:35,batch:419]:acc: 0.921577,loss:0.190837\n",
      "[epoch:35,batch:449]:acc: 0.922292,loss:0.189718\n",
      "[epoch:35,batch:479]:acc: 0.922461,loss:0.189614\n",
      "[epoch:35,batch:509]:acc: 0.922243,loss:0.189769\n",
      "[epoch:35,batch:539]:acc: 0.922222,loss:0.189690\n",
      "[epoch:35,batch:569]:acc: 0.922643,loss:0.188717\n",
      "[epoch:35,batch:599]:acc: 0.923177,loss:0.187874\n",
      "[epoch:35,batch:599]: val_loss:0.381492,val_acc:0.858559,val_total:4539\n",
      "[epoch:35,batch:629]:acc: 0.923313,loss:0.187859\n",
      "[epoch:35,batch:659]:acc: 0.923674,loss:0.187268\n",
      "[epoch:35,batch:689]:acc: 0.923687,loss:0.187656\n",
      "[epoch:35,batch:719]:acc: 0.923611,loss:0.187849\n",
      "[epoch:35,batch:749]:acc: 0.923875,loss:0.187423\n",
      "[epoch:35,batch:779]:acc: 0.924279,loss:0.187290\n",
      "[epoch:35,batch:809]:acc: 0.924460,loss:0.187401\n",
      "[epoch:35,batch:839]:acc: 0.924479,loss:0.187266\n",
      "[epoch:35,batch:869]:acc: 0.924605,loss:0.187099\n",
      "[epoch:35,batch:899]:acc: 0.924931,loss:0.186651\n",
      "[epoch:35,batch:899]: val_loss:0.373520,val_acc:0.863626,val_total:4539\n",
      "[epoch:35,batch:929]:acc: 0.925067,loss:0.186581\n",
      "[epoch:35,batch:959]:acc: 0.925260,loss:0.186150\n",
      "[epoch:35,batch:989]:acc: 0.925568,loss:0.185811\n",
      "[epoch:35] :acc: 0.925560,loss:0.185797,lr:0.000000,patience:0\n",
      "[epoch:35]: val_loss:0.377324,val_acc:0.860983,\n",
      "Epoch 36/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 1 \n",
      "[epoch:36,batch:29]:acc: 0.939583,loss:0.166986\n",
      "[epoch:36,batch:59]:acc: 0.931771,loss:0.176671\n",
      "[epoch:36,batch:89]:acc: 0.928472,loss:0.178213\n",
      "[epoch:36,batch:119]:acc: 0.925000,loss:0.181940\n",
      "[epoch:36,batch:149]:acc: 0.925833,loss:0.182269\n",
      "[epoch:36,batch:179]:acc: 0.924826,loss:0.181916\n",
      "[epoch:36,batch:209]:acc: 0.922917,loss:0.184763\n",
      "[epoch:36,batch:239]:acc: 0.924609,loss:0.182990\n",
      "[epoch:36,batch:269]:acc: 0.923727,loss:0.184274\n",
      "[epoch:36,batch:299]:acc: 0.923854,loss:0.185494\n",
      "[epoch:36,batch:299]: val_loss:0.377509,val_acc:0.861423,val_total:4539\n",
      "[epoch:36,batch:329]:acc: 0.923390,loss:0.185688\n",
      "[epoch:36,batch:359]:acc: 0.923351,loss:0.185713\n",
      "[epoch:36,batch:389]:acc: 0.923157,loss:0.187701\n",
      "[epoch:36,batch:419]:acc: 0.922396,loss:0.189188\n",
      "[epoch:36,batch:449]:acc: 0.922778,loss:0.190375\n",
      "[epoch:36,batch:479]:acc: 0.923112,loss:0.190226\n",
      "[epoch:36,batch:509]:acc: 0.924203,loss:0.189309\n",
      "[epoch:36,batch:539]:acc: 0.924711,loss:0.189085\n",
      "[epoch:36,batch:569]:acc: 0.924178,loss:0.189998\n",
      "[epoch:36,batch:599]:acc: 0.924427,loss:0.189805\n",
      "[epoch:36,batch:599]: val_loss:0.375671,val_acc:0.860322,val_total:4539\n",
      "[epoch:36,batch:629]:acc: 0.924058,loss:0.189865\n",
      "[epoch:36,batch:659]:acc: 0.924195,loss:0.189654\n",
      "[epoch:36,batch:689]:acc: 0.923958,loss:0.190395\n",
      "[epoch:36,batch:719]:acc: 0.923828,loss:0.190315\n",
      "[epoch:36,batch:749]:acc: 0.923917,loss:0.190218\n",
      "[epoch:36,batch:779]:acc: 0.923758,loss:0.190258\n",
      "[epoch:36,batch:809]:acc: 0.923418,loss:0.190595\n",
      "[epoch:36,batch:839]:acc: 0.923326,loss:0.190938\n",
      "[epoch:36,batch:869]:acc: 0.923455,loss:0.190806\n",
      "[epoch:36,batch:899]:acc: 0.923264,loss:0.190652\n",
      "[epoch:36,batch:899]: val_loss:0.374305,val_acc:0.862525,val_total:4539\n",
      "[epoch:36,batch:929]:acc: 0.923589,loss:0.190377\n",
      "[epoch:36,batch:959]:acc: 0.923568,loss:0.190876\n",
      "[epoch:36,batch:989]:acc: 0.923958,loss:0.190695\n",
      "[epoch:36] :acc: 0.924016,loss:0.190544,lr:0.000000,patience:1\n",
      "[epoch:36]: val_loss:0.381928,val_acc:0.860542,\n",
      "Epoch 37/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 0 \n",
      "[epoch:37,batch:29]:acc: 0.922917,loss:0.201364\n",
      "[epoch:37,batch:59]:acc: 0.917188,loss:0.201391\n",
      "[epoch:37,batch:89]:acc: 0.915972,loss:0.202340\n",
      "[epoch:37,batch:119]:acc: 0.918750,loss:0.200108\n",
      "[epoch:37,batch:149]:acc: 0.920000,loss:0.197448\n",
      "[epoch:37,batch:179]:acc: 0.920833,loss:0.196010\n",
      "[epoch:37,batch:209]:acc: 0.923512,loss:0.191523\n",
      "[epoch:37,batch:239]:acc: 0.922266,loss:0.193152\n",
      "[epoch:37,batch:269]:acc: 0.922801,loss:0.191523\n",
      "[epoch:37,batch:299]:acc: 0.923438,loss:0.190847\n",
      "[epoch:37,batch:299]: val_loss:0.374669,val_acc:0.860983,val_total:4539\n",
      "[epoch:37,batch:329]:acc: 0.922917,loss:0.190675\n",
      "[epoch:37,batch:359]:acc: 0.922396,loss:0.190398\n",
      "[epoch:37,batch:389]:acc: 0.920673,loss:0.193609\n",
      "[epoch:37,batch:419]:acc: 0.920387,loss:0.193928\n",
      "[epoch:37,batch:449]:acc: 0.920000,loss:0.193840\n",
      "[epoch:37,batch:479]:acc: 0.919271,loss:0.194813\n",
      "[epoch:37,batch:509]:acc: 0.919853,loss:0.193451\n",
      "[epoch:37,batch:539]:acc: 0.920602,loss:0.192912\n",
      "[epoch:37,batch:569]:acc: 0.920285,loss:0.193301\n",
      "[epoch:37,batch:599]:acc: 0.920729,loss:0.192959\n",
      "[epoch:37,batch:599]: val_loss:0.376030,val_acc:0.860542,val_total:4539\n",
      "[epoch:37,batch:629]:acc: 0.921181,loss:0.192817\n",
      "[epoch:37,batch:659]:acc: 0.921733,loss:0.192318\n",
      "[epoch:37,batch:689]:acc: 0.921966,loss:0.191852\n",
      "[epoch:37,batch:719]:acc: 0.921701,loss:0.192483\n",
      "[epoch:37,batch:749]:acc: 0.922250,loss:0.191910\n",
      "[epoch:37,batch:779]:acc: 0.922236,loss:0.192178\n",
      "[epoch:37,batch:809]:acc: 0.922685,loss:0.191789\n",
      "[epoch:37,batch:839]:acc: 0.923177,loss:0.191070\n",
      "[epoch:37,batch:869]:acc: 0.922881,loss:0.192130\n",
      "[epoch:37,batch:899]:acc: 0.922917,loss:0.191834\n",
      "[epoch:37,batch:899]: val_loss:0.377269,val_acc:0.859440,val_total:4539\n",
      "[epoch:37,batch:929]:acc: 0.923219,loss:0.191415\n",
      "[epoch:37,batch:959]:acc: 0.923079,loss:0.191413\n",
      "[epoch:37,batch:989]:acc: 0.922790,loss:0.191492\n",
      "[epoch:37] :acc: 0.922723,loss:0.191469,lr:0.000000,patience:0\n",
      "[epoch:37]: val_loss:0.378881,val_acc:0.858559,\n",
      "Epoch 38/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 1 \n",
      "[epoch:38,batch:29]:acc: 0.920833,loss:0.185876\n",
      "[epoch:38,batch:59]:acc: 0.917188,loss:0.204196\n",
      "[epoch:38,batch:89]:acc: 0.920486,loss:0.199524\n",
      "[epoch:38,batch:119]:acc: 0.917969,loss:0.202599\n",
      "[epoch:38,batch:149]:acc: 0.921250,loss:0.196762\n",
      "[epoch:38,batch:179]:acc: 0.922396,loss:0.193600\n",
      "[epoch:38,batch:209]:acc: 0.921429,loss:0.194120\n",
      "[epoch:38,batch:239]:acc: 0.922266,loss:0.192546\n",
      "[epoch:38,batch:269]:acc: 0.923264,loss:0.190180\n",
      "[epoch:38,batch:299]:acc: 0.922188,loss:0.191256\n",
      "[epoch:38,batch:299]: val_loss:0.375265,val_acc:0.860542,val_total:4539\n",
      "[epoch:38,batch:329]:acc: 0.922064,loss:0.191333\n",
      "[epoch:38,batch:359]:acc: 0.922917,loss:0.190815\n",
      "[epoch:38,batch:389]:acc: 0.923638,loss:0.189828\n",
      "[epoch:38,batch:419]:acc: 0.923810,loss:0.190188\n",
      "[epoch:38,batch:449]:acc: 0.922639,loss:0.191850\n",
      "[epoch:38,batch:479]:acc: 0.922135,loss:0.192557\n",
      "[epoch:38,batch:509]:acc: 0.922181,loss:0.192667\n",
      "[epoch:38,batch:539]:acc: 0.921759,loss:0.193735\n",
      "[epoch:38,batch:569]:acc: 0.922697,loss:0.192333\n",
      "[epoch:38,batch:599]:acc: 0.922604,loss:0.191957\n",
      "[epoch:38,batch:599]: val_loss:0.375744,val_acc:0.864067,val_total:4539\n",
      "[epoch:38,batch:629]:acc: 0.922817,loss:0.191438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:38,batch:659]:acc: 0.923201,loss:0.191014\n",
      "[epoch:38,batch:689]:acc: 0.922871,loss:0.191796\n",
      "[epoch:38,batch:719]:acc: 0.923090,loss:0.191846\n",
      "[epoch:38,batch:749]:acc: 0.923208,loss:0.191644\n",
      "[epoch:38,batch:779]:acc: 0.923758,loss:0.191011\n",
      "[epoch:38,batch:809]:acc: 0.923765,loss:0.190734\n",
      "[epoch:38,batch:839]:acc: 0.923884,loss:0.190701\n",
      "[epoch:38,batch:869]:acc: 0.923994,loss:0.190622\n",
      "[epoch:38,batch:899]:acc: 0.923819,loss:0.190623\n",
      "[epoch:38,batch:899]: val_loss:0.375975,val_acc:0.862304,val_total:4539\n",
      "[epoch:38,batch:929]:acc: 0.924026,loss:0.190222\n",
      "[epoch:38,batch:959]:acc: 0.924089,loss:0.189945\n",
      "[epoch:38,batch:989]:acc: 0.924621,loss:0.189467\n",
      "[epoch:38] :acc: 0.924583,loss:0.189703,lr:0.000000,patience:1\n",
      "[epoch:38]: val_loss:0.376066,val_acc:0.862745,\n",
      "Epoch 39/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 0 \n",
      "[epoch:39,batch:29]:acc: 0.926042,loss:0.178980\n",
      "[epoch:39,batch:59]:acc: 0.921354,loss:0.190619\n",
      "[epoch:39,batch:89]:acc: 0.920486,loss:0.191602\n",
      "[epoch:39,batch:119]:acc: 0.922135,loss:0.188117\n",
      "[epoch:39,batch:149]:acc: 0.923750,loss:0.186514\n",
      "[epoch:39,batch:179]:acc: 0.922049,loss:0.187805\n",
      "[epoch:39,batch:209]:acc: 0.919643,loss:0.193130\n",
      "[epoch:39,batch:239]:acc: 0.920703,loss:0.191880\n",
      "[epoch:39,batch:269]:acc: 0.920255,loss:0.191811\n",
      "[epoch:39,batch:299]:acc: 0.921562,loss:0.190074\n",
      "[epoch:39,batch:299]: val_loss:0.373608,val_acc:0.861203,val_total:4539\n",
      "[epoch:39,batch:329]:acc: 0.921307,loss:0.190720\n",
      "[epoch:39,batch:359]:acc: 0.921615,loss:0.190532\n",
      "[epoch:39,batch:389]:acc: 0.921715,loss:0.191403\n",
      "[epoch:39,batch:419]:acc: 0.922396,loss:0.190511\n",
      "[epoch:39,batch:449]:acc: 0.922639,loss:0.191076\n",
      "[epoch:39,batch:479]:acc: 0.921940,loss:0.192116\n",
      "[epoch:39,batch:509]:acc: 0.922426,loss:0.191671\n",
      "[epoch:39,batch:539]:acc: 0.922164,loss:0.191151\n",
      "[epoch:39,batch:569]:acc: 0.922643,loss:0.190073\n",
      "[epoch:39,batch:599]:acc: 0.922500,loss:0.190455\n",
      "[epoch:39,batch:599]: val_loss:0.373267,val_acc:0.862084,val_total:4539\n",
      "[epoch:39,batch:629]:acc: 0.923115,loss:0.189676\n",
      "[epoch:39,batch:659]:acc: 0.923864,loss:0.188463\n",
      "[epoch:39,batch:689]:acc: 0.924185,loss:0.187734\n",
      "[epoch:39,batch:719]:acc: 0.924132,loss:0.187963\n",
      "[epoch:39,batch:749]:acc: 0.924417,loss:0.187441\n",
      "[epoch:39,batch:779]:acc: 0.924599,loss:0.187377\n",
      "[epoch:39,batch:809]:acc: 0.924306,loss:0.187315\n",
      "[epoch:39,batch:839]:acc: 0.923847,loss:0.187143\n",
      "[epoch:39,batch:869]:acc: 0.924138,loss:0.186776\n",
      "[epoch:39,batch:899]:acc: 0.923993,loss:0.186587\n",
      "[epoch:39,batch:899]: val_loss:0.376113,val_acc:0.863406,val_total:4539\n",
      "[epoch:39,batch:929]:acc: 0.924194,loss:0.186733\n",
      "[epoch:39,batch:959]:acc: 0.923730,loss:0.187401\n",
      "[epoch:39,batch:989]:acc: 0.923516,loss:0.187577\n",
      "[epoch:39] :acc: 0.923574,loss:0.187624,lr:0.000000,patience:0\n",
      "[epoch:39]: val_loss:0.373734,val_acc:0.861864,\n",
      "save new model loss,now loss is  0.3737336993217468\n",
      "Epoch 40/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 0 \n",
      "[epoch:40,batch:29]:acc: 0.915625,loss:0.207697\n",
      "[epoch:40,batch:59]:acc: 0.914062,loss:0.206214\n",
      "[epoch:40,batch:89]:acc: 0.913889,loss:0.205770\n",
      "[epoch:40,batch:119]:acc: 0.917969,loss:0.196871\n",
      "[epoch:40,batch:149]:acc: 0.916875,loss:0.199534\n",
      "[epoch:40,batch:179]:acc: 0.917882,loss:0.196171\n",
      "[epoch:40,batch:209]:acc: 0.918899,loss:0.192930\n",
      "[epoch:40,batch:239]:acc: 0.920182,loss:0.194097\n",
      "[epoch:40,batch:269]:acc: 0.920718,loss:0.193234\n",
      "[epoch:40,batch:299]:acc: 0.919167,loss:0.194377\n",
      "[epoch:40,batch:299]: val_loss:0.375136,val_acc:0.859000,val_total:4539\n",
      "[epoch:40,batch:329]:acc: 0.919223,loss:0.193813\n",
      "[epoch:40,batch:359]:acc: 0.919531,loss:0.195569\n",
      "[epoch:40,batch:389]:acc: 0.919231,loss:0.195691\n",
      "[epoch:40,batch:419]:acc: 0.919122,loss:0.196295\n",
      "[epoch:40,batch:449]:acc: 0.919722,loss:0.195847\n",
      "[epoch:40,batch:479]:acc: 0.919792,loss:0.196093\n",
      "[epoch:40,batch:509]:acc: 0.919914,loss:0.196942\n",
      "[epoch:40,batch:539]:acc: 0.920312,loss:0.196458\n",
      "[epoch:40,batch:569]:acc: 0.920066,loss:0.196728\n",
      "[epoch:40,batch:599]:acc: 0.919896,loss:0.196781\n",
      "[epoch:40,batch:599]: val_loss:0.375612,val_acc:0.862965,val_total:4539\n",
      "[epoch:40,batch:629]:acc: 0.920089,loss:0.196313\n",
      "[epoch:40,batch:659]:acc: 0.920218,loss:0.196801\n",
      "[epoch:40,batch:689]:acc: 0.920063,loss:0.196764\n",
      "[epoch:40,batch:719]:acc: 0.920095,loss:0.196491\n",
      "[epoch:40,batch:749]:acc: 0.920625,loss:0.195692\n",
      "[epoch:40,batch:779]:acc: 0.921074,loss:0.194977\n",
      "[epoch:40,batch:809]:acc: 0.921373,loss:0.195070\n",
      "[epoch:40,batch:839]:acc: 0.921726,loss:0.194012\n",
      "[epoch:40,batch:869]:acc: 0.922234,loss:0.193379\n",
      "[epoch:40,batch:899]:acc: 0.922118,loss:0.193136\n",
      "[epoch:40,batch:899]: val_loss:0.376743,val_acc:0.862965,val_total:4539\n",
      "[epoch:40,batch:929]:acc: 0.922379,loss:0.193083\n",
      "[epoch:40,batch:959]:acc: 0.922689,loss:0.192640\n",
      "[epoch:40,batch:989]:acc: 0.922854,loss:0.192378\n",
      "[epoch:40] :acc: 0.922880,loss:0.192377,lr:0.000000,patience:0\n",
      "[epoch:40]: val_loss:0.376881,val_acc:0.861864,\n",
      "Epoch 41/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 1 \n",
      "[epoch:41,batch:29]:acc: 0.916667,loss:0.196551\n",
      "[epoch:41,batch:59]:acc: 0.918750,loss:0.202369\n",
      "[epoch:41,batch:89]:acc: 0.920833,loss:0.192850\n",
      "[epoch:41,batch:119]:acc: 0.921615,loss:0.191473\n",
      "[epoch:41,batch:149]:acc: 0.920833,loss:0.190768\n",
      "[epoch:41,batch:179]:acc: 0.922569,loss:0.189486\n",
      "[epoch:41,batch:209]:acc: 0.922173,loss:0.190527\n",
      "[epoch:41,batch:239]:acc: 0.923958,loss:0.188250\n",
      "[epoch:41,batch:269]:acc: 0.923495,loss:0.187159\n",
      "[epoch:41,batch:299]:acc: 0.923438,loss:0.185744\n",
      "[epoch:41,batch:299]: val_loss:0.375447,val_acc:0.861643,val_total:4539\n",
      "[epoch:41,batch:329]:acc: 0.924432,loss:0.184388\n",
      "[epoch:41,batch:359]:acc: 0.923872,loss:0.184972\n",
      "[epoch:41,batch:389]:acc: 0.924199,loss:0.184539\n",
      "[epoch:41,batch:419]:acc: 0.923884,loss:0.184929\n",
      "[epoch:41,batch:449]:acc: 0.924444,loss:0.183481\n",
      "[epoch:41,batch:479]:acc: 0.924023,loss:0.184007\n",
      "[epoch:41,batch:509]:acc: 0.922917,loss:0.186225\n",
      "[epoch:41,batch:539]:acc: 0.922743,loss:0.186442\n",
      "[epoch:41,batch:569]:acc: 0.923136,loss:0.186302\n",
      "[epoch:41,batch:599]:acc: 0.923750,loss:0.185547\n",
      "[epoch:41,batch:599]: val_loss:0.376284,val_acc:0.860101,val_total:4539\n",
      "[epoch:41,batch:629]:acc: 0.923512,loss:0.186545\n",
      "[epoch:41,batch:659]:acc: 0.923248,loss:0.187151\n",
      "[epoch:41,batch:689]:acc: 0.923460,loss:0.186631\n",
      "[epoch:41,batch:719]:acc: 0.923307,loss:0.186447\n",
      "[epoch:41,batch:749]:acc: 0.923125,loss:0.187378\n",
      "[epoch:41,batch:779]:acc: 0.923037,loss:0.187898\n",
      "[epoch:41,batch:809]:acc: 0.922878,loss:0.188031\n",
      "[epoch:41,batch:839]:acc: 0.923214,loss:0.187667\n",
      "[epoch:41,batch:869]:acc: 0.922953,loss:0.188086\n",
      "[epoch:41,batch:899]:acc: 0.923056,loss:0.187876\n",
      "[epoch:41,batch:899]: val_loss:0.380689,val_acc:0.856576,val_total:4539\n",
      "[epoch:41,batch:929]:acc: 0.923253,loss:0.187694\n",
      "[epoch:41,batch:959]:acc: 0.922949,loss:0.188461\n",
      "[epoch:41,batch:989]:acc: 0.922727,loss:0.188824\n",
      "[epoch:41] :acc: 0.922754,loss:0.189322,lr:0.000000,patience:1\n",
      "[epoch:41]: val_loss:0.376742,val_acc:0.861643,\n",
      "Epoch 42/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 0 \n",
      "[epoch:42,batch:29]:acc: 0.909375,loss:0.203836\n",
      "[epoch:42,batch:59]:acc: 0.920833,loss:0.189017\n",
      "[epoch:42,batch:89]:acc: 0.920486,loss:0.191203\n",
      "[epoch:42,batch:119]:acc: 0.917448,loss:0.195490\n",
      "[epoch:42,batch:149]:acc: 0.917083,loss:0.196625\n",
      "[epoch:42,batch:179]:acc: 0.919618,loss:0.193160\n",
      "[epoch:42,batch:209]:acc: 0.919792,loss:0.193530\n",
      "[epoch:42,batch:239]:acc: 0.921094,loss:0.193800\n",
      "[epoch:42,batch:269]:acc: 0.921644,loss:0.193094\n",
      "[epoch:42,batch:299]:acc: 0.920521,loss:0.195308\n",
      "[epoch:42,batch:299]: val_loss:0.375438,val_acc:0.861423,val_total:4539\n",
      "[epoch:42,batch:329]:acc: 0.922254,loss:0.193093\n",
      "[epoch:42,batch:359]:acc: 0.922135,loss:0.193130\n",
      "[epoch:42,batch:389]:acc: 0.922356,loss:0.192826\n",
      "[epoch:42,batch:419]:acc: 0.923363,loss:0.192067\n",
      "[epoch:42,batch:449]:acc: 0.923264,loss:0.192211\n",
      "[epoch:42,batch:479]:acc: 0.923763,loss:0.191011\n",
      "[epoch:42,batch:509]:acc: 0.923284,loss:0.190906\n",
      "[epoch:42,batch:539]:acc: 0.922859,loss:0.191895\n",
      "[epoch:42,batch:569]:acc: 0.922752,loss:0.191704\n",
      "[epoch:42,batch:599]:acc: 0.922969,loss:0.191777\n",
      "[epoch:42,batch:599]: val_loss:0.375296,val_acc:0.863626,val_total:4539\n",
      "[epoch:42,batch:629]:acc: 0.922917,loss:0.191398\n",
      "[epoch:42,batch:659]:acc: 0.922869,loss:0.191123\n",
      "[epoch:42,batch:689]:acc: 0.922645,loss:0.191075\n",
      "[epoch:42,batch:719]:acc: 0.923307,loss:0.189958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:42,batch:749]:acc: 0.924167,loss:0.189492\n",
      "[epoch:42,batch:779]:acc: 0.924519,loss:0.189663\n",
      "[epoch:42,batch:809]:acc: 0.924961,loss:0.189242\n",
      "[epoch:42,batch:839]:acc: 0.925074,loss:0.189256\n",
      "[epoch:42,batch:869]:acc: 0.925575,loss:0.188682\n",
      "[epoch:42,batch:899]:acc: 0.926111,loss:0.188112\n",
      "[epoch:42,batch:899]: val_loss:0.377907,val_acc:0.862525,val_total:4539\n",
      "[epoch:42,batch:929]:acc: 0.926109,loss:0.188274\n",
      "[epoch:42,batch:959]:acc: 0.926400,loss:0.187452\n",
      "[epoch:42,batch:989]:acc: 0.925852,loss:0.188199\n",
      "[epoch:42] :acc: 0.925876,loss:0.188210,lr:0.000000,patience:0\n",
      "[epoch:42]: val_loss:0.374539,val_acc:0.862745,\n",
      "Epoch 43/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 1 \n",
      "[epoch:43,batch:29]:acc: 0.918750,loss:0.190381\n",
      "[epoch:43,batch:59]:acc: 0.923958,loss:0.188419\n",
      "[epoch:43,batch:89]:acc: 0.927083,loss:0.185450\n",
      "[epoch:43,batch:119]:acc: 0.925781,loss:0.187398\n",
      "[epoch:43,batch:149]:acc: 0.927083,loss:0.185919\n",
      "[epoch:43,batch:179]:acc: 0.925174,loss:0.187805\n",
      "[epoch:43,batch:209]:acc: 0.924702,loss:0.188130\n",
      "[epoch:43,batch:239]:acc: 0.924870,loss:0.187750\n",
      "[epoch:43,batch:269]:acc: 0.926042,loss:0.187303\n",
      "[epoch:43,batch:299]:acc: 0.925208,loss:0.187536\n",
      "[epoch:43,batch:299]: val_loss:0.376270,val_acc:0.861864,val_total:4539\n",
      "[epoch:43,batch:329]:acc: 0.925000,loss:0.187910\n",
      "[epoch:43,batch:359]:acc: 0.925955,loss:0.186515\n",
      "[epoch:43,batch:389]:acc: 0.926763,loss:0.186231\n",
      "[epoch:43,batch:419]:acc: 0.926190,loss:0.186791\n",
      "[epoch:43,batch:449]:acc: 0.925972,loss:0.186466\n",
      "[epoch:43,batch:479]:acc: 0.925391,loss:0.186845\n",
      "[epoch:43,batch:509]:acc: 0.925061,loss:0.188299\n",
      "[epoch:43,batch:539]:acc: 0.923900,loss:0.190041\n",
      "[epoch:43,batch:569]:acc: 0.923849,loss:0.189896\n",
      "[epoch:43,batch:599]:acc: 0.924115,loss:0.189879\n",
      "[epoch:43,batch:599]: val_loss:0.373984,val_acc:0.861643,val_total:4539\n",
      "[epoch:43,batch:629]:acc: 0.924008,loss:0.189431\n",
      "[epoch:43,batch:659]:acc: 0.923627,loss:0.189624\n",
      "[epoch:43,batch:689]:acc: 0.923641,loss:0.189315\n",
      "[epoch:43,batch:719]:acc: 0.924219,loss:0.189000\n",
      "[epoch:43,batch:749]:acc: 0.923708,loss:0.189299\n",
      "[epoch:43,batch:779]:acc: 0.923958,loss:0.189014\n",
      "[epoch:43,batch:809]:acc: 0.924228,loss:0.188329\n",
      "[epoch:43,batch:839]:acc: 0.924144,loss:0.188463\n",
      "[epoch:43,batch:869]:acc: 0.924282,loss:0.188357\n",
      "[epoch:43,batch:899]:acc: 0.924097,loss:0.188750\n",
      "[epoch:43,batch:899]: val_loss:0.374976,val_acc:0.861423,val_total:4539\n",
      "[epoch:43,batch:929]:acc: 0.924160,loss:0.189019\n",
      "[epoch:43,batch:959]:acc: 0.924544,loss:0.188699\n",
      "[epoch:43,batch:989]:acc: 0.924684,loss:0.188330\n",
      "[epoch:43] :acc: 0.924678,loss:0.188377,lr:0.000000,patience:1\n",
      "[epoch:43]: val_loss:0.378422,val_acc:0.862084,\n",
      "Epoch 44/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 0 \n",
      "[epoch:44,batch:29]:acc: 0.921875,loss:0.181164\n",
      "[epoch:44,batch:59]:acc: 0.925521,loss:0.175698\n",
      "[epoch:44,batch:89]:acc: 0.929167,loss:0.171505\n",
      "[epoch:44,batch:119]:acc: 0.929948,loss:0.175562\n",
      "[epoch:44,batch:149]:acc: 0.930417,loss:0.177133\n",
      "[epoch:44,batch:179]:acc: 0.929861,loss:0.180123\n",
      "[epoch:44,batch:209]:acc: 0.927679,loss:0.182864\n",
      "[epoch:44,batch:239]:acc: 0.927865,loss:0.183645\n",
      "[epoch:44,batch:269]:acc: 0.926157,loss:0.186462\n",
      "[epoch:44,batch:299]:acc: 0.926354,loss:0.186850\n",
      "[epoch:44,batch:299]: val_loss:0.374448,val_acc:0.862965,val_total:4539\n",
      "[epoch:44,batch:329]:acc: 0.927557,loss:0.184589\n",
      "[epoch:44,batch:359]:acc: 0.927083,loss:0.185848\n",
      "[epoch:44,batch:389]:acc: 0.926603,loss:0.186042\n",
      "[epoch:44,batch:419]:acc: 0.927158,loss:0.185355\n",
      "[epoch:44,batch:449]:acc: 0.927014,loss:0.186780\n",
      "[epoch:44,batch:479]:acc: 0.926432,loss:0.187985\n",
      "[epoch:44,batch:509]:acc: 0.925919,loss:0.188255\n",
      "[epoch:44,batch:539]:acc: 0.925868,loss:0.187637\n",
      "[epoch:44,batch:569]:acc: 0.926151,loss:0.187479\n",
      "[epoch:44,batch:599]:acc: 0.925781,loss:0.188054\n",
      "[epoch:44,batch:599]: val_loss:0.375927,val_acc:0.862525,val_total:4539\n",
      "[epoch:44,batch:629]:acc: 0.926389,loss:0.187302\n",
      "[epoch:44,batch:659]:acc: 0.925663,loss:0.188100\n",
      "[epoch:44,batch:689]:acc: 0.925272,loss:0.188841\n",
      "[epoch:44,batch:719]:acc: 0.924870,loss:0.190018\n",
      "[epoch:44,batch:749]:acc: 0.924792,loss:0.190154\n",
      "[epoch:44,batch:779]:acc: 0.924279,loss:0.190770\n",
      "[epoch:44,batch:809]:acc: 0.923688,loss:0.191706\n",
      "[epoch:44,batch:839]:acc: 0.923326,loss:0.191755\n",
      "[epoch:44,batch:869]:acc: 0.923707,loss:0.191781\n",
      "[epoch:44,batch:899]:acc: 0.923924,loss:0.191505\n",
      "[epoch:44,batch:899]: val_loss:0.375815,val_acc:0.860762,val_total:4539\n",
      "[epoch:44,batch:929]:acc: 0.924362,loss:0.190752\n",
      "[epoch:44,batch:959]:acc: 0.924219,loss:0.191168\n",
      "[epoch:44,batch:989]:acc: 0.924085,loss:0.191039\n",
      "[epoch:44] :acc: 0.924016,loss:0.191271,lr:0.000000,patience:0\n",
      "[epoch:44]: val_loss:0.375819,val_acc:0.862525,\n",
      "Epoch 45/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 1 \n",
      "[epoch:45,batch:29]:acc: 0.935417,loss:0.173771\n",
      "[epoch:45,batch:59]:acc: 0.926042,loss:0.188789\n",
      "[epoch:45,batch:89]:acc: 0.926736,loss:0.184485\n",
      "[epoch:45,batch:119]:acc: 0.927083,loss:0.182539\n",
      "[epoch:45,batch:149]:acc: 0.926875,loss:0.182999\n",
      "[epoch:45,batch:179]:acc: 0.927778,loss:0.182588\n",
      "[epoch:45,batch:209]:acc: 0.926339,loss:0.184203\n",
      "[epoch:45,batch:239]:acc: 0.927734,loss:0.183720\n",
      "[epoch:45,batch:269]:acc: 0.928588,loss:0.182056\n",
      "[epoch:45,batch:299]:acc: 0.928021,loss:0.182283\n",
      "[epoch:45,batch:299]: val_loss:0.376897,val_acc:0.860101,val_total:4539\n",
      "[epoch:45,batch:329]:acc: 0.927178,loss:0.183317\n",
      "[epoch:45,batch:359]:acc: 0.925694,loss:0.185746\n",
      "[epoch:45,batch:389]:acc: 0.925080,loss:0.186360\n",
      "[epoch:45,batch:419]:acc: 0.925372,loss:0.186955\n",
      "[epoch:45,batch:449]:acc: 0.925278,loss:0.187379\n",
      "[epoch:45,batch:479]:acc: 0.925326,loss:0.187124\n",
      "[epoch:45,batch:509]:acc: 0.924265,loss:0.188059\n",
      "[epoch:45,batch:539]:acc: 0.924595,loss:0.186909\n",
      "[epoch:45,batch:569]:acc: 0.925000,loss:0.186078\n",
      "[epoch:45,batch:599]:acc: 0.925208,loss:0.186297\n",
      "[epoch:45,batch:599]: val_loss:0.378594,val_acc:0.861203,val_total:4539\n",
      "[epoch:45,batch:629]:acc: 0.924802,loss:0.187018\n",
      "[epoch:45,batch:659]:acc: 0.924716,loss:0.187614\n",
      "[epoch:45,batch:689]:acc: 0.924683,loss:0.187842\n",
      "[epoch:45,batch:719]:acc: 0.923785,loss:0.189130\n",
      "[epoch:45,batch:749]:acc: 0.923417,loss:0.189859\n",
      "[epoch:45,batch:779]:acc: 0.923678,loss:0.189618\n",
      "[epoch:45,batch:809]:acc: 0.923457,loss:0.189976\n",
      "[epoch:45,batch:839]:acc: 0.923884,loss:0.189539\n",
      "[epoch:45,batch:869]:acc: 0.924066,loss:0.189229\n",
      "[epoch:45,batch:899]:acc: 0.924340,loss:0.188539\n",
      "[epoch:45,batch:899]: val_loss:0.377499,val_acc:0.861423,val_total:4539\n",
      "[epoch:45,batch:929]:acc: 0.924227,loss:0.188524\n",
      "[epoch:45,batch:959]:acc: 0.924316,loss:0.188199\n",
      "[epoch:45,batch:989]:acc: 0.924463,loss:0.187914\n",
      "[epoch:45] :acc: 0.924457,loss:0.188199,lr:0.000000,patience:1\n",
      "[epoch:45]: val_loss:0.378304,val_acc:0.864508,\n",
      "Epoch 46/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 0 \n",
      "[epoch:46,batch:29]:acc: 0.934375,loss:0.174060\n",
      "[epoch:46,batch:59]:acc: 0.928646,loss:0.184956\n",
      "[epoch:46,batch:89]:acc: 0.920486,loss:0.192138\n",
      "[epoch:46,batch:119]:acc: 0.922135,loss:0.190330\n",
      "[epoch:46,batch:149]:acc: 0.922083,loss:0.190656\n",
      "[epoch:46,batch:179]:acc: 0.923611,loss:0.187019\n",
      "[epoch:46,batch:209]:acc: 0.924256,loss:0.187238\n",
      "[epoch:46,batch:239]:acc: 0.923047,loss:0.188317\n",
      "[epoch:46,batch:269]:acc: 0.923380,loss:0.188011\n",
      "[epoch:46,batch:299]:acc: 0.921562,loss:0.191107\n",
      "[epoch:46,batch:299]: val_loss:0.378805,val_acc:0.861203,val_total:4539\n",
      "[epoch:46,batch:329]:acc: 0.921591,loss:0.191074\n",
      "[epoch:46,batch:359]:acc: 0.922049,loss:0.190495\n",
      "[epoch:46,batch:389]:acc: 0.920753,loss:0.192748\n",
      "[epoch:46,batch:419]:acc: 0.919940,loss:0.194483\n",
      "[epoch:46,batch:449]:acc: 0.920694,loss:0.193167\n",
      "[epoch:46,batch:479]:acc: 0.920703,loss:0.192679\n",
      "[epoch:46,batch:509]:acc: 0.920772,loss:0.193606\n",
      "[epoch:46,batch:539]:acc: 0.921354,loss:0.193498\n",
      "[epoch:46,batch:569]:acc: 0.922423,loss:0.192204\n",
      "[epoch:46,batch:599]:acc: 0.922240,loss:0.192048\n",
      "[epoch:46,batch:599]: val_loss:0.376999,val_acc:0.862304,val_total:4539\n",
      "[epoch:46,batch:629]:acc: 0.922123,loss:0.192653\n",
      "[epoch:46,batch:659]:acc: 0.921875,loss:0.192418\n",
      "[epoch:46,batch:689]:acc: 0.922418,loss:0.192037\n",
      "[epoch:46,batch:719]:acc: 0.922786,loss:0.191316\n",
      "[epoch:46,batch:749]:acc: 0.922833,loss:0.190868\n",
      "[epoch:46,batch:779]:acc: 0.923197,loss:0.190492\n",
      "[epoch:46,batch:809]:acc: 0.923302,loss:0.190562\n",
      "[epoch:46,batch:839]:acc: 0.922954,loss:0.190953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:46,batch:869]:acc: 0.923276,loss:0.190412\n",
      "[epoch:46,batch:899]:acc: 0.923681,loss:0.190040\n",
      "[epoch:46,batch:899]: val_loss:0.375196,val_acc:0.862304,val_total:4539\n",
      "[epoch:46,batch:929]:acc: 0.923454,loss:0.189781\n",
      "[epoch:46,batch:959]:acc: 0.923372,loss:0.189690\n",
      "[epoch:46,batch:989]:acc: 0.922443,loss:0.190385\n",
      "[epoch:46] :acc: 0.922439,loss:0.190475,lr:0.000000,patience:0\n",
      "[epoch:46]: val_loss:0.377956,val_acc:0.860322,\n",
      "Epoch 47/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 1 \n",
      "[epoch:47,batch:29]:acc: 0.915625,loss:0.195762\n",
      "[epoch:47,batch:59]:acc: 0.913021,loss:0.203071\n",
      "[epoch:47,batch:89]:acc: 0.917361,loss:0.191414\n",
      "[epoch:47,batch:119]:acc: 0.920833,loss:0.190482\n",
      "[epoch:47,batch:149]:acc: 0.919167,loss:0.192601\n",
      "[epoch:47,batch:179]:acc: 0.918924,loss:0.192807\n",
      "[epoch:47,batch:209]:acc: 0.920387,loss:0.191108\n",
      "[epoch:47,batch:239]:acc: 0.920443,loss:0.190544\n",
      "[epoch:47,batch:269]:acc: 0.920486,loss:0.191709\n",
      "[epoch:47,batch:299]:acc: 0.919792,loss:0.194773\n",
      "[epoch:47,batch:299]: val_loss:0.376190,val_acc:0.862525,val_total:4539\n",
      "[epoch:47,batch:329]:acc: 0.919413,loss:0.195687\n",
      "[epoch:47,batch:359]:acc: 0.919792,loss:0.195770\n",
      "[epoch:47,batch:389]:acc: 0.920112,loss:0.194526\n",
      "[epoch:47,batch:419]:acc: 0.920610,loss:0.193658\n",
      "[epoch:47,batch:449]:acc: 0.920903,loss:0.192798\n",
      "[epoch:47,batch:479]:acc: 0.920833,loss:0.192230\n",
      "[epoch:47,batch:509]:acc: 0.921998,loss:0.191014\n",
      "[epoch:47,batch:539]:acc: 0.921470,loss:0.191371\n",
      "[epoch:47,batch:569]:acc: 0.921930,loss:0.190454\n",
      "[epoch:47,batch:599]:acc: 0.921875,loss:0.190094\n",
      "[epoch:47,batch:599]: val_loss:0.375626,val_acc:0.862745,val_total:4539\n",
      "[epoch:47,batch:629]:acc: 0.922768,loss:0.189222\n",
      "[epoch:47,batch:659]:acc: 0.922254,loss:0.190162\n",
      "[epoch:47,batch:689]:acc: 0.921966,loss:0.190846\n",
      "[epoch:47,batch:719]:acc: 0.922483,loss:0.190317\n",
      "[epoch:47,batch:749]:acc: 0.922875,loss:0.189436\n",
      "[epoch:47,batch:779]:acc: 0.922917,loss:0.189392\n",
      "[epoch:47,batch:809]:acc: 0.922801,loss:0.189762\n",
      "[epoch:47,batch:839]:acc: 0.923177,loss:0.189297\n",
      "[epoch:47,batch:869]:acc: 0.922809,loss:0.189325\n",
      "[epoch:47,batch:899]:acc: 0.923160,loss:0.189113\n",
      "[epoch:47,batch:899]: val_loss:0.375893,val_acc:0.861864,val_total:4539\n",
      "[epoch:47,batch:929]:acc: 0.923185,loss:0.189137\n",
      "[epoch:47,batch:959]:acc: 0.923828,loss:0.188592\n",
      "[epoch:47,batch:989]:acc: 0.923737,loss:0.188574\n",
      "[epoch:47] :acc: 0.923669,loss:0.189083,lr:0.000000,patience:1\n",
      "[epoch:47]: val_loss:0.384091,val_acc:0.858779,\n",
      "Epoch 48/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 0 \n",
      "[epoch:48,batch:29]:acc: 0.950000,loss:0.144399\n",
      "[epoch:48,batch:59]:acc: 0.927083,loss:0.179497\n",
      "[epoch:48,batch:89]:acc: 0.924306,loss:0.183934\n",
      "[epoch:48,batch:119]:acc: 0.924219,loss:0.186724\n",
      "[epoch:48,batch:149]:acc: 0.925000,loss:0.187923\n",
      "[epoch:48,batch:179]:acc: 0.927257,loss:0.186718\n",
      "[epoch:48,batch:209]:acc: 0.926935,loss:0.187627\n",
      "[epoch:48,batch:239]:acc: 0.926953,loss:0.185982\n",
      "[epoch:48,batch:269]:acc: 0.925810,loss:0.187966\n",
      "[epoch:48,batch:299]:acc: 0.925729,loss:0.189150\n",
      "[epoch:48,batch:299]: val_loss:0.376679,val_acc:0.861864,val_total:4539\n",
      "[epoch:48,batch:329]:acc: 0.925284,loss:0.190075\n",
      "[epoch:48,batch:359]:acc: 0.924653,loss:0.189797\n",
      "[epoch:48,batch:389]:acc: 0.923958,loss:0.190633\n",
      "[epoch:48,batch:419]:acc: 0.924777,loss:0.189848\n",
      "[epoch:48,batch:449]:acc: 0.924514,loss:0.189524\n",
      "[epoch:48,batch:479]:acc: 0.925000,loss:0.188842\n",
      "[epoch:48,batch:509]:acc: 0.924510,loss:0.189478\n",
      "[epoch:48,batch:539]:acc: 0.924363,loss:0.188983\n",
      "[epoch:48,batch:569]:acc: 0.924232,loss:0.188967\n",
      "[epoch:48,batch:599]:acc: 0.924844,loss:0.188175\n",
      "[epoch:48,batch:599]: val_loss:0.378249,val_acc:0.862084,val_total:4539\n",
      "[epoch:48,batch:629]:acc: 0.924405,loss:0.189212\n",
      "[epoch:48,batch:659]:acc: 0.924432,loss:0.189266\n",
      "[epoch:48,batch:689]:acc: 0.924502,loss:0.189037\n",
      "[epoch:48,batch:719]:acc: 0.924306,loss:0.188775\n",
      "[epoch:48,batch:749]:acc: 0.924458,loss:0.188447\n",
      "[epoch:48,batch:779]:acc: 0.924279,loss:0.188537\n",
      "[epoch:48,batch:809]:acc: 0.924228,loss:0.188331\n",
      "[epoch:48,batch:839]:acc: 0.924144,loss:0.188734\n",
      "[epoch:48,batch:869]:acc: 0.923851,loss:0.189307\n",
      "[epoch:48,batch:899]:acc: 0.924271,loss:0.188672\n",
      "[epoch:48,batch:899]: val_loss:0.375116,val_acc:0.862525,val_total:4539\n",
      "[epoch:48,batch:929]:acc: 0.924093,loss:0.188960\n",
      "[epoch:48,batch:959]:acc: 0.924121,loss:0.189304\n",
      "[epoch:48,batch:989]:acc: 0.923769,loss:0.189612\n",
      "[epoch:48] :acc: 0.923763,loss:0.189615,lr:0.000000,patience:0\n",
      "[epoch:48]: val_loss:0.377899,val_acc:0.863626,\n",
      "Epoch 49/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 1 \n",
      "[epoch:49,batch:29]:acc: 0.932292,loss:0.193107\n",
      "[epoch:49,batch:59]:acc: 0.928125,loss:0.199434\n",
      "[epoch:49,batch:89]:acc: 0.930556,loss:0.191012\n",
      "[epoch:49,batch:119]:acc: 0.926302,loss:0.193891\n",
      "[epoch:49,batch:149]:acc: 0.927917,loss:0.191145\n",
      "[epoch:49,batch:179]:acc: 0.926215,loss:0.192817\n",
      "[epoch:49,batch:209]:acc: 0.924256,loss:0.194176\n",
      "[epoch:49,batch:239]:acc: 0.924870,loss:0.191624\n",
      "[epoch:49,batch:269]:acc: 0.922917,loss:0.192908\n",
      "[epoch:49,batch:299]:acc: 0.923125,loss:0.193271\n",
      "[epoch:49,batch:299]: val_loss:0.374718,val_acc:0.863186,val_total:4539\n",
      "[epoch:49,batch:329]:acc: 0.923390,loss:0.192866\n",
      "[epoch:49,batch:359]:acc: 0.923872,loss:0.191905\n",
      "[epoch:49,batch:389]:acc: 0.922516,loss:0.193436\n",
      "[epoch:49,batch:419]:acc: 0.921875,loss:0.193706\n",
      "[epoch:49,batch:449]:acc: 0.920972,loss:0.194856\n",
      "[epoch:49,batch:479]:acc: 0.921484,loss:0.194322\n",
      "[epoch:49,batch:509]:acc: 0.921875,loss:0.193126\n",
      "[epoch:49,batch:539]:acc: 0.921817,loss:0.192751\n",
      "[epoch:49,batch:569]:acc: 0.922094,loss:0.192666\n",
      "[epoch:49,batch:599]:acc: 0.921719,loss:0.192686\n",
      "[epoch:49,batch:599]: val_loss:0.378368,val_acc:0.861643,val_total:4539\n",
      "[epoch:49,batch:629]:acc: 0.921577,loss:0.192541\n",
      "[epoch:49,batch:659]:acc: 0.921686,loss:0.192302\n",
      "[epoch:49,batch:689]:acc: 0.922011,loss:0.191241\n",
      "[epoch:49,batch:719]:acc: 0.922005,loss:0.190837\n",
      "[epoch:49,batch:749]:acc: 0.922125,loss:0.190372\n",
      "[epoch:49,batch:779]:acc: 0.922716,loss:0.189785\n",
      "[epoch:49,batch:809]:acc: 0.922454,loss:0.189600\n",
      "[epoch:49,batch:839]:acc: 0.922805,loss:0.189127\n",
      "[epoch:49,batch:869]:acc: 0.922522,loss:0.189467\n",
      "[epoch:49,batch:899]:acc: 0.923056,loss:0.188862\n",
      "[epoch:49,batch:899]: val_loss:0.375230,val_acc:0.862304,val_total:4539\n",
      "[epoch:49,batch:929]:acc: 0.922816,loss:0.189108\n",
      "[epoch:49,batch:959]:acc: 0.922982,loss:0.188995\n",
      "[epoch:49,batch:989]:acc: 0.923011,loss:0.188632\n",
      "[epoch:49] :acc: 0.923070,loss:0.188625,lr:0.000000,patience:1\n",
      "[epoch:49]: val_loss:0.377602,val_acc:0.860762,\n",
      "Epoch 50/59\n",
      "----------\n",
      "lr now is 0.000000\n",
      "now patience is 0 \n",
      "[epoch:50,batch:29]:acc: 0.920833,loss:0.187272\n",
      "[epoch:50,batch:59]:acc: 0.926042,loss:0.180325\n",
      "[epoch:50,batch:89]:acc: 0.926389,loss:0.184411\n",
      "[epoch:50,batch:119]:acc: 0.927865,loss:0.183135\n",
      "[epoch:50,batch:149]:acc: 0.928542,loss:0.182796\n",
      "[epoch:50,batch:179]:acc: 0.925174,loss:0.187366\n",
      "[epoch:50,batch:209]:acc: 0.926339,loss:0.185630\n",
      "[epoch:50,batch:239]:acc: 0.927344,loss:0.185860\n",
      "[epoch:50,batch:269]:acc: 0.925926,loss:0.186197\n",
      "[epoch:50,batch:299]:acc: 0.925833,loss:0.186440\n",
      "[epoch:50,batch:299]: val_loss:0.377459,val_acc:0.861864,val_total:4539\n",
      "[epoch:50,batch:329]:acc: 0.926610,loss:0.185208\n",
      "[epoch:50,batch:359]:acc: 0.926389,loss:0.186427\n",
      "[epoch:50,batch:389]:acc: 0.926603,loss:0.186858\n",
      "[epoch:50,batch:419]:acc: 0.924926,loss:0.188990\n",
      "[epoch:50,batch:449]:acc: 0.925139,loss:0.188626\n",
      "[epoch:50,batch:479]:acc: 0.925716,loss:0.187882\n",
      "[epoch:50,batch:509]:acc: 0.925306,loss:0.187704\n",
      "[epoch:50,batch:539]:acc: 0.925637,loss:0.187570\n",
      "[epoch:50,batch:569]:acc: 0.926042,loss:0.187450\n",
      "[epoch:50,batch:599]:acc: 0.926302,loss:0.187413\n",
      "[epoch:50,batch:599]: val_loss:0.376215,val_acc:0.861864,val_total:4539\n",
      "[epoch:50,batch:629]:acc: 0.926091,loss:0.187938\n",
      "[epoch:50,batch:659]:acc: 0.925663,loss:0.188810\n",
      "[epoch:50,batch:689]:acc: 0.925725,loss:0.188616\n",
      "[epoch:50,batch:719]:acc: 0.925564,loss:0.188627\n",
      "[epoch:50,batch:749]:acc: 0.925125,loss:0.188948\n",
      "[epoch:50,batch:779]:acc: 0.924800,loss:0.189363\n",
      "[epoch:50,batch:809]:acc: 0.925347,loss:0.188904\n",
      "[epoch:50,batch:839]:acc: 0.925037,loss:0.189331\n",
      "[epoch:50,batch:869]:acc: 0.924749,loss:0.189669\n",
      "[epoch:50,batch:899]:acc: 0.924826,loss:0.189666\n",
      "[epoch:50,batch:899]: val_loss:0.376349,val_acc:0.862084,val_total:4539\n",
      "[epoch:50,batch:929]:acc: 0.924933,loss:0.189454\n"
     ]
    }
   ],
   "source": [
    "reuseTrain('../model/DesNet161/2018-11-01_acc_best.pth',60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainWithRawData(path,epochNum):\n",
    "    writer=SummaryWriter('log/'+date+'/DesNet161/') # 创建 /log/日期/InceptionResnet的组织形式\n",
    "    train_dataset=MyDataSet(json_Description=ANNOTATION_TRAIN,transform=preprocess(normalize_torch,IMAGE_SIZE),path_pre=IMAGE_TRAIN_PRE)\n",
    "    val_dataset=MyDataSet(json_Description=ANNOTATION_VAL,transform=preprocess(normalize_torch,IMAGE_SIZE),path_pre=IMAGE_VAL_PRE)\n",
    "    train_dataLoader=DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE,num_workers=16,shuffle=True)\n",
    "    val_dataLoader=DataLoader(dataset=val_dataset,batch_size=BATCH_SIZE,num_workers=1,shuffle=False)\n",
    "    model=getmodel()\n",
    "    criterion=nn.CrossEntropyLoss().cuda()\n",
    "    modelParams=torch.load(path)\n",
    "    model.load_state_dict(modelParams['state_dict'])\n",
    "    min_loss=modelParams['val_loss']\n",
    "    print('min_loss is :%f'%(min_loss))\n",
    "    print('val_correct is %f'%(modelParams['val_correct']))\n",
    "    min_acc=max(modelParams['val_correct'],0.81)\n",
    "    optinizerSave=modelParams['optimizer']\n",
    "    patience=0\n",
    "    lr=1e-4\n",
    "    momentum=0.9\n",
    "    beginepoch=modelParams['epoch']\n",
    "    for epoch in range(beginepoch,epochNum):\n",
    "        print('Epoch {}/{}'.format(epoch, epochNum - 1))\n",
    "        print('-' * 10)\n",
    "        if patience==3:\n",
    "            patience=0\n",
    "            model.load_state_dict(torch.load('../model/DesNet161/'+date+'_loss_best.pth')['state_dict'])\n",
    "            lr=lr/5\n",
    "            print('lr desencd')\n",
    "        if epoch==beginepoch:\n",
    "            optimizer=torch.optim.SGD(params=model.parameters(),lr=lr,momentum=momentum)\n",
    "#             optimizer.load_state_dict(optinizerSave)\n",
    "#             lr=optimizer['lr']\n",
    "#             momentum=optimizer['momentum']\n",
    "            print('begin lr is ',lr)\n",
    "            \n",
    "        else:\n",
    "            optimizer=torch.optim.SGD(params=model.parameters(),lr=lr,momentum=momentum)\n",
    "                   \n",
    "        running_loss = RunningMean()\n",
    "        running_corrects = RunningMean()\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_dataLoader):\n",
    "            model.train(True)\n",
    "            n_batchsize=inputs.size(0)\n",
    "            inputs = Variable(inputs).cuda()\n",
    "            labels = Variable(labels).cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            if isinstance(outputs,tuple):\n",
    "                loss=sum((criterion(o,labels)) for o in outputs)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "            running_loss.update(loss.item(),1)\n",
    "            running_corrects.update(torch.sum(preds == labels.data).data,n_batchsize)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx%30==29:\n",
    "                print('[epoch:%d,batch:%d]:acc: %f,loss:%f'%(epoch,batch_idx,running_corrects.value,running_loss.value))\n",
    "                if batch_idx%300==299: \n",
    "                    niter = epoch * len(train_dataset)/BATCH_SIZE + batch_idx\n",
    "                    writer.add_scalar('Train/Acc',running_corrects.value,niter)\n",
    "                    writer.add_scalar('Train/Loss',running_loss.value,niter)\n",
    "                    lx,px=utils.predict(model,val_dataLoader)\n",
    "                    log_loss = criterion(Variable(px), Variable(lx))\n",
    "                    log_loss = log_loss.item()\n",
    "                    _, preds = torch.max(px, dim=1)\n",
    "                    accuracy = torch.mean((preds == lx).float())\n",
    "                    writer.add_scalar('Val/Acc',accuracy,niter)\n",
    "                    writer.add_scalar('Val/Loss',log_loss,niter)\n",
    "                    print('[epoch:%d,batch:%d]: val_loss:%f,val_acc:%f,val_total:%d'%(epoch,batch_idx,log_loss,accuracy,len(val_dataset)))\n",
    "                    if  log_loss < min_loss:\n",
    "                        utils.snapshot('../model/', 'DesNet161', {\n",
    "                               'epoch': epoch + 1,\n",
    "                               'state_dict': model.state_dict(),\n",
    "                               'optimizer': optimizer.state_dict(),\n",
    "                               'val_loss': log_loss,\n",
    "                               'val_correct':accuracy })          \n",
    "\n",
    "                        min_loss=log_loss\n",
    "                        print('save new model loss,now loss is ',min_loss)\n",
    "\n",
    "                    if accuracy>min_acc:\n",
    "                        utils.snapshot('../model/', 'DesNet161', {\n",
    "                               'epoch': epoch + 1,\n",
    "                               'state_dict': model.state_dict(),\n",
    "                               'optimizer': optimizer.state_dict(),\n",
    "                               'val_loss': log_loss,\n",
    "                               'val_correct':accuracy },key='acc') \n",
    "                        min_acc=accuracy\n",
    "                        print('save new model acc,now acc is ',min_acc)\n",
    "        print('[epoch:%d] :acc: %f,loss:%f,lr:%f,patience:%d'%(epoch,running_corrects.value,running_loss.value,lr,patience))         \n",
    "        lx,px=utils.predict(model,val_dataLoader)\n",
    "        log_loss = criterion(Variable(px), Variable(lx))\n",
    "        log_loss = log_loss.item()\n",
    "        _, preds = torch.max(px, dim=1)\n",
    "        accuracy = torch.mean((preds == lx).float())\n",
    "        writer.add_scalar('Val/Acc',accuracy,(epoch+1) * len(train_dataset)/BATCH_SIZE)\n",
    "        writer.add_scalar('Val/Loss',log_loss,(epoch+1) * len(train_dataset)/BATCH_SIZE)\n",
    "        print('[epoch:%d]: val_loss:%f,val_acc:%f,'%(epoch,log_loss,accuracy))\n",
    "        if  log_loss < min_loss:\n",
    "            utils.snapshot('../model/', 'DesNet161', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy })          \n",
    "            patience = 0\n",
    "            min_loss=log_loss\n",
    "            print('save new model loss,now loss is ',min_loss)\n",
    "        else:\n",
    "            patience += 1\n",
    "        if accuracy>min_acc:\n",
    "            utils.snapshot('../model/', 'DesNet161', {\n",
    "                   'epoch': epoch + 1,\n",
    "                   'state_dict': model.state_dict(),\n",
    "                   'optimizer': optimizer.state_dict(),\n",
    "                   'val_loss': log_loss,\n",
    "                   'val_correct':accuracy },key='acc') \n",
    "            min_acc=accuracy\n",
    "            print('save new model acc,now acc is ',min_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] loading model... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lujunfeng/anaconda3/envs/Conda_Env_Pytorch/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "min_loss is :0.382938\n",
      "val_correct is 0.865389\n",
      "Epoch 12/59\n",
      "----------\n",
      "begin lr is  0.0001\n",
      "[epoch:12,batch:29]:acc: 0.944792,loss:0.134083\n",
      "[epoch:12,batch:59]:acc: 0.941146,loss:0.144881\n",
      "[epoch:12,batch:89]:acc: 0.942014,loss:0.143863\n",
      "[epoch:12,batch:119]:acc: 0.941927,loss:0.145871\n",
      "[epoch:12,batch:149]:acc: 0.941875,loss:0.146085\n",
      "[epoch:12,batch:179]:acc: 0.941840,loss:0.146879\n",
      "[epoch:12,batch:209]:acc: 0.939435,loss:0.148838\n",
      "[epoch:12,batch:239]:acc: 0.940755,loss:0.148168\n",
      "[epoch:12,batch:269]:acc: 0.939815,loss:0.149295\n",
      "[epoch:12,batch:299]:acc: 0.939583,loss:0.150478\n",
      "[epoch:12,batch:299]: val_loss:0.372221,val_acc:0.869134,val_total:4539\n",
      "save new model loss,now loss is  0.3722209632396698\n",
      "save new model acc,now acc is  tensor(0.8691, device='cuda:0')\n",
      "[epoch:12,batch:329]:acc: 0.939962,loss:0.150594\n",
      "[epoch:12,batch:359]:acc: 0.939670,loss:0.150664\n",
      "[epoch:12,batch:389]:acc: 0.939904,loss:0.150077\n",
      "[epoch:12,batch:419]:acc: 0.938765,loss:0.150779\n",
      "[epoch:12,batch:449]:acc: 0.938958,loss:0.150684\n",
      "[epoch:12,batch:479]:acc: 0.939128,loss:0.150681\n",
      "[epoch:12,batch:509]:acc: 0.939032,loss:0.151313\n",
      "[epoch:12,batch:539]:acc: 0.938947,loss:0.151384\n",
      "[epoch:12,batch:569]:acc: 0.939693,loss:0.150590\n",
      "[epoch:12,batch:599]:acc: 0.939896,loss:0.150612\n",
      "[epoch:12,batch:599]: val_loss:0.368670,val_acc:0.868914,val_total:4539\n",
      "save new model loss,now loss is  0.3686703145503998\n",
      "[epoch:12,batch:629]:acc: 0.940327,loss:0.149370\n",
      "[epoch:12,batch:659]:acc: 0.940246,loss:0.149527\n",
      "[epoch:12,batch:689]:acc: 0.940670,loss:0.149469\n",
      "[epoch:12,batch:719]:acc: 0.940408,loss:0.149571\n",
      "[epoch:12,batch:749]:acc: 0.940542,loss:0.149521\n",
      "[epoch:12,batch:779]:acc: 0.940825,loss:0.149208\n",
      "[epoch:12,batch:809]:acc: 0.941049,loss:0.148811\n",
      "[epoch:12,batch:839]:acc: 0.941406,loss:0.148339\n",
      "[epoch:12,batch:869]:acc: 0.941307,loss:0.148208\n",
      "[epoch:12,batch:899]:acc: 0.941354,loss:0.148144\n",
      "[epoch:12,batch:899]: val_loss:0.370629,val_acc:0.869795,val_total:4539\n",
      "save new model acc,now acc is  tensor(0.8698, device='cuda:0')\n",
      "[epoch:12,batch:929]:acc: 0.941465,loss:0.148137\n",
      "[epoch:12,batch:959]:acc: 0.941829,loss:0.147649\n",
      "[epoch:12,batch:989]:acc: 0.941225,loss:0.148042\n",
      "[epoch:12] :acc: 0.941262,loss:0.148024,lr:0.000100,patience:0\n",
      "[epoch:12]: val_loss:0.373393,val_acc:0.869575,\n",
      "Epoch 13/59\n",
      "----------\n",
      "[epoch:13,batch:29]:acc: 0.943750,loss:0.163214\n",
      "[epoch:13,batch:59]:acc: 0.944271,loss:0.154372\n",
      "[epoch:13,batch:89]:acc: 0.944097,loss:0.149638\n",
      "[epoch:13,batch:119]:acc: 0.945833,loss:0.147381\n",
      "[epoch:13,batch:149]:acc: 0.946458,loss:0.147578\n",
      "[epoch:13,batch:179]:acc: 0.946528,loss:0.147273\n",
      "[epoch:13,batch:209]:acc: 0.946280,loss:0.144946\n",
      "[epoch:13,batch:239]:acc: 0.945312,loss:0.145163\n",
      "[epoch:13,batch:269]:acc: 0.945833,loss:0.143979\n",
      "[epoch:13,batch:299]:acc: 0.945833,loss:0.144513\n",
      "[epoch:13,batch:299]: val_loss:0.369326,val_acc:0.868694,val_total:4539\n",
      "[epoch:13,batch:329]:acc: 0.946023,loss:0.144555\n",
      "[epoch:13,batch:359]:acc: 0.946354,loss:0.143850\n",
      "[epoch:13,batch:389]:acc: 0.945673,loss:0.143725\n",
      "[epoch:13,batch:419]:acc: 0.945015,loss:0.144039\n",
      "[epoch:13,batch:449]:acc: 0.944792,loss:0.144407\n",
      "[epoch:13,batch:479]:acc: 0.945052,loss:0.144183\n",
      "[epoch:13,batch:509]:acc: 0.945404,loss:0.143899\n",
      "[epoch:13,batch:539]:acc: 0.944907,loss:0.144696\n",
      "[epoch:13,batch:569]:acc: 0.945230,loss:0.144358\n",
      "[epoch:13,batch:599]:acc: 0.945729,loss:0.143829\n",
      "[epoch:13,batch:599]: val_loss:0.367989,val_acc:0.868914,val_total:4539\n",
      "save new model loss,now loss is  0.36798936128616333\n",
      "[epoch:13,batch:629]:acc: 0.945585,loss:0.143548\n",
      "[epoch:13,batch:659]:acc: 0.945265,loss:0.143341\n",
      "[epoch:13,batch:689]:acc: 0.945018,loss:0.143556\n",
      "[epoch:13,batch:719]:acc: 0.945052,loss:0.143456\n",
      "[epoch:13,batch:749]:acc: 0.945208,loss:0.142923\n",
      "[epoch:13,batch:779]:acc: 0.944952,loss:0.143159\n",
      "[epoch:13,batch:809]:acc: 0.945062,loss:0.143163\n",
      "[epoch:13,batch:839]:acc: 0.944940,loss:0.143217\n",
      "[epoch:13,batch:869]:acc: 0.945438,loss:0.142451\n",
      "[epoch:13,batch:899]:acc: 0.945521,loss:0.142153\n",
      "[epoch:13,batch:899]: val_loss:0.370864,val_acc:0.868253,val_total:4539\n",
      "[epoch:13,batch:929]:acc: 0.945766,loss:0.142086\n",
      "[epoch:13,batch:959]:acc: 0.945898,loss:0.141718\n",
      "[epoch:13,batch:989]:acc: 0.946149,loss:0.141151\n",
      "[epoch:13] :acc: 0.946117,loss:0.141567,lr:0.000100,patience:1\n",
      "[epoch:13]: val_loss:0.371455,val_acc:0.868914,\n",
      "Epoch 14/59\n",
      "----------\n",
      "[epoch:14,batch:29]:acc: 0.954167,loss:0.128743\n",
      "[epoch:14,batch:59]:acc: 0.951562,loss:0.129443\n",
      "[epoch:14,batch:89]:acc: 0.952431,loss:0.128019\n",
      "[epoch:14,batch:119]:acc: 0.952604,loss:0.128663\n",
      "[epoch:14,batch:149]:acc: 0.953125,loss:0.127428\n",
      "[epoch:14,batch:179]:acc: 0.951215,loss:0.129745\n",
      "[epoch:14,batch:209]:acc: 0.952381,loss:0.128682\n",
      "[epoch:14,batch:239]:acc: 0.952734,loss:0.127102\n",
      "[epoch:14,batch:269]:acc: 0.951852,loss:0.129468\n",
      "[epoch:14,batch:299]:acc: 0.951458,loss:0.129853\n",
      "[epoch:14,batch:299]: val_loss:0.366515,val_acc:0.869795,val_total:4539\n",
      "save new model loss,now loss is  0.3665153980255127\n",
      "[epoch:14,batch:329]:acc: 0.951989,loss:0.129477\n",
      "[epoch:14,batch:359]:acc: 0.952604,loss:0.128615\n",
      "[epoch:14,batch:389]:acc: 0.952564,loss:0.128669\n",
      "[epoch:14,batch:419]:acc: 0.953051,loss:0.127502\n",
      "[epoch:14,batch:449]:acc: 0.953194,loss:0.127540\n",
      "[epoch:14,batch:479]:acc: 0.953190,loss:0.127489\n",
      "[epoch:14,batch:509]:acc: 0.953983,loss:0.126967\n",
      "[epoch:14,batch:539]:acc: 0.953819,loss:0.127122\n",
      "[epoch:14,batch:569]:acc: 0.953399,loss:0.127589\n",
      "[epoch:14,batch:599]:acc: 0.953229,loss:0.127688\n",
      "[epoch:14,batch:599]: val_loss:0.366001,val_acc:0.870015,val_total:4539\n",
      "save new model loss,now loss is  0.3660013675689697\n",
      "save new model acc,now acc is  tensor(0.8700, device='cuda:0')\n",
      "[epoch:14,batch:629]:acc: 0.953026,loss:0.128246\n",
      "[epoch:14,batch:659]:acc: 0.953504,loss:0.127914\n",
      "[epoch:14,batch:689]:acc: 0.953940,loss:0.127402\n",
      "[epoch:14,batch:719]:acc: 0.954167,loss:0.127223\n",
      "[epoch:14,batch:749]:acc: 0.953292,loss:0.127958\n",
      "[epoch:14,batch:779]:acc: 0.953045,loss:0.128501\n",
      "[epoch:14,batch:809]:acc: 0.953086,loss:0.128340\n",
      "[epoch:14,batch:839]:acc: 0.952939,loss:0.128326\n",
      "[epoch:14,batch:869]:acc: 0.952550,loss:0.128765\n",
      "[epoch:14,batch:899]:acc: 0.952535,loss:0.128994\n",
      "[epoch:14,batch:899]: val_loss:0.366789,val_acc:0.869354,val_total:4539\n",
      "[epoch:14,batch:929]:acc: 0.952487,loss:0.129040\n",
      "[epoch:14,batch:959]:acc: 0.952507,loss:0.128990\n",
      "[epoch:14,batch:989]:acc: 0.952967,loss:0.128654\n",
      "[epoch:14] :acc: 0.952927,loss:0.128855,lr:0.000100,patience:2\n",
      "[epoch:14]: val_loss:0.370885,val_acc:0.868033,\n",
      "Epoch 15/59\n",
      "----------\n",
      "lr desencd\n",
      "[epoch:15,batch:29]:acc: 0.957292,loss:0.117399\n",
      "[epoch:15,batch:59]:acc: 0.956250,loss:0.119672\n",
      "[epoch:15,batch:89]:acc: 0.957986,loss:0.120453\n",
      "[epoch:15,batch:119]:acc: 0.957031,loss:0.120483\n",
      "[epoch:15,batch:149]:acc: 0.957917,loss:0.120228\n",
      "[epoch:15,batch:179]:acc: 0.956424,loss:0.123479\n",
      "[epoch:15,batch:209]:acc: 0.956548,loss:0.123217\n",
      "[epoch:15,batch:239]:acc: 0.955729,loss:0.124492\n",
      "[epoch:15,batch:269]:acc: 0.956134,loss:0.125063\n",
      "[epoch:15,batch:299]:acc: 0.955937,loss:0.124154\n",
      "[epoch:15,batch:299]: val_loss:0.365744,val_acc:0.870236,val_total:4539\n",
      "save new model loss,now loss is  0.3657442629337311\n",
      "save new model acc,now acc is  tensor(0.8702, device='cuda:0')\n",
      "[epoch:15,batch:329]:acc: 0.957386,loss:0.122760\n",
      "[epoch:15,batch:359]:acc: 0.957899,loss:0.121746\n",
      "[epoch:15,batch:389]:acc: 0.957853,loss:0.122022\n",
      "[epoch:15,batch:419]:acc: 0.958333,loss:0.120954\n",
      "[epoch:15,batch:449]:acc: 0.958681,loss:0.120849\n",
      "[epoch:15,batch:479]:acc: 0.958203,loss:0.121408\n",
      "[epoch:15,batch:509]:acc: 0.958640,loss:0.121169\n",
      "[epoch:15,batch:539]:acc: 0.958738,loss:0.121147\n",
      "[epoch:15,batch:569]:acc: 0.958717,loss:0.121322\n",
      "[epoch:15,batch:599]:acc: 0.959167,loss:0.120355\n",
      "[epoch:15,batch:599]: val_loss:0.368128,val_acc:0.870015,val_total:4539\n",
      "[epoch:15,batch:629]:acc: 0.958829,loss:0.121160\n",
      "[epoch:15,batch:659]:acc: 0.958996,loss:0.120472\n",
      "[epoch:15,batch:689]:acc: 0.958741,loss:0.120910\n",
      "[epoch:15,batch:719]:acc: 0.958854,loss:0.120404\n",
      "[epoch:15,batch:749]:acc: 0.958917,loss:0.120182\n",
      "[epoch:15,batch:779]:acc: 0.958774,loss:0.120150\n",
      "[epoch:15,batch:809]:acc: 0.959144,loss:0.119599\n",
      "[epoch:15,batch:839]:acc: 0.959115,loss:0.119612\n",
      "[epoch:15,batch:869]:acc: 0.959591,loss:0.118853\n",
      "[epoch:15,batch:899]:acc: 0.959549,loss:0.118632\n",
      "[epoch:15,batch:899]: val_loss:0.367742,val_acc:0.870676,val_total:4539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save new model acc,now acc is  tensor(0.8707, device='cuda:0')\n",
      "[epoch:15,batch:929]:acc: 0.959341,loss:0.118820\n",
      "[epoch:15,batch:959]:acc: 0.959473,loss:0.118541\n",
      "[epoch:15,batch:989]:acc: 0.959470,loss:0.118535\n",
      "[epoch:15] :acc: 0.959422,loss:0.118795,lr:0.000020,patience:0\n",
      "[epoch:15]: val_loss:0.367276,val_acc:0.870456,\n",
      "Epoch 16/59\n",
      "----------\n",
      "[epoch:16,batch:29]:acc: 0.965625,loss:0.101848\n",
      "[epoch:16,batch:59]:acc: 0.963021,loss:0.108099\n",
      "[epoch:16,batch:89]:acc: 0.960764,loss:0.112451\n",
      "[epoch:16,batch:119]:acc: 0.960156,loss:0.113375\n",
      "[epoch:16,batch:149]:acc: 0.959167,loss:0.114261\n",
      "[epoch:16,batch:179]:acc: 0.957986,loss:0.115825\n",
      "[epoch:16,batch:209]:acc: 0.957589,loss:0.117024\n",
      "[epoch:16,batch:239]:acc: 0.957682,loss:0.117070\n",
      "[epoch:16,batch:269]:acc: 0.957986,loss:0.117376\n",
      "[epoch:16,batch:299]:acc: 0.958750,loss:0.117106\n",
      "[epoch:16,batch:299]: val_loss:0.368306,val_acc:0.869354,val_total:4539\n",
      "[epoch:16,batch:329]:acc: 0.959564,loss:0.116206\n",
      "[epoch:16,batch:359]:acc: 0.959462,loss:0.116014\n",
      "[epoch:16,batch:389]:acc: 0.959455,loss:0.116069\n",
      "[epoch:16,batch:419]:acc: 0.959598,loss:0.116309\n",
      "[epoch:16,batch:449]:acc: 0.960069,loss:0.115968\n",
      "[epoch:16,batch:479]:acc: 0.960352,loss:0.115867\n",
      "[epoch:16,batch:509]:acc: 0.960294,loss:0.116062\n",
      "[epoch:16,batch:539]:acc: 0.960301,loss:0.116085\n",
      "[epoch:16,batch:569]:acc: 0.960417,loss:0.116074\n",
      "[epoch:16,batch:599]:acc: 0.960938,loss:0.115139\n",
      "[epoch:16,batch:599]: val_loss:0.366566,val_acc:0.868694,val_total:4539\n",
      "[epoch:16,batch:629]:acc: 0.960813,loss:0.115402\n",
      "[epoch:16,batch:659]:acc: 0.960938,loss:0.115011\n",
      "[epoch:16,batch:689]:acc: 0.960417,loss:0.115448\n",
      "[epoch:16,batch:719]:acc: 0.960720,loss:0.115561\n",
      "[epoch:16,batch:749]:acc: 0.960958,loss:0.115218\n",
      "[epoch:16,batch:779]:acc: 0.961018,loss:0.115271\n",
      "[epoch:16,batch:809]:acc: 0.961111,loss:0.115154\n",
      "[epoch:16,batch:839]:acc: 0.961012,loss:0.115385\n",
      "[epoch:16,batch:869]:acc: 0.960776,loss:0.115560\n",
      "[epoch:16,batch:899]:acc: 0.960486,loss:0.115940\n",
      "[epoch:16,batch:899]: val_loss:0.367621,val_acc:0.869795,val_total:4539\n",
      "[epoch:16,batch:929]:acc: 0.960316,loss:0.115983\n",
      "[epoch:16,batch:959]:acc: 0.960124,loss:0.115931\n",
      "[epoch:16,batch:989]:acc: 0.960227,loss:0.116061\n",
      "[epoch:16] :acc: 0.960179,loss:0.116183,lr:0.000020,patience:1\n",
      "[epoch:16]: val_loss:0.366879,val_acc:0.869795,\n",
      "Epoch 17/59\n",
      "----------\n",
      "[epoch:17,batch:29]:acc: 0.961458,loss:0.113777\n",
      "[epoch:17,batch:59]:acc: 0.966146,loss:0.105695\n",
      "[epoch:17,batch:89]:acc: 0.965278,loss:0.105474\n",
      "[epoch:17,batch:119]:acc: 0.964844,loss:0.107291\n",
      "[epoch:17,batch:149]:acc: 0.965625,loss:0.108872\n",
      "[epoch:17,batch:179]:acc: 0.966146,loss:0.108740\n",
      "[epoch:17,batch:209]:acc: 0.965774,loss:0.108560\n",
      "[epoch:17,batch:239]:acc: 0.965625,loss:0.107598\n",
      "[epoch:17,batch:269]:acc: 0.964699,loss:0.109716\n",
      "[epoch:17,batch:299]:acc: 0.964375,loss:0.110397\n",
      "[epoch:17,batch:299]: val_loss:0.365791,val_acc:0.870015,val_total:4539\n",
      "[epoch:17,batch:329]:acc: 0.964489,loss:0.111468\n",
      "[epoch:17,batch:359]:acc: 0.964497,loss:0.111360\n",
      "[epoch:17,batch:389]:acc: 0.964663,loss:0.111567\n",
      "[epoch:17,batch:419]:acc: 0.964509,loss:0.112231\n",
      "[epoch:17,batch:449]:acc: 0.964722,loss:0.112150\n",
      "[epoch:17,batch:479]:acc: 0.964128,loss:0.112722\n",
      "[epoch:17,batch:509]:acc: 0.964400,loss:0.112438\n",
      "[epoch:17,batch:539]:acc: 0.964063,loss:0.113166\n",
      "[epoch:17,batch:569]:acc: 0.964254,loss:0.112523\n",
      "[epoch:17,batch:599]:acc: 0.964844,loss:0.112121\n",
      "[epoch:17,batch:599]: val_loss:0.367172,val_acc:0.870676,val_total:4539\n",
      "[epoch:17,batch:629]:acc: 0.964931,loss:0.111439\n",
      "[epoch:17,batch:659]:acc: 0.964962,loss:0.111788\n",
      "[epoch:17,batch:689]:acc: 0.964583,loss:0.112237\n",
      "[epoch:17,batch:719]:acc: 0.964627,loss:0.111700\n",
      "[epoch:17,batch:749]:acc: 0.964667,loss:0.111349\n",
      "[epoch:17,batch:779]:acc: 0.964383,loss:0.111674\n",
      "[epoch:17,batch:809]:acc: 0.964660,loss:0.111449\n",
      "[epoch:17,batch:839]:acc: 0.964435,loss:0.111717\n",
      "[epoch:17,batch:869]:acc: 0.964296,loss:0.111600\n",
      "[epoch:17,batch:899]:acc: 0.964201,loss:0.111663\n",
      "[epoch:17,batch:899]: val_loss:0.365943,val_acc:0.868694,val_total:4539\n",
      "[epoch:17,batch:929]:acc: 0.963978,loss:0.112062\n",
      "[epoch:17,batch:959]:acc: 0.963867,loss:0.111978\n",
      "[epoch:17,batch:989]:acc: 0.963763,loss:0.112267\n",
      "[epoch:17] :acc: 0.963742,loss:0.112483,lr:0.000020,patience:2\n",
      "[epoch:17]: val_loss:0.368533,val_acc:0.871558,\n",
      "save new model acc,now acc is  tensor(0.8716, device='cuda:0')\n",
      "Epoch 18/59\n",
      "----------\n",
      "lr desencd\n",
      "[epoch:18,batch:29]:acc: 0.954167,loss:0.127693\n",
      "[epoch:18,batch:59]:acc: 0.957812,loss:0.121785\n",
      "[epoch:18,batch:89]:acc: 0.955208,loss:0.123289\n",
      "[epoch:18,batch:119]:acc: 0.956771,loss:0.119808\n",
      "[epoch:18,batch:149]:acc: 0.956875,loss:0.118667\n",
      "[epoch:18,batch:179]:acc: 0.959722,loss:0.114893\n",
      "[epoch:18,batch:209]:acc: 0.960714,loss:0.116763\n",
      "[epoch:18,batch:239]:acc: 0.960026,loss:0.117316\n",
      "[epoch:18,batch:269]:acc: 0.960069,loss:0.116897\n",
      "[epoch:18,batch:299]:acc: 0.959375,loss:0.118256\n",
      "[epoch:18,batch:299]: val_loss:0.367412,val_acc:0.868694,val_total:4539\n",
      "[epoch:18,batch:329]:acc: 0.959470,loss:0.117713\n",
      "[epoch:18,batch:359]:acc: 0.958681,loss:0.118552\n",
      "[epoch:18,batch:389]:acc: 0.958974,loss:0.118691\n",
      "[epoch:18,batch:419]:acc: 0.959524,loss:0.118010\n",
      "[epoch:18,batch:449]:acc: 0.959097,loss:0.118460\n",
      "[epoch:18,batch:479]:acc: 0.959049,loss:0.119010\n",
      "[epoch:18,batch:509]:acc: 0.958456,loss:0.119828\n",
      "[epoch:18,batch:539]:acc: 0.958333,loss:0.120189\n",
      "[epoch:18,batch:569]:acc: 0.958059,loss:0.120703\n",
      "[epoch:18,batch:599]:acc: 0.958333,loss:0.120517\n",
      "[epoch:18,batch:599]: val_loss:0.365422,val_acc:0.868694,val_total:4539\n",
      "save new model loss,now loss is  0.3654223680496216\n",
      "[epoch:18,batch:629]:acc: 0.958036,loss:0.120941\n",
      "[epoch:18,batch:659]:acc: 0.958144,loss:0.120842\n",
      "[epoch:18,batch:689]:acc: 0.958197,loss:0.120896\n",
      "[epoch:18,batch:719]:acc: 0.958030,loss:0.121037\n",
      "[epoch:18,batch:749]:acc: 0.958042,loss:0.121025\n",
      "[epoch:18,batch:779]:acc: 0.957652,loss:0.121170\n",
      "[epoch:18,batch:809]:acc: 0.957562,loss:0.121271\n",
      "[epoch:18,batch:839]:acc: 0.957552,loss:0.121466\n",
      "[epoch:18,batch:869]:acc: 0.957399,loss:0.121576\n",
      "[epoch:18,batch:899]:acc: 0.957431,loss:0.121501\n",
      "[epoch:18,batch:899]: val_loss:0.368051,val_acc:0.868033,val_total:4539\n",
      "[epoch:18,batch:929]:acc: 0.957124,loss:0.122160\n",
      "[epoch:18,batch:959]:acc: 0.957389,loss:0.121647\n",
      "[epoch:18,batch:989]:acc: 0.957418,loss:0.121494\n",
      "[epoch:18] :acc: 0.957341,loss:0.121680,lr:0.000004,patience:0\n",
      "[epoch:18]: val_loss:0.370278,val_acc:0.868033,\n",
      "Epoch 19/59\n",
      "----------\n",
      "[epoch:19,batch:29]:acc: 0.959375,loss:0.119427\n",
      "[epoch:19,batch:59]:acc: 0.964583,loss:0.124092\n",
      "[epoch:19,batch:89]:acc: 0.964583,loss:0.119956\n",
      "[epoch:19,batch:119]:acc: 0.963542,loss:0.122298\n",
      "[epoch:19,batch:149]:acc: 0.962083,loss:0.122847\n",
      "[epoch:19,batch:179]:acc: 0.961111,loss:0.121483\n",
      "[epoch:19,batch:209]:acc: 0.959673,loss:0.121167\n",
      "[epoch:19,batch:239]:acc: 0.959115,loss:0.121314\n",
      "[epoch:19,batch:269]:acc: 0.958102,loss:0.121819\n",
      "[epoch:19,batch:299]:acc: 0.958750,loss:0.120877\n",
      "[epoch:19,batch:299]: val_loss:0.366679,val_acc:0.870236,val_total:4539\n",
      "[epoch:19,batch:329]:acc: 0.957955,loss:0.121683\n",
      "[epoch:19,batch:359]:acc: 0.957292,loss:0.123149\n",
      "[epoch:19,batch:389]:acc: 0.957292,loss:0.122877\n",
      "[epoch:19,batch:419]:acc: 0.956845,loss:0.123569\n",
      "[epoch:19,batch:449]:acc: 0.957639,loss:0.122371\n",
      "[epoch:19,batch:479]:acc: 0.957747,loss:0.122595\n",
      "[epoch:19,batch:509]:acc: 0.957108,loss:0.123312\n",
      "[epoch:19,batch:539]:acc: 0.957639,loss:0.123001\n",
      "[epoch:19,batch:569]:acc: 0.957785,loss:0.122687\n",
      "[epoch:19,batch:599]:acc: 0.957812,loss:0.122402\n",
      "[epoch:19,batch:599]: val_loss:0.367842,val_acc:0.871337,val_total:4539\n",
      "[epoch:19,batch:629]:acc: 0.957639,loss:0.122427\n",
      "[epoch:19,batch:659]:acc: 0.957481,loss:0.122602\n",
      "[epoch:19,batch:689]:acc: 0.957654,loss:0.122322\n",
      "[epoch:19,batch:719]:acc: 0.957682,loss:0.122516\n",
      "[epoch:19,batch:749]:acc: 0.957792,loss:0.122316\n",
      "[epoch:19,batch:779]:acc: 0.957732,loss:0.122560\n",
      "[epoch:19,batch:809]:acc: 0.957870,loss:0.122426\n",
      "[epoch:19,batch:839]:acc: 0.958036,loss:0.121957\n",
      "[epoch:19,batch:869]:acc: 0.958261,loss:0.121669\n",
      "[epoch:19,batch:899]:acc: 0.958160,loss:0.121980\n",
      "[epoch:19,batch:899]: val_loss:0.367403,val_acc:0.871117,val_total:4539\n",
      "[epoch:19,batch:929]:acc: 0.958233,loss:0.121884\n",
      "[epoch:19,batch:959]:acc: 0.957682,loss:0.122333\n",
      "[epoch:19,batch:989]:acc: 0.957734,loss:0.122369\n",
      "[epoch:19] :acc: 0.957688,loss:0.122714,lr:0.000004,patience:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:19]: val_loss:0.369849,val_acc:0.868694,\n",
      "Epoch 20/59\n",
      "----------\n",
      "[epoch:20,batch:29]:acc: 0.954167,loss:0.126430\n",
      "[epoch:20,batch:59]:acc: 0.952083,loss:0.129708\n",
      "[epoch:20,batch:89]:acc: 0.953472,loss:0.125645\n",
      "[epoch:20,batch:119]:acc: 0.958333,loss:0.119517\n",
      "[epoch:20,batch:149]:acc: 0.958958,loss:0.119525\n",
      "[epoch:20,batch:179]:acc: 0.958160,loss:0.121246\n",
      "[epoch:20,batch:209]:acc: 0.958482,loss:0.120624\n",
      "[epoch:20,batch:239]:acc: 0.958333,loss:0.120137\n",
      "[epoch:20,batch:269]:acc: 0.958912,loss:0.119471\n",
      "[epoch:20,batch:299]:acc: 0.958125,loss:0.119634\n",
      "[epoch:20,batch:299]: val_loss:0.368250,val_acc:0.870676,val_total:4539\n",
      "[epoch:20,batch:329]:acc: 0.958617,loss:0.119016\n",
      "[epoch:20,batch:359]:acc: 0.957812,loss:0.120113\n",
      "[epoch:20,batch:389]:acc: 0.957772,loss:0.120056\n",
      "[epoch:20,batch:419]:acc: 0.958333,loss:0.119202\n",
      "[epoch:20,batch:449]:acc: 0.959097,loss:0.117667\n",
      "[epoch:20,batch:479]:acc: 0.958984,loss:0.118482\n",
      "[epoch:20,batch:509]:acc: 0.959252,loss:0.117996\n",
      "[epoch:20,batch:539]:acc: 0.959375,loss:0.118061\n",
      "[epoch:20,batch:569]:acc: 0.959539,loss:0.118106\n",
      "[epoch:20,batch:599]:acc: 0.959635,loss:0.117717\n",
      "[epoch:20,batch:599]: val_loss:0.369162,val_acc:0.871117,val_total:4539\n",
      "[epoch:20,batch:629]:acc: 0.959524,loss:0.117741\n",
      "[epoch:20,batch:659]:acc: 0.959091,loss:0.118336\n",
      "[epoch:20,batch:689]:acc: 0.958605,loss:0.118830\n",
      "[epoch:20,batch:719]:acc: 0.958377,loss:0.118900\n",
      "[epoch:20,batch:749]:acc: 0.958417,loss:0.119005\n",
      "[epoch:20,batch:779]:acc: 0.958253,loss:0.119399\n",
      "[epoch:20,batch:809]:acc: 0.958372,loss:0.119201\n",
      "[epoch:20,batch:839]:acc: 0.958259,loss:0.119878\n",
      "[epoch:20,batch:869]:acc: 0.958872,loss:0.119284\n",
      "[epoch:20,batch:899]:acc: 0.958854,loss:0.119248\n",
      "[epoch:20,batch:899]: val_loss:0.369701,val_acc:0.870015,val_total:4539\n",
      "[epoch:20,batch:929]:acc: 0.958602,loss:0.119589\n",
      "[epoch:20,batch:959]:acc: 0.958464,loss:0.119747\n",
      "[epoch:20,batch:989]:acc: 0.958428,loss:0.119603\n",
      "[epoch:20] :acc: 0.958445,loss:0.119659,lr:0.000004,patience:2\n",
      "[epoch:20]: val_loss:0.368360,val_acc:0.870456,\n",
      "Epoch 21/59\n",
      "----------\n",
      "lr desencd\n",
      "[epoch:21,batch:29]:acc: 0.958333,loss:0.118882\n",
      "[epoch:21,batch:59]:acc: 0.959896,loss:0.118692\n",
      "[epoch:21,batch:89]:acc: 0.958333,loss:0.121987\n",
      "[epoch:21,batch:119]:acc: 0.956510,loss:0.126241\n",
      "[epoch:21,batch:149]:acc: 0.957292,loss:0.124916\n",
      "[epoch:21,batch:179]:acc: 0.957292,loss:0.124471\n",
      "[epoch:21,batch:209]:acc: 0.956399,loss:0.126289\n",
      "[epoch:21,batch:239]:acc: 0.955078,loss:0.126900\n",
      "[epoch:21,batch:269]:acc: 0.955324,loss:0.125843\n",
      "[epoch:21,batch:299]:acc: 0.956250,loss:0.124764\n",
      "[epoch:21,batch:299]: val_loss:0.365870,val_acc:0.869795,val_total:4539\n",
      "[epoch:21,batch:329]:acc: 0.957102,loss:0.123838\n",
      "[epoch:21,batch:359]:acc: 0.957986,loss:0.122771\n",
      "[epoch:21,batch:389]:acc: 0.958413,loss:0.121427\n",
      "[epoch:21,batch:419]:acc: 0.957738,loss:0.121995\n",
      "[epoch:21,batch:449]:acc: 0.957361,loss:0.122362\n",
      "[epoch:21,batch:479]:acc: 0.957747,loss:0.122079\n",
      "[epoch:21,batch:509]:acc: 0.958088,loss:0.121642\n",
      "[epoch:21,batch:539]:acc: 0.958044,loss:0.121519\n",
      "[epoch:21,batch:569]:acc: 0.958333,loss:0.120873\n",
      "[epoch:21,batch:599]:acc: 0.958385,loss:0.120956\n"
     ]
    }
   ],
   "source": [
    "TrainWithRawData('../model/DesNet161/2018-11-01_acc_best.pth',60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Conda_Env_Pytorch]",
   "language": "python",
   "name": "conda-env-Conda_Env_Pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
